---
title: "Hands-on-ML with R"
author: "Koji Mizumura"
date: "2018-12-26 - `r Sys.Date()`"
always_allow_html: yes
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
    toc_number: yes
    theme: "spacelab"
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

# Gradient Boosting Machines {#gradient-boosting-machines}

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```

<img src="images/boosted_stumps.gif"  style="float:right; margin: -20px 0px 0px 0px; width: 30%; height: 30%;" />

Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions.  Whereas random forests (Chapter \@ref(random_forest)) build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous.  When combined, these many weak successive trees produce a powerful "committee" that are often hard to beat with other algorithms.  This chapter will cover the fundamentals to understanding and implementing GBMs.

## Package requirements {#gbm-prereq}

This chapter leverages the following packages. Some of these packages paly a supporting role; however, our focus is on demonstrating how to implement GBMs with the __gbm__ [@R-gbm], __xgboost__ [@R-xgboost] and __h2o__ pacakges and discuss the pros and cons to each.

```{r pkg-req, cache=FALSE}
library(rsample)  # data splitting 
library(gbm)      # original implementation of gbm
library(xgboost)  # a faster implementation of gbm
library(h2o)      # a java-based platform
library(vip)      # visualize feature importance 
library(pdp)      # visualize feature effects
library(ggplot2)  # model visualization
```

## Advantages & Disadvantages {#gbm-proscons}

__Advantages:__
* Often provides predictive accracy that cannot be beat
* Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.
* No data pre-processing required - often works great with categorical and numerical values as is.
* Handles missing data - imputation not required.

__Disadvantages:__
* GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize. 
* Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.
* The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
* Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, local variable importance, etc.).

## The idea {#gbm-idea}

Sevaral supervised machine learning models are founded on a single predictive model such as linear regression, penalized models, naive Bayes, support vector machines. Alternatively, other approaches such as bagging and random forests are built on the idea of building an ensenmble of models where each individual model predicts the outcome and then the ensemble simply averages the predicted values. The family of boosting methods is based on a different, constructive strategy of ensenmle formation.

The main idea of boosting is to add new models to the ensemble __sequentially__. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far. 









