---
title: "Caret introudction"
author: "Koji Mizumura"
date: 'September 6,2018 '
output:
  word_document:
    toc: yes
  html_document:
    code_folding: hide
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
---

Reference:  
[Bookdown](http://topepo.github.io/caret/index.html)  
[video](https://www.youtube.com/watch?v=z8PRU46I3NY)   
[Jap](https://logics-of-blue.com/r%E3%81%AB%E3%82%88%E3%82%8B%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%EF%BC%9Acaret%E3%83%91%E3%83%83%E3%82%B1%E3%83%BC%E3%82%B8%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9/)  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Set-up
```{r include=FALSE}
library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(magrittr)
library(caret)
library(earth)
```


# Visualizations
The `featurePlot` function is a wrapper for different `lattice` plots to visualize the data. For example, the following figures show the default plot fo continuous outcomes generated using the `featureplot` function. 

For classification data sets, the `iris` data are used for illusion.
```{r}
str(iris)
```

```{r eval=FALSE,include=FALSE}
install.packages("AppliedPredictiveModeling")
install.packages("caret")
```

## Scatterplot matrix
```{r }
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

## Scatterplot matrix with ellipses
```{r}
featurePlot(x=iris[,1:4],
            y=iris$Species,
            plot="ellipse",
            ## Add a key at the topc
            auto.key=list(columns=3))
```

## Overlayed density plots
```{r}
transparentTheme(trans = .9)
featurePlot(x = iris[, 1:4], 
            y = iris$Species,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5,
            pch = "|",
            layout = c(4, 1),
            auto.key = list(columns = 3))
```

## Box Plots
```{r}
featurePlot(x=iris[,1:4],
            y=iris$Species,
            plot="box",
            ## Pass in options to bwplot()
            scales=list(y=list(relation="free"),
                        x=list(rot=90)),
            layout=c(4,1),
            auto.key=list(columns=2))
```

## Scatter Plots
For regresson, the Boston Housing data is used:
```{r}
# install.packages("mlbench")

library(mlbench)
data(BostonHousing)

regVar <- c("age","lstat","tax")
str(BostonHousing[,regVar])
```

When the predictors are `continuous`, featurePlot can be used to create scatter plots of each of the predictors with the outcome. For example:
```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col=rgb(.2,.2,.2,.4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)

featurePlot(x=BostonHousing[,regVar],
            y=BostonHousing$medv,
            plot="scatter",
            layout=c(3,1))
```

Note that the x-axis scales are different. The function automatically uses `scales = list(y = list(relation = "free"))` so you don’t have to add it. We can also pass in options to the `**lattice**` function `xyplot`. For example, we can add a scatter plot smoother by passing in new options:
```{r}
trellis.par.set(theme1)


featurePlot(x = BostonHousing[, regVar], 
            y = BostonHousing$medv, 
            plot = "scatter",
            type = c("p", "smooth"),
            span = .5,
            layout = c(3, 1))
```

The options `degree` and `span` control the smoothness of the smoother.

# Chapter 3. Pre-processing
There are multiple techniques for data pre-processing.

*Creating Dummy Variables
*Zero- and Near Zero-Variance Predictors
*Identifying Correlated Predictors
*Linear Dependencies
*The preProcess Function
*Centering and Scaling
*Imputation
*Transforming Predictors
*Putting It All Together
*Class Distance Calculations

`caret` includes several functions to pre-process the predictor data. It assumes that all of the data are numeric(i.e., factors have been converted to dummy variables via `model.matrix`, `dummyVars` or other means).

note that the later chapter on using [`recipes`](https://tidymodels.github.io/recipes/) with `train` shows how that approach canoffer a more diverse and customizable interface to pre-process in the package.

## Creating dummy variables
The function `dummyVars` can be used to generate a compete (less than full rank parameterized) set of dummy variables from one or more factors. The function takes a fomula and a dataset and ouputs an object that can be used to create the dummy variables using theb predict method.

For example, the `etitanic` dataset in the `earth` package includes two factors: `pclass`(passenger class with levels 1st,2nd and 3rd) and `sex` (with levels female, male). The base R function `model.matrix` would generate th following variables:

```{r}
data(etitanic)
head(etitanic)

model.matrix(survived~.,data=etitanic) %>% 
  head()
```

Using `dummyVars`:
```{r}
dummies <- dummyVars(survived~.,data=etitanic)
predict(dummies,newdata=etitanic) %>% 
  head()
```

Now there is no intercept and each factor has a dummy variable for each level, so this parameterization may not be useful for some model functions, such as `lm`.

## Zero- and Near Zero-Variance Predictors
In some situations, the data generating mechanism can create predictors that only have a single unique value (ie. a **"zero-variance-predictor"**). For many models (excluding tree-based models), this may cause the model to crash or the fit to be unstable. 

Similarly, predictors might have only a handful of unique values that occur with very low frequencies. For example, in the drug resistance data, the `nR11` descritor (number of 11-membered rings) data have a few unique numeric values that are highly unbalanced:
```{r}
data(mdrr)

# Multidrug Resistance Reversal (MDRR) Agent Data

mdrrDescr$nR11 %>% 
  table() %>% 
  data.frame()
```

The concern here that these predictors may become zero-variance predictors when the data are split into cross-validation/bootstrap sub-samples or that a few samples may have an undue influence on the model. These “near-zero-variance” predictors may need to be identified and eliminated prior to modeling.

To identify these types of predictors, the following two metrics can be calculated:
* the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data and
* the “percent of unique values’’ is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases

If the frequency ratio is greater than a pre-specified threshold and the unique value percentage is less than a threshold, we might consider a predictor to be near zero-variance.

We would not want to falsely identify data that have low **granularity** but are evenly distributed, such as data from a discrete uniform distribution. Using both criteria should not falsely detect such predictors.

Looking at the MDRR data, the `nearZeroVar` function can be used to identify near zero-variance variables (the `saveMetrics` argument can be used to show the details and usually defaults to `FALSE`):
```{r}
nzv <- nearZeroVar(mdrrDescr,saveMetrics = T)
nzv[nzv$nzv,][1:10,]
```

```{r}
dim(mdrrDescr)
```

```{r}
nzv <- nearZeroVar(mdrrDescr)
filteredDescr <- mdrrDescr[, -nzv]
dim(filteredDescr)
```

## Identifying correlated predictors
While there are some models that thriveon correlated predictors (such as `pls`), other models may benefit from reducing the level of correlation between predictors.

Given a correlation matrix, the `findCorrelation` function uses the following algorithm to flag predictors for removal:
```{r}
descCor <- cor(filteredDescr)
highCorr <- sum(abs(descCor[upper.tri(descCor)])>.999)

highCorr
```

For the previous MDRR data, there are 65 descriptors that are almost perfectly correlated (|correlation| > 0.999), such as the total information index of atomic composition (IAC) and the total information content index (neighborhood symmetry of 0-order) (TIC0) (correlation = 1). The code chunk below shows the effect of removing descriptors with absolute correlations above 0.75.

```{r}
descCor <- cor(filteredDescr)
summary(descCor[upper.tri(descCor)])

m2 <- matrix(1:20, 4, 5)
lower.tri(m2)
m2[lower.tri(m2)] <- NA
m2
```

## Linear dependencies
The function `findLinearCombos` uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist). For example, consider the following matrix that could have produced by a less-than-full-rank parameterizations of a two-way experimental layout:
```{r}
ltfrDesign <- matrix(0, nrow=6, ncol=6)
ltfrDesign[,1] <- c(1, 1, 1, 1, 1, 1)
ltfrDesign[,2] <- c(1, 1, 1, 0, 0, 0)
ltfrDesign[,3] <- c(0, 0, 0, 1, 1, 1)
ltfrDesign[,4] <- c(1, 0, 0, 1, 0, 0)
ltfrDesign[,5] <- c(0, 1, 0, 0, 1, 0)
ltfrDesign[,6] <- c(0, 0, 1, 0, 0, 1)

ltfrDesign
```

Note that columns two and three add up to the first column. Similarly, columns four, five and six add up the first column. `findLinearCombos` will return a list that enumerates these dependencies. For each linear combination, it will incrementally remove columns from the matrix and test to see if the dependencies have been resolved. `findLinearCombos` will also return a vector of column positions can be removed to eliminate the linear dependencies:
```{r}
comboInfo <- findLinearCombos(ltfrDesign)
comboInfo
```

```{r}
ltfrDesign[,-comboInfo$remove]
```

These types of dependencies can arise when large numbers of binary chemial fingerprints are used to describe the structure of a molecule.

## 3.5 The `preProcess` Function

The `preProcess` class can be used for many operations on predictors, including *centering* and *scaling*. The function `preProcess` estimates the required parameters for each operation and `predict.preProcess` is used to apply them to specific data sets. This function can also be interfaces when calling the `train` function.

Several types of techniques are described in the next few sections and then another example is used to demonstrate how multiple methods can be used. Note that, in all cases, the preProcess function estimates whatever it requires from a specific data set (e.g. the training set) and then applies these transformations to any data set without recomputing the values

## Centering and scaling
In the example below, the half of the MDRR data are used to estimate the location and scale of the predictors. The function `preProcess` doesn't actually pre-process the data. `predict.preProcess` is used to pre-process this and other data sets.
```{r}
set.seed(96)
inTrain <- sample(seq(along=mdrrClass),length(mdrrClass)/2)

training <- filteredDescr[inTrain,]
test <- filteredDescr[-inTrain,]
trainMDRR <- mdrrClass[inTrain]
testMDRR <- mdrrClass[-inTrain]

preProcValues <- preProcess(training,method=c("center","scale"))

trainTrainsformed <- predict(preProcValues,training)
testTransformed <- predict(preProcValues,test)
```

The `preProcess` option `"range` scales the data to the interval between zero and one. 

## Imputation
`preProcess` can be used to impute data sets based only on information in the training set. One method of doing this is with K-nearest neighbors. For an arbitrary sample, the K closest neighbors are found in the training set and the value for the predictor is imputed using these values (e.g. using the mean). Using this approach will automatically trigger preProcess to center and scale the data, regardless of what is in the method argument. Alternatively, bagged trees can also be used to impute. For each predictor in the data, a bagged tree is created using all of the other predictors in the training set. When a new sample has a missing predictor value, the bagged model is used to predict the value. While, in theory, this is a more powerful method of imputing, the computational costs are much higher than the nearest neighbor technique.

## Transforming predictors
In some cases, there is a need to use **principal component analysis (PCA)** to transform the data to a smaller sub–space where the new variable are uncorrelated with one another. The `preProcess` class can apply this transformation by including `"pca"` in the `method` argument. Doing this will also force scaling of the predictors. Note that when PCA is requested, `predict.preProcess` changes the column names to `PC1`, `PC2` and so on.

Similarly, **independent component analysis (ICA)** can also be used to find new variables that are linear combinations of the original set such that the components are independent (as opposed to uncorrelated in PCA). The new variables will be labeled as `IC1`, `IC2` and so on.

The "spatial sign" transformation [Serneels et al, 2006](https://pubs.acs.org/doi/abs/10.1021/ci050498u) projects the data for a preidctor to the unit circle in `p` dimensions, where `p` is the number of predictors. Essentially, a vector of data is divided by its norm.

The two figures below show two centered and scaled discritors from the MDRR data before and after the spatial sign transformation. The predictors should be centered and scaled before applyin this transformation.

```{r}
library(AppliedPredictiveModeling)
transparentTheme(trans=.4)

plotSubset <- data.frame(scale(mdrrDescr[, c("nC", "X4v")])) 
xyplot(nC ~ X4v,
       data = plotSubset,
       groups = mdrrClass, 
       auto.key = list(columns = 2))  
```

After the spatial sign:
```{r}
transformed <- spatialSign(plotSubset)
transformed <- as.data.frame(transformed)

xyplot(nC~X4v,
       data=transformed,
       groups=mdrrClass,
       auto.key=list(columns=2))
```

Another option, `"BoxCox"` will estimate a **Box–Cox transformation** on the predictors if the data are greater than zero.
```{r}
preProcValues2 <- preProcess(training,method="BoxCox")
trainBC <- predict(preProcValues2,training)
testBC <- predict(preProcValues2,test)
preProcValues2
```

The `NA` values correspond to the predictors that could not be transformed. This transformation requires the data to be greater than zero. Two similar transformations, the Yeo-Johnson and exponential transformation of Manly (1976) can also be used in `preProcess`.

## Putting it all together

In *Applied PRedictive Modeling* threre is a case study where the execution times of jobs in a high performance computing environment are being predicted. The data are:

```{r}
library(AppliedPredictiveModeling)
data(schedulingData)
str(schedulingData)
```

The data are a mix of categorical and numeric predictors. Suppose we want to use the Yeo-Johnson transformation on the continuous predictors then center and scale them. Let’s also suppose that we will be running a tree-based models so we might want to keep the factors as factors (as opposed to creating dummy variables). We run the function on all the columns except the last, which is the outcome.
```{r}
pp_hpc <- preProcess(schedulingData[,-8],
                     method=c("center","scale","YeoJohnson"))
pp_hpc
```

```{r}
transformed <- predict(pp_hpc,newdata=schedulingData[,-8])
head(transformed)
```

The two predictors labeled as “ignored” in the output are the two factor predictors. These are not altered but the numeric predictors are transformed. However, the predictor for the number of pending jobs, has a very sparse and unbalanced distribution:

```{r}
mean(schedulingData$NumPending==0)
```

For some other models, this might be an issue (especially if we resample or down-sample the data). We can add a filter to check for zero- or near zero-variance predictors prior to running the processing calculations:
```{r}
schedulingData %>% 
  dplyr::select(8) %>% 
  head()

pp_no_nzv <- preProcess(schedulingData[,-8],
                         method=c("center","scale","YeoJohnson","nzv"))
pp_no_nzv
```

```{r}
schedulingData[1:6,-8]
predict(pp_no_nzv,newdata = schedulingData[1:6,-8])
```

Note that one predictor is labeled as “removed” and the processed data lack the sparse predictor.

## 3.10 Class distance calculations

`caret` contains functions to generate new predictors variables based on distances to class centroids (similar to how linear discriminant analysis works). For each level of a factor variable, the class centroid and covariance matrix is calculated. 

For new samples, the Mahalanobis distance to each of the class centroids is computed and can be used as an additional predictor. This can be helpful for non--linear models when the true decision boundary is actually linear.

In cases where there are more predictors within a class than samples, the `classDist` function has arguments called `pca` and `keep` arguments that allow for principal components analysis within each class to be used to avoid issues with singular covariance matrices.

`predict.classDist` is then used to generate the class distances. By default, the distances are logged, but this can be changed via the `trans` argument to `predict.classDist`.

As an example, we can used the MDRR data.
```{r eval=FALSE}
centroids <- classDist(trainBC, trainMDRR)
distances <- predict(centroids, testBC)
distances <- as.data.frame(distances)
head(distances)
```

This image shows a scatterplot matrix of the class distances for the held-out samples:
```{r eval=FALSE}
xyplot(dist.Active ~ dist.Inactive,
       data = distances, 
       groups = testMDRR, 
       auto.key = list(columns = 2))
```

# Chapter 4: Data splitting
Contents
*Simple Splitting Based on the Outcome
*Splitting Based on the Predictors
*Data Splitting for Time Series
*Data Splitting with Important Groups

## Simple splitting based on the outcome
The function `createDataPartition` can be used to create balanced splits of the data. If the `y` argument to this function is a factor, the random sampling occurs which each class and should preserve the overall class distribution of the data. For example, to create a single 80/20% split of the iris data:

```{r}
library(caret)
set.seed(3456)
trainIndex <- createDataPartition(iris$Species,p=.8,
                                  list=FALSE,
                                  times=1)
head(trainIndex)
```

```{r}
irisTrain <- iris[trainIndex,]
irisTest <- iris[-trainIndex,]
```

The `list = FALSE` avoids returning the data as a list. This function also has an argument, `times`, that can create multiple splits at once; the data indices are returned in a list of integer vectors. Similarly, createResample can be used to make simple bootstrap samples and `createFolds` can be used to generate balanced cross–validation groupings from a set of data.

## Splitting based on the predictors
Also, the function `maxDissim` can be used to create sub-samples using a maximum dissimilarity approach. Suppose there is a dataset $A$ with $m$ samples and a larger data set $B$ with $n$ samples. We may want to create a sub-sample from $B$ that is diverse when compared to $A$ . The most dissimilar point in $B$ is-added to $A$ and the process continues. 

There are many methods in R to calculate dissimilarity. ``caret` uses the `proxy` package. See the manual for the package for a list of available measures. Also, there are many ways to claculate which sample is "most similar". The argument `obj` can be used to specify any function that returns a scaler measure. `caret` includes two functions, `minDiss` and `sumDiss`, that can be used to maximize the minimum and total dissimilarities respectively.

As an example, the figure below shows a scatter plot of two chemical descriptors for the Cox2 data. Using an initial random sample of 5 compounds, we can select 20 more compounds from the data so that the new compounds are most dissimilar from the initial 5 that were specified. 

The panesl in the figure show that results using several combinations of distance metrics and scoring functions. For these data, the distance measure has less of an impact thant the scoring method for determining which compounds are most dissimilar.
```{r}
library(mlbench)
data(BostonHousing)

testing <- scale(BostonHousing[,c("age","nox")])
set.seed(5)


## A random sample of 5 data points
startSet <- sample(1:dim(testing)[1],5)
samplePool <- testing[-startSet,]
start <- testing[startSet,]
# newSamp <- maxDissim(start, samplePool, n = 20)
# head(newSamp)
```

The visualization below shows the data set (small points), the starting samples (larger blue points) and the order in which the other 20 samples are added.

## Data splitting for time series

Simple random sampling of time series is probably not the best way to resample time series data. [Hyndman and Athanasopoulos (2013)](https://www.otexts.org/fpp/2/5) discuss *rolling forecasting origin* techniques that move the training and test sets in a time. `caret` contains a function called `createTimeSlices` that can create the indices for this type of splitting.

The three parameters for this type of splitting are:
- `initialWindow`: the initial number of consecutive values in each training set sample
- `horizon`: The number of consecutive values in test set sample
- `fixedWindow`: A logical: if `FALSE`, the training set always start at the first sample and the training set size will vary over data splits.

As an exampl,e suppose we have a time series with 20 data points. We can fixe `initialWindow=5` and look at different settings of the other two arguments. In the plot below, rows in each panel correspond to different data splits (i.e. resamples) and the columns correspond to different data points. Also, red indicates samples that are in included in the training set and the blue indicates samples in the test set.

## Simple splitting with important groups

In some cases there is an important qualitative factor in the data that should be considered during (re)sampling. For example:

- in clinical trials, there may be hospital-to-hospital differences
- with longitudinal or repeated measures data, subjects (or general independent experimental unit) may have multiple rows in the data set, etc.

There may be an interest in making sure that these groups are not contained in the training and testing set since this may bias the test set performance to be more optimistic. Also, when one or more specific groups are held out, the resampling might capture the “ruggedness” of the model. In the example where clinical data is recorded over multiple sites, the resampling performance estimates partly measure how extensible the model is across sites.

To split the data based on groups, `groupKFold` can be used:
```{r}
set.seed(3527)
subjects <- sample(1:20,size=80,replace=TRUE)
table(subjects)
```

```{r}
folds <- groupKFold(subjects,k=15)
```

The results in `folds` can be used as an inputs into the `index` argument of the `trainControl` function. This plot shows how each subject is partitioned between the modeling and holdout sets. Note that since `k` was less than 20 when `folds` was created, there are some `k` was less than 20 when `folds` was created, there are some holdouts with model than one subject.

# Model training and tuning
http://topepo.github.io/caret/model-training-and-tuning.html

## Model Training and Parameter Tuning

The `caret` package has several functions that attempt to streamline the model building and evaluation process.

The `train` function can be used to 
- evaluate, using **resampling**, the effect of model tuning parameters on performance
- choose the optimal model across these parameters
- estimate model performance from a training set

First, a specific model must be chosen. Currently, 237 are available using `caret`; see `train` [model list](http://topepo.github.io/caret/available-models.html)  or `train` [Model by Tag](http://topepo.github.io/caret/train-models-by-tag.html) for details. 

On thrse pages, there are lists of tuning parameters that can potentially be optimized. User-defined models can also be created. 

The first step in tuning the model (line 1 in the algortihm below) is to choose a set of parameters to evaluate. For example, if fitting a Partial Least Squares (PLS) model, the number of PLS components to evaluate must be specified.

1. Define sets of model parapmeters to evaluate
2. **for** each parameter set **do** 
  3. **for** each resampling iteration **do**
  - 4. Hold-out specific samples
  - 5. [Optional] pre-process the data
  - 6. Fit the model on the remainder
  - 7. Predict the hold-out samples
8. *end*
9. Calculate the average performance across hold-out predictors
10. *end*
11. Determine the optimal parameter set
12. Fit the final model to all the training data using the optimal parameter set

Once the model and tuning parameter values have been defined, the type of resampling should be also specified, Currently, *k-fold* cross-validation (once or repeated), leave-one-out-cross validation and bootstrap (simple estimation or the 632 rule) resampling methods can be used by `train`. After resampling, the process produces a profile of performance is available to guide the user as to which tuning parameter values should be chosen. By default, the function automatically chooses the tuning parameters associated with the best value, although different algorithms can be used (see details below)

## An example
The Sonar data are available in the ,`mlbench` package. Here, we load the data:
```{r}
library(mlbench)
data(Sonar)
str(Sonar[,1:10])
```

The function `createDataPartition` can be used to create a stratified randome sample of the data into training and test sets:
```{r}
library(caret)
set.seed(998)

inTraining <- createDataPartition(Sonar$Class,p=.75,list=FALSE)
training <- Sonar[inTraining,]
testing <- Sonar[-inTraining,]
```

We will use these data illustrate functionality on this (and other) pages.

## Basic parameter tuning
By default, simple bootstrap resampling is used for line 3 in the algorithm above. Others are available, such as repeated K-fold cross-validation, leave-one-out etc. The function `trainControl` can be used to specifiy the type of resampling:
```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
```

More information about `trainControl` is given in a [section below](http://topepo.github.io/caret/model-training-and-tuning.html#custom).

The first two arguments to `train` are the predictor and outcome data objects, respectively. The third pargument `method` specifies the type of model (see [`train` model list](http://topepo.github.io/caret/available-models.html') or [`train` Models By Tag](http://topepo.github.io/caret/train-models-by-tag.html)) .
To illustrate we will fit a boosted tree moel via the `gbm` package. The basic syntax for fitting this model using repeated cross-validation is shown below:
```{r}
library(gbm)

head(training)

set.seed(825)
gbmFit1 <- train(Class~.,data=training,
                 method="gbm",
                 trControl=fitControl,
                 ## This last option is actualyy one
                 ## for gbm() that passes through
                 verbose=FALSE)

gbmFit1
gbmFit1 %>% plot()
```

For a gradient boosting machine (GBM) model, there are three main tuning parameters:
- number of iterations, i.e.,trees (called `n.trees` in the `gbm` function)
- complexity of the tree called `interaction.depth`
- learning rate: how quickly the algorithm adapts called `shrinkage`
- the minimum number of training set samples in a node to commence splitting (`n.minobsinnode`)

The default values tested for this model are shown in the first two columns (`shrinkage` and `n.minobsinnode` are not shown beause the grid set of candidate models all use a single value for these tuning parameters). The column labeled “`Accuracy`” is the overall agreement rate averaged over cross-validation iterations. The agreement standard deviation is also calculated from the cross-validation results. The column “`Kappa`” is Cohen’s (unweighted) Kappa statistic averaged across the resampling results. `train` works with specific models (see `train` Model List or `train Models` By Tag). For these models, `train` can automatically create a grid of tuning parameters. By default, if $p$ is the number of tuning parameters, the grid size is $3^p$. As another example, regularized discriminant analysis (RDA) models have two parameters (`gamma` and `lambda`), both of which lie between zero and one. The default training grid would produce nine combinations in this two-dimensional space.

There is additional functionality in `train` that is described in the next section.

## Notes on reproducibility
Many models utilize random numbers during the phase where parameters are estimated. Also, the resampling indices are chosen using random numbers. There are two main ways to control the randomness in order to assure reproducible results.

- There are two approaches to ensuring that the same *resamples* are used between calls to train. The first is to use `set.seed` just prior to calling `train`. The first use of random numbers is to create the resampling information. Alternatively, if you would like to use specific splits of the data, the index argument of the trainControl function can be used. This is briefly discussed below.

- When the models are created *inside of resampling*, the seeds can also be set. While setting the seed prior to calling train may guarantee that the same random numbers are used, this is unlikely to be the case when [parallel processing](http://topepo.github.io/caret/parallel-processing.html) is used (depending which technology is utilized). To set the model fitting seeds, `trainControl` has an additional argument called `seeds` that can be used. The value for this argument is a list of integer vectors that are used as seeds. The help page for `trainControl` describes the appropriate format for this option.

## 5.5 Customizing the Tuning Process

There are a few ways to customize the process of selecting tuning/complexity parameters and building the final model.

### 5.5.1 Pre-processing options
As previously mentioned,`train` can pre-process the data in various ways prior to model fitting. The function `preProcess` is automatically used. This function can be used for centering and scaling, imputation (see details below), applying the spatial sign transformation and feature extraction via principal component analysis or independent component analysis.

To specify what pre-processing should occur, the `train` function has an argument called `preProcess`. This argument takes a character string of methods that would normally be passed to the `method` argument of the `prePRocess` function. Additional options for the `preProcess` function can be passed via the `trainControl` function.

These processing steps would be applied during any predictions generated using `predict.train`, `extractPrediction` or `extractProbs` (see details later in this document). The pre-processing would not be applied to predictions that directly use the `boject$finalModel` object. 

For imputation, there are three methods currently implemented:

- **k-nearest neighbors** takes a sample with missing values and finds the k closest samples in the training set. The average of the k training set values for that predictor are used as a substitute for the original data. When calculating the distances to the training set samples, the predictors used in the calculation are the ones with no missing values for that sample and no missing values in the training set.

- another approach is to fit a **bagged tree model** for each predictor using the training set samples. This is usually a fairly accurate model and can handle missing values. When a predictor for a sample requires imputation, the values for the other predictors are fed through the bagged tree and the prediction is used as the new value. This model can have significant computational cost.

- the **median** of the predictor’s training set values can be used to estimate the missing data.

If there are missing values in the training set, PCA and ICA models only use complete samples.

### 5.5.2 Alternate Tuning Grids
The tuning parameter grid can be specified by the user. The argument `tuneGrid` can take a data frame with columns for each tuning parameter.The column names should be the same as the fitting function’s arguments. For the previously mentioned RDA example, the names would be `gamma` and `lambda`. `train` will tune the model over each combination of values in the rows.

For the boosted tree model, we can fix the learning rate and evaluate more than three values of `n.trees`:
```{r}
gbmGrid <- expand.grid(interaction.depth=c(1,5,9),
                       n.trees=(1:30)*50,
                       shrinkage=0.1,
                       n.minobsinnode=20)

nrow(gbmGrid)

set.seed(825)

gbmFit2 <- train(Class~., data=training,
                 method="gbm",
                 trControl=fitControl,
                 verbose=FALSE,
                 ## Now specify the exact models
                 ## to evaluate:
                 tuneGrid=gbmGrid
              )
gbmFit2
```

Another option is to use a random sample of possible tuning parameter combinations, i.e. “random search”(pdf). This functionality is described on [this page](http://topepo.github.io/caret/random-hyperparameter-search.html).

To use a random search, use the option `search = "random"` in the call to `trainControl`. In this situation, the `tuneLength` parameter defines the total number of parameter combinations that will be evaluated.

### Plotting the resampling profile

The `plot` function can be used to examine the relationship between the estimates of performance and the tuninng parameters. For example, a simple invokation of the function shows the results for the first performance measure:
2
```{r}
trellis.par.set(caretTheme())
plot(gbmFit2)
```

Other performance metrics can be shown using the `metric` option:
```{r}
trellis.par.set(caretTheme())
plot(gbmFit2, metric="Kappa")
```

Other types of plot are also available. See `?plot.train` for more details. The code below shows a heatmap of the results:
```{r}
trellis.par.set(caretTheme())
plot(gbmFit2, metric="Kappa",plotType="level",
     scales=list(x=list(rot=90)))
```

### The `trainControl` function
The function `trainControl` generates parameters that further control how models are created, with possible values.

- `method`:The resampling method: `"boot"`, `"cv"`, `"LOOCV"`, `"LGOCV"`, `"repeatedcv"`, `"timeslice"`, `"none"` and `"oob"`. The last value, out-of-bag estimates, can only be used by random forest, bagged trees, bagged earth, bagged flexible discriminant analysis, or conditional tree forest models. GBM models are not included (the `gbm` package maintainer has indicated that it would not be a good idea to choose tuning parameter values based on the model OOB error estimates with boosted trees). Also, for leave-one-out cross-validation, no uncertainty estimates are given for the resampled performance measures.

- `number` and `repeats`: `number` controls with the number of folds in K-fold cross-validation or number of resampling iterations for bootstrapping and leave-group-out cross-validation. repeats applied only to repeated $K-fold$ cross-validation. Suppose that `method = "repeatedcv"`, `number = 10` and `repeats = 3`,then three separate 10-fold cross-validations are used as the resampling scheme.
- `verboseIter`: A logical for printing a training log.
- `returnData`: A logical for saving the data into a slot called `trainingData`
- `p`: For leave-group out cross-validation: the training percentage
- For `method="timeslice`, `trainControl` has options `initialWindow`, `horizon` and `fixedWindow` that govern how cross-validation can be used for time series data.
- `classProbs`: a logical value determining whether class probabilities should be computed for held-out samples during resample.
- `index` and `indexOut`: optional lists with elements for each resampling iteration. Each list element is the sample rows used for training at that iteration or should be held-out. When these values are not specified, `train` will generate them.
- `summaryFunction`: a function to computed alternate performance summaries.
- `selectionFunction`: a function to choose the optimal tuning parameters. and examples.
- `PCAthresh`, `ICAcomp` and `k`: these are all options to pass to the preProcess function (when used).
- `returnResamp`: a character string containing one of the following values: "all", "final" or "none". This specifies how much of the resampled performance measures to save.
- `allowParallel`: a logical that governs whether train should use parallel processing (if availible).

### Alternatate performance metrics
The user can change the metric used to determine the best settings. By default, RMSE, $R^2$, and the mean absolute error (MAE) are computed for regression while accuracy and Kappa are computed for classification. Also by default, the parameter values are chosen using RMSE and accuracy, respectively for regression and classification. The `metric` argument of the `train` function allows the user to control which the optimality criterion is used. For example, in problems where there are a low percentage of samples in one class, using `metric = "Kappa"` can improve quality of the final model.

If none of these parameters are satisfactory, the user can also compute custom performance metrics. The `trainControl` function has a argument called `summaryFunction` that specifies a function for computing performance. The function should have these arguments:

- `data` is reference for a data frame or matrix with columns called `obs` and `pred` for the observed and predicted outcome values (either numeric data for regression or character values for classification). Currently, class probabilities are not passed to the function. The values in data are the held-out predictions (and their associated reference values) for a single combination of tuning parameters. If the classProbs argument of the trainControl object is set to TRUE, additional columns in data will be present that contains the class probabilities. The names of these columns are the same as the class levels. Also, if weights were specified in the call to train, a column called weights will also be in the data set. Additionally, if the recipe method for train was used (see this section of documentation), other variables not used in the model will also be included. This can be accomplished by adding a role in the recipe of `"performance var"`. An example is given in the recipe section of this site.)
- `lev` is a character string that has the outcome factor levels taken from the training data. For regression, a value of `NULL` is passed into the function.
- `model` is a character string for the model being used (i.e. the value passed to the method argument of `train`).

The output to the function should be a vector of numeric summary metrics with non-null names. By default, `train` evaluate classification models in terms of the predicted classes. Optionally, class probabilities can also be used to measure performance. To obtain predicted class probabilities within the resampling process, the argument `classProbs` in `trainControl` must be set to `TRUE`. This merges columns of probabilities into the predictions generated from each resample (there is a column per class and the column names are the class names).

As shown in the last section, custom fuctions can be used to calculate performance scores that are averaged over the **resamples**. Another built-in function, `twoClasssummary`, will compute the **sensitivity**, **specificity** and **are under the ROC curve**. 

```{r}
head(twoClassSummary)
```


To rebuild the boosted tree model using this criterion, we can see the relationship between the tuning parameters and the are under the ROC curve using the following code: 
```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)

set.seed(825)
gbmFit3 <- train(Class~., data = training,
                 method="gbm",
                 trControl=fitControl,
                 verbose=FALSE,
                 tuneGrid=gbmGrid,
                 ## specify which metric to optimize
                 metric="ROC"
                 )
gbmFit3
```

In this case, the average area under the ROC curve associated with the optimal tuning parameters was 0.914 across the 100 resamples.

## Choosing the Final Model
Another method for customizing the tuning process is to modify the algorithm that is used to select the `best` parameter values, given the performance numbers. By default, the `train` function chooses the model with the largest performance value (or smallest, for mean squared error in regression models).

Other schemes for selecting model can be used. Breiman et al (1984) suggested the “one standard error rule” for simple tree-based models. In this case, the model with the best performance value is identified and, using resampling, we can estimate the standard error of performance. The final model used was the simplest model within one standard error of the (empirically) best model. With simple trees this makes sense, since these models will start to over-fit as they become more and more specific to the training data.

`train` allows the user to specify alternate rules for selecting the final model. The argument `selectionFunction` can be used to supply a function to algorithmically determine the final model. There are three existing functions in the package: `best` is chooses the largest/smallest value, `oneSE` attempts to capture the spirit of Breiman et al (1984) and `tolerance` selects the least complex model within some percent tolerance of the best value. See `?best` for more details.

User-defined functions can be used, as long as they have the floowing arguments:
- `x` is a data frame containing the tune parameters and their associated performance metrics. Each row corresponds to a different tuning parameter combination.
- `metric`a character string indicating which performance metric should be optimized (this is passed in directly from the metric argument of `train`
- `maximize`is a single logical value indicating whether larger values of the performance metric are better (this is also directly passed from the call to `train`).

The function should output a single integer indicating which row in `x` is chosen.

As an example, if we chose the previous boosted tree model on the basis of overall accuracy, we would choose: n.trees = 250, interaction.depth = 9, shrinkage = 0.1, n.minobsinnode = 20. However, the scale in this plots is fairly tight, with accuracy values ranging from 0.872 to 0.914. A less complex model (e.g. fewer, more shallow trees) might also yield acceptable accuracy.

```{r}
whichTwoPct <- tolerance(gbmFit3$results,metric="ROC",
                         tol=2,maximize=T)
cat("best model within 2pct of best:\n")
```

```{r}
gbmFit3$results[whichTwoPct,1:6]
```

This indicates that we can get a less complex model with an area under the ROC curve of 0.901 (compared to the “pick the best” value of 0.914).

The main issue with these functions is related to ordering the models from simplest to complex. In some cases, this is easy (e.g. simple trees, partial least squares), but in cases such as this model, the ordering of models is subjective. For example, is a boosted tree model using 100 iterations and a tree depth of 2 more complex than one with 50 iterations and a depth of 8? The package makes some choices regarding the orderings. In the case of boosted trees, the package assumes that increasing the number of iterations adds complexity at a faster rate than increasing the tree depth, so models are ordered on the number of iterations then ordered with depth. See `?best` for more examples for specific models.

## Extracting predictions and class probabilities

As previously mentioned, objects produced by the `train` function contain the "optimized" model in the `finalModel` sub-object. Predictions can be made from these objects as usual. 

In some cases, such as `pls` or `gbm` objects, additional parameters from the optimized fit may beed to be specified. In these cases, the `train` objects uses the results of the parameter optimization to predict new samples. For example, if predictons were created using `predict.gbm`, the user would have to specify the number of trees directly (there is no default). Aoso, for binary classification, the predictions from this function take the form of the probability of one of the classes, so extra steps are required to convert this to a factor vector. `predict.train` automatically handles these details for this.

Also, there are very few standard syntaxes for model predictions in R. For example, to get class probabilities, many `predict` methods have an argument called `type` that is used to specify whether the classes or probabilities should be generated. Different packages use values of `type`, such as `prob`, `posterior`, `response`, `probability` or `raw`. In other cases, completely different syntax is used.

For `predict.train`, the type options are standardized to be "class" and "probs" (the underlying code matches these to the appropriate choices for each model). For example:
```{r}
predict(gbmFit3,newdata = head(testing))
```

```{r}
predict(gbmFit3,newdata = head(testing),type="prob")
```

## Exploring and comparing resampling distributions
### Within-model
There are several `lattice` functions that can be used to explore relationships between tuning parameters and the resampling results for a specific model:
- `xplot` and `stripplot` can be used to plot resampling statistics against (numeric) tuning parameters.
- `histogram` and `densityplot` can be used to look at distributions of the tuning parameters across tuning parameters.

For example, the following statement create a density plot:
```{r}
trellis.par.set(caretTheme())
densityplot(gbmFit3,pch="|")
```

Note that if you are interested in plotting the resampling results across multiple tuning parameters, the option `resamples = "all"` should be used in the control object.

### Between-models
The `caret` package also includes functions to characterize the differences between models(generated using `train`,`sbf` or `rfe`) via their resampling distributions. These functions are based on the work of [Hothorn et al. (2005)](https://homepage.boku.ac.at/leisch/papers/Hothorn+Leisch+Zeileis-2005.pdf) and [ and Eugster et al (2008).](https://epub.ub.uni-muenchen.de/10604/1/tr56.pdf).

First, a support vector machine model is to fit to the Sonar data. The data are centered and scaled using the `preProc` argument. Note that the same random number seed is set prior to the model that is identical to the seed used for the **boosted tree model**.  This ensures that the same resampling sets are used, which will come in handy when we compare the resampling profiles between models.
```{r}
set.seed(825)

# SVM with radial basis function kernel
svmFit <- train(Class~., data=training,
                method="svmRadial",
                trControl=fitControl,
                preProc=c("center","scale"),
                tuneLength=8,
                metric="ROC")
svmFit
```

Also, a regularized discriminant analysis model was fit.
```{r}

# Regularized discriminant analysis
set.seed(825)
rdaFit <- train(Class ~ ., data = training, 
                 method = "rda", 
                 trControl = fitControl, 
                 tuneLength = 4,
                 metric = "ROC")
rdaFit
```

Given these models, can we make statistical statements about their performance differences? To do this, we first collect the resampling results using `resamples`.
```{r}
resamps <- resamples(list(GBM=gbmFit3,
                          SVM=svmFit,
                          RDA=rdaFit))
resamps
```

```{r}
summary(resamps)
```

Note that, in this case, the option `resamples = "final"` should be user-defined in the control objects.

There are several lattice plot methods that can be used to visualize the resampling distributions: density plots, box-whisker plots, scatterplot matrices and scatterplots of summary statistics. For example:
```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col=rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)

bwplot(resamps, layout = c(3, 1))
```

```{r}
trellis.par.set(caretTheme())
dotplot(resamps,metric="ROC")
```

```{r}
trellis.par.set(theme1)
xyplot(resamps,what="BlandAltman")
```

```{r}
splom(resamps)
```

Other visualizations are available in `densityplot.resamples` and `parallel.resamples`.

Since models are fit on the same versions of the training data, it makes sense to make inferences on the differences between models. In this way we reduce the within-sample correlation that may exist. We can compute the differences, then use a simle t-test to evaluate the null hypothesis that there is no difference between models. 
```{r}
difValues <- diff(resamps)
difValues
```

```{r}
summary(difValues)
```

```{r}
trellis.par.set(theme1)
bwplot(difValues,layout=c(3,1))
```

```{r}
trellis.par.set(caretTheme())
dotplot(difValues)
```

## Fitting models without parameter tuning
In cases where the model tuning values are known, `train` can be used to fit the model to the entire training set without any resampling or parameter tuning. Using the `method = "none"` option in `trainControl` can be used. For example:
```{r}
fitControl <- trainControl(method="none",classProbs=T)

set.seed(825)
gbmFit4 <- train(Class ~ ., data = training, 
                 method = "gbm", 
                 trControl = fitControl, 
                 verbose = FALSE, 
                 ## Only a single model can be passed to the
                 ## function when no resampling is used:
                 tuneGrid = data.frame(interaction.depth = 4,
                                       n.trees = 100,
                                       shrinkage = .1,
                                       n.minobsinnode = 20),
                 metric = "ROC")
gbmFit4
```

Note that `plot.train`, `resamples`, `confusionMatrix.train` and several other functions will not work with this object but `predict.train` and others will:
```{r}
predict(gbmFit4, newdata=head(testing))
```

```{r}
predict(gbmFit4, newdata=head(testing),type="prob")
```

# Available models
The models below are available in `train`. The code behind these protocols can be obtained using the function getModelInfo or by going to the [github repository](https://github.com/topepo/caret/tree/master/models/files).

http://topepo.github.io/caret/available-models.html

# `Train` models by tag
The following is a basic list of model types or relevant characteristics. There entries in these list are arguable. For example: random forests theoretically use **feature selection** but effectively may not, support vector machines use L2 regulazation.

### Accepts case weights 
#### Adjacent categories probability model for ordinal data
```{r eval=FALSE}
  method='vglmAdjCat'
```

Type:Type: Classification

Tuning parameters:
- `parallel` (Parallel Curves)
- `link` (Link Function)
Required packages: `VGAM`

#### Bagged CART
```{r eval=FALSE}
  method = 'treebag'
```

Type: Regression, Classification
No tuning parameters for this model
Required packages: `ipred`, `plyr`, `e1071`
A model-specific variable importance metric is available.

#### Bagged Flexible Discriminant Analysis
```{r eval=FALSE}
  method = 'bagFDA'
```

Type: Classification
Tuning parameters:
- `degree` (Product Degree)
- `nprune` (#Terms)
Required packages: `earth`, `mda`

A model-specific variable importance metric is available. Notes: Unlike other packages used by train, the earth package is fully loaded when this model is used.

#### Bagged MARS
```{r eval=FALSE}
  method='bagEarth'
```


Type: Regression, Classification

Tuning parameters:
- `nprune` (#Terms)
- `degree` (Product Degree)

Required packages: `earth`

A model-specific variable importance metric is available. Notes: Unlike other packages used by `train`, the `earth` package is fully loaded when this model is used.

#### Bagged MARS using gCV Pruning
```{r eval=FALSE}
  method = 'bagEarthGCV'
```

Type: Regression, Classification

Tuning parameters:
- degree (Product Degree)
- Required packages: `earth`

A model-specific variable importance metric is available. Notes: Unlike other packages used by `train`, the `earth` package is fully loaded when this model is used.

#### Bayesian Generalized Linear Model
```{r eval=FALSE}
  method='bayesglm'
```

Type: Regression, Classification
No tuning parameters for this model
Required packages: `arm`

http://topepo.github.io/caret/train-models-by-tag.html#Bagging


# Parallel Processing
In this package, resampling is promary approach for optimizing predictive models with tuninng parameters. To do this, many alternate versions of the training set are used to train the model and predict a hold-out set. This process is repeated many times to get performance estimates that generalize to new data sets.

Each of the resampled data sets is independent of the others, so there is no formal requirement that the models must be run sequentially. If a computer with multiple processor or cores is available, the computations could be spread across these "workers" to increase the computational efficiency. `caret` leverages one of the parallel processing frameworks in R to do just this.

The `foreach` package allows R code to be run either sequentially or in parallel using several different technologies, such as the `multicore` or `Rmpi` packages (see Schmidberger et al, 2009 for summaries and descriptions of the available options). There are several R packages that work with foreach to implement these techniques, such as `doMC` (for `multicore`) or `doMPI` (for `Rmpi`).

A fairly comprehensive study of the benefits of parallel processing can be found in [this blog post](http://appliedpredictivemodeling.com/blog/2018/1/17/parallel-processing). 

To tune a predictive model using multiple workers, the function syntax in the `caret` package functions (e.g., `train`, `rfe` or `sbf`) do not change. A separate function is used to "register" the parallel processing technique and specify the number of workers to use. For example, to use the `doParallel` package with finve cores on the same machine, the package is loaded and then registed:
```{r eval=FALSE}
# install.packages("doParallel")
library(doParallel)

cl <- makePSOCKcluster(5)
registerDoParallel(cl)

## ALLsubsequent models are then run in parallel
model <- train(y~.,data=training,method="rf")

## When you are done:
stopCluster(cl)
```

The syntax for other packages associated with `foreach` is very similar. Note that as the number of workers increases, the memory required also increase. For example, using five workers would keep a total of six versions of the data in memory. If the data are large or the computational model is demanding, performance can be affected if the amount of required memory exceeds the physical amount available. Also, for `rfe` and `sbf`, these functions may call train for some models. In this case, registering $M$ workers will actually invoke $M^2$ total processes.

Does this help reduce the time to fit models? A moderately sized data set (4331 rows and 8) was modeled multiple times with different number of workers for several models. Random forest was used with 2000 trees and tuned over 10 values of $m_{try}$. Variable importance calculations were also conducted during each model fit. 

Linear discriminant analysis was also run, as was a cost-sensitive radial basis function support vector machine (tuned over 15 cost values). All models were tuned using five repeats of 10-fold cross-validation. The results are shown in the figure below. The y-axis corresponds to the total execution time (encompassing model tuning and the final model fit) versus the number of workers. Random forest clearly took the longest to train and the LDA models were very computationally efficient. 

The total time (in minutes) decreased as the number of workers increase but stabilized around seven workers. The data for this plot were generated in a randomized fashion so that there should be no bias in the run order. The bottom right panel shows the speed-up which is the sequential time divided by the parallel time. For example, a speed-up of three indicates that the parallel version was three times faster than the sequential version. At best, parallelization can achieve linear speed-ups; that is, for M workers, the parallel time is 1/M. For these models, the speed-up is close to linear until four or five workers are used. After this, there is a small improvement in performance. Since LDA is already computationally efficient, the speed-up levels off more rapidly than the other models. While not linear, the decrease in execution time is helpful - a nearly 10 hour model fit was decreased to about 90 minutes.

Note that some models, especially those using the `RWeka` package, may not be able to be run in parallel due to the underlying code structure.

`train`, `rfe`, `sbf`, `bag` and `avNNet` were given an additional argument in their respective control files called `allowParallel` that defaults to `TRUE`. When `TRUE`, the code will be executed in parallel if a parallel backend (e.g. doMC) is registered. When `allowParallel = FALSE`, the parallel backend is always ignored. The use case is when `rfe` or `sbf` calls `train`. If a parallel backend with P processors is being used, the combination of these functions will create P2 processes. Since some operations benefit more from parallelization than others, the user has the ability to concentrate computing resources for specific functions.

One additional “trick” that `train` exploits to increase computational efficiency is to use sub-models; a single model fit can produce predictions for multiple tuning parameters. For example, in most implementations of boosted models, a model trained on B boosting iterations can produce predictions for models for iterations less than B. Suppose a `gbm` model was tuned over the following grid
```{r eval=FALSE}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:15)*100,
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
```

In reality, train only created objects for 3 models and derived the other predictions from these objects. This trick is used for the following models: 
`ada`, `AdaBag`, `AdaBoost.M1`, `bagEarth`, `blackboost`, `blasso`, `BstLm`, `bstSm`, `bstTree`, `C5.0`, `C5.0Cost`, `cubist`, `earth`, `enet`, `foba`, `gamboost`, `gbm`, `glmboost`, `glmnet`, `kernelpls`, `lars`, `lars2`, `lasso`, `lda2`, `leapBackward`, `leapForward`, `leapSeq`, `LogitBoost`, `pam`, `partDSA`, `pcr`, `PenalizedLDA`, `pls`, `relaxo`, `rfRules`, `rotationForest`, `rotationForestCp`, `rpart`, `rpart2`, `rpartCost`, `simpls`, `spikeslab`, `superpc`, `widekernelpls`, `xgbDART`, `xgbTree`.

# Random hyperparameter search
The default method for optimizing tuning parameters in `train` is to use a [grid search](http://topepo.github.io/caret/model-training-and-tuning.html#grids). This approach is usually effective but, in cases when there are many tuning parameters, it can be inefficient. An alternative is to use a combination of grid search and racing. Another is to use a random selection of tuning parameter combinations to cover the parameter space to a lesser extent.

There are a number of models where this can be beneficial in finding reasonable values of the tuning parameters in a relatively short time. However, there are  some models where the efficiency in a small search field can cancel out other optimizations. For example, a number of models in caret utilize the “sub-model trick” where $M$  tuning parameter combinations are evaluated, potentially far fewer than M model fits are required. This approach is best leveraged when a simple grid search is used. For this reason, it may be inefficient to use random search for the following model codes: 
`ada`, `AdaBag`, `AdaBoost.M1`, `bagEarth`, `blackboost`, `blasso`, `BstLm`, `bstSm`, `bstTree`, `C5.0`, `C5.0Cost`, `cubist`, `earth`, `enet`, `foba`, `gamboost`, `gbm`, `glmboost`, `glmnet`, `kernelpls`, `lars`, `lars2`, `lasso`, `lda2`, `leapBackward`, `leapForward`, `leapSeq`, `LogitBoost`, `pam`, `partDSA`, `pcr`, `PenalizedLDA`, `pls`, `relaxo`, `rfRules`, `rotationForest`, `rotationForestCp`, `rpart`, `rpart2`, `rpartCost`, `simpls`, `spikeslab`, `superpc`, `widekernelpls`, `xgbDART`, `xgbTree`.

Finally, many of the models wrapped by `train` have a small number of parameters. The average number of parameters is 2.

To use random search, another option is available in `trainControl` called `search`. Possible values of this argument are `"grid"` and `"random"`. The built-in models contained in caret contain code to generate random tuning parameter combinations. The total number of unique combinations is specified by the `tuneLength` option to `train`.

Again, we will use the sonar data from the previous training page to demonstrate the method with a regularized discriminant analysis by looking at a total of 30 tuning parameters combinations:
```{r}
library(mlbench)
data(Sonar)

library(caret)
set.seed(998)

inTraining <- createDataPartition(Sonar$Class,p=.75,list=FALSE)
training <- Sonar[inTraining,]
testing <- Sonar[-inTraining,]

fitControl <- trainControl(method="repeatedcv",
                           number=10,
                           repeats=10,
                           classProbs=T,
                           summaryFunction = twoClassSummary,
                           search="random"
                           )

set.seed(825)
rda_fit <- train(Class~.,data=training,
                 method="rda",
                 metric="ROC",
                 tuneLength=30,
                 trControl=fitControl)
rda_fit

```

There is currently only a `ggplot` method (instead of a basic `plot` method). The results of this function with random searching depends on the number and type of tuning parameters. In this case, it produces a scatter plot of the continuous parameters.
```{r}
ggplot(rda_fit)+
  theme(legend.position = "top")
```

# Subsampling for class imbalances

In classification problems, a disparity in the frequencies of the observed classes can have a significant negative impact on model fitting. One technique for resolving such a class imbalance is to subsample the training data in a manner that mitigates the issues. Examples of sampling methods for this purpose are:

- *down-sampling*: randomly subset all the classes in the training set so that their class frequencies match the least prevalent class. For example, suppose that 80% of the training set samples are the first class and the remaining 20% are in the second class. Down-sampling would randomly sample the first class to be the same size as the second class (so that only 40% of the total training set is used to fit the model). **caret** contains a function (`downSample`) to do this.

- *up-sampling*: randomly sample (with replacement) the minority class to be the same size as the majority class. caret contains a function (`upSample`) to do this.

- *hybrid methods*: techniques such as SMOTE and ROSE down-sample the majority class and synthesize new data points in the minority class. There are two packages (**DMwR** and **ROSE*) that implement these procedures.

Note that this type of sampling is different from splitting the data into a training and test set. You would never want to artificially balance the test set; its class frequencies should be in-line with what one would see “in the wild”. Also, the above procedures are independent of resampling methods such as cross-validation and the bootstrap.

In practice, one could take the training set and, before model fitting, sample the data. There are two issues with this approach

- Firstly, during model tuning the holdout samples generated during resampling are also glanced and may not reflect the class imbalance that future predictions would encounter. This is likely to lead to overly optimistic estimates of performance.

- Secondly, the subsampling process will probably induce more model uncertainty. Would the model results differ under a different subsample? As above, the resampling statistics are more likely to make the model appear more effective than it actually is.

The alternative is to include the subsampling inside of the usual resampling procedure. This is also advocated for pre-process and featur selection steps too. The two disadvantages are that it might increase computational times and that it might also complicate the analysis in other ways (see the section [below](http://topepo.github.io/caret/subsampling-for-class-imbalances.html#complications) about the pitfalls).

## Subsampling techniques
To illustrate these mothods, let's simulate some data within a class inbalance using this method. We will stimulate a training set where each contains 10000 samples and a minority class rate of about 5.9%:
```{r}
library(caret)

set.seed(2969)
imbal_train <- twoClassSim(10000,intercept=-20,linearVars = 20)
imbal_test <- twoClassSim(10000,intercept=-20,linearVars = 20)
table(imbal_train$Class)
```

Let's create different versions of the training set prior to model tuning:
```{r}
set.seed(9560)
down_train <- downSample(x=imbal_train[,-ncol(imbal_train)],
                         y=imbal_train$Class)
table(down_train$Class)
```

```{r}
set.seed(9560)
up_train <- upSample(x=imbal_train[,-ncol(imbal_train)],
                     y=imbal_train$Class)
table(up_train$Class)
```

```{r}
# install.packages("DMwR")
library(DMwR)

set.seed(9560)
smote_train <- SMOTE(Class~., data=imbal_train)
table(smote_train$Class)
```

```{r}
# install.packages("ROSE")
library(ROSE)

set.seed(9560)
rose_train <- ROSE(Class~., data=imbal_train)$data
table(rose_train$Class)
```

For these data, we’ll use a bagged classification and estimate `the area under the ROC curve` using five repeats of `10-fold CV`.

```{r eval=FALSE}
ctrl <- trainControl(method="repeatedcv",repeats=5,
                     classProbs=T,
                     summaryFunction=twoClassSummary)

set.seed(5627)
orig_fit <- train(Class~., data=imbal_train,
                  method="treebag",
                  nbagg=50,
                  metric="ROC",
                  trControl=ctrl)


set.seed(5627)
down_outside <- train(Class~., data=down_train,
                      method="treebag",
                      nbagg=50,
                      metric="ROC",
                      trControl=ctrl)

set.seed(5627)
up_outside <- train(Class ~ ., data = up_train, 
                    method = "treebag",
                    nbagg = 50,
                    metric = "ROC",
                    trControl = ctrl)

set.seed(5627)
rose_outside <- train(Class ~ ., data = rose_train, 
                      method = "treebag",
                      nbagg = 50,
                      metric = "ROC",
                      trControl = ctrl)

set.seed(5627)
smote_outside <- train(Class ~ ., data = smote_train, 
                       method = "treebag",
                       nbagg = 50,
                       metric = "ROC",
                       trControl = ctrl)
```

We will collate the resampling results and create a wrapper to estimate the test set performance:
```{r eval=FALSE}
outside_models <- list(original=orig_fit,
                       down=down_outside,
                       up=up_outside,
                       SMOTE=smote_outside,
                       ROSE=rose_outside)

outside_resampling <- resamples(outside_models)

test_rec <- function(model,data){
  library(pROC)
  roc_obj <- roc(data$Class, 
                 predict(model, data, type = "prob")[, "Class1"],
                 levels = c("Class2", "Class1"))
  ci(roc_obj)
}

outside_test <- lapply(outside_models, test_roc, data = imbal_test)
outside_test <- lapply(outside_test, as.vector)
outside_test <- do.call("rbind", outside_test)
colnames(outside_test) <- c("lower", "ROC", "upper")
outside_test <- as.data.frame(outside_test)

summary(outside_resampling, metric = "ROC")
```

```{r eval=FALSE}
outside_test
```

The training and test set estimates for the area under the ROC curve do not appear to correlate. Based on the resampling results, one would infer that up-sampling is nearly perfect and that ROSE does relatively poorly. The reason that up-sampling appears to perform so well is that the samples in the majority class are replicated and have a large potential to be in both the model building and hold-out sets. In essence, the hold-outs here are not truly independent samples.

In reality, all of the sampling methods do about the same (based on the test set). The statistics for the basic model fit with no sampling are fairly in-line with one another (0.939 via resampling and 0.922 for the test set).

## Subsampling during resampling
Recent versions of **caret** allow the user to specify subsampling when using `train` so that it is conducted inside of resampling. All four methods shown above can be accessed with the basic package using simple syntax. If you want to use your own technique, or want to change some of the parameters for SMOTE or ROSE, the last section below shows how to use custom subsampling.

The way to enable subsampling is to use yet another option in `trainControl` called `sampling`. The most basic syntax is to use a character string with the name of the sampling method, either `"down"`, `"up"`, `"smote"`, or `"rose"`. Note that you will need to have the DMwR and ROSE packages installed to use **SMOTE** and **ROSE**, respectively.

One complication is related to pre-processing. Should the subsampling occur before or after the pre-processing? or example, if you down-sample the data and using PCA for signal extraction, should the loadings be estimated from the entire training set? The estimate is potentially better since the entire training set is being used but the subsample may happen to capture a small potion of the PCA space. There isn’t any obvious answer.

The default behavior is to subsample the data prior to pre-processing. This can be easily changed and an example is given below.

Now let’s re-run our bagged tree models while sampling inside of cross-validation:
```{r eval=FALSE}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     ## new option here:
                     sampling = "down")

set.seed(5627)
down_inside <- train(Class ~ ., data = imbal_train,
                     method = "treebag",
                     nbagg = 50,
                     metric = "ROC",
                     trControl = ctrl)

## now just change that option
ctrl$sampling <- "up"

set.seed(5627)
up_inside <- train(Class ~ ., data = imbal_train,
                   method = "treebag",
                   nbagg = 50,
                   metric = "ROC",
                   trControl = ctrl)

ctrl$sampling <- "rose"

set.seed(5627)
rose_inside <- train(Class ~ ., data = imbal_train,
                     method = "treebag",
                     nbagg = 50,
                     metric = "ROC",
                     trControl = ctrl)

ctrl$sampling <- "smote"

set.seed(5627)
smote_inside <- train(Class ~ ., data = imbal_train,
                      method = "treebag",
                      nbagg = 50,
                      metric = "ROC",
                      trControl = ctrl)
```

Here are the resampling and test set results:
```{r eval=FALSE}
inside_models <- list(
  original=orig_fit,
  down=down_inside,
  up=up_inside,
  SMOTE=smote_inside,
  ROSE=rose_inside
)
inside_test <- lapply(inside_models,test_roc,data=imbal_test)
inside_test <- lapply(inside_test,as.vector)
inside_test <- do.call("rbind",inside_test)
colnames(inside_test) <- c("lower","ROC","upper")

inside_test <- as.data.frame(inside_test)
summary(inside_resampling,metric="ROC")
```

```{r eval=FALSE}
inside_test
```

The figure below shows the difference in the area under the ROC curve and the test set results for the approaches shown here. Repeating the subsampling procedures for every resample produces results that are more consistent with the test set.

## Complications
The user should be aware that there are a few things that can happening when subsampling that can cause issues in their code. As previously mentioned, when sampling occurs in relation to pre-processing is one such issue. Others are:
- Sparsely represented categories in factor variables may turn into zero-variance predictors or may be completely sampled out of the model.
- The underlying functions that do the sampling (e.g. `SMOTE`, `downSample`, etc) operate in very different ways and this can affect your results. For example, `SMOTE` and `ROSE` will convert your predictor input argument into a data frame (even if you start with a matrix).
- Currently, sample weights are not supported with sub-sampling.
- If you use `tuneLength` to specify the search grid, understand that the data that is used to determine the grid has not been sampled. In most cases, this will not matter but if the grid creation process is affected by the sample size, you may end up using a sub-optimal tuning grid.
- For some models that require more samples than parameters, a reduction in the sample size may prevent you from being able to fit the model.

## Using custom subsampling techniques
Users have the ability to create their own type of subsampling procedure. To do this, alternative syntax is used with the `sampling` argument of the `trainControl`. Previously, we used a simple string as the value of this argument. Another way to specify the argument is to use a list with three (named) elements:

- The `name` value is a character string used when the train object is printed. It can be any string.
- The `func` element is a function that does the subsampling. It should have arguments called x and y that will contain the predictors and outcome data, respectively. The function should return a list with elements of the same name.
- The `first` element is a single logical value that indicates whether the subsampling should occur first relative to pre-process. A value of FALSE means that the subsampling function will receive the sampled versions of `x` and `y`. 

For example, here is what the list version of the `sampling` argument looks like when simple down-sampling is used:
```{r eval=FALSE}
down_inside$control$sampling
```

As another example, suppose we want to use SMOTE but use 10 nearest neighbors instead of the default of 5. To do this, we can create a simple wrapper around the `SMOTE` function and call this instead:
```{r}
smotest <- list(name = "SMOTE with more neighbors!",
                func = function (x, y) {
                  library(DMwR)
                  dat <- if (is.data.frame(x)) x else as.data.frame(x)
                  dat$.y <- y
                  dat <- SMOTE(.y ~ ., data = dat, k = 10)
                  list(x = dat[, !grepl(".y", colnames(dat), fixed = TRUE)], 
                       y = dat$.y)
                  },
                first = TRUE)
```

The control object would then be:
```{r}
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary,
                     sampling = smotest)
```

# Using Recipes with train

Modeling functions in R let you specifc a model using a formula, the `x/y` interface, or both. Formulas are good because they will handle a lot of minutia for your (e.g., dummy variables, interaction etc.) so you don't have to get your hands dirty.

They work [pretty well](https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/), but  also [have limitations](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/),Their biggest issue is that not all modeling functions have a formula interface (although train helps solve that).

`Recipes` are a third method for specifying model terms but also allow for a broad set of preprocessing options for encoding, manupulating, and transforming data. They cover a lot of techniques that formulas cannnot naturally. 

`Recipes` can be built incrementally in a way similar to how `dplyr` or `ggplot2` are created. The package website has example of how to use the package and lists the possible techniques (called **steps**). A recipe can then be handed to `train` in lieu of a formula. 

## Why should you learn this?
Here are two reasons.

### More versatile toos for preprocessing data

`caret`'S preprocessing tools have a lot of options but the list is not exhaustive and they will only be called in a specific order. If you would lik
- a broader set of options,
- the ability to write your own preprocessing tools, or 
- to call them in the order that you desire

Then you can use a recipe to do that.

### Using additional data to measure performance
In most modeling functions, including `train`, most variables are consigned to be either predictors or outcomes. For `recipes`, you might want to have specific columns of your data set be available when you compute how well the model is performing, such as:

- if different stratification variables(e.g., patients, ZIP codes etc.) are required to do correct summaries or;
- ancillary data might be needed to compute the expected profit or loss based on the model results

To get these data properly, they need to be made available and handled the same way as all of the other data. This means they should be sub- or **resampled** as all of the other data. Recipes let your do that.

## An example

The `QSARdata` package contains several chemistry data sets. These data sets have rows for different pontential drugs(called "compounds" here). For each compound, some important characteristic is measured. This illustration will use the `AquaticTox` data. The outcome is called **"Acticvity"** is a measure of how harmful the compound might be people. We want to predict this during the drug discovery phase in R&D . To do this, a set of *molecular descritors* are computed based on the compounds formula. There are a lot of different types of these, and we will use the 2-dimensional MOE descriptor set. First let's load the package and get the data together:
```{r}
library(caret)
library(recipes)
library(dplyr)
library(QSARdata)

data(AquaticTox)
tox <- AquaticTox_moe2D
ncol(tox)

# Add the outcome variable to the data frame
tox$Activity <- AquaticTox_Outcome$Activity

tox
```

We will build a model on these data to predict the activity. Some notes:
- A common aspect to chemical descriptors is that they are *highly correlated*. Many descriptors often measure some variation of the same thing. For example, in these data, there are 56 potential predictors that measure different flavors of surface area. It might be a good idea to *reduce the dimensionality* of these data by pre-filtering the predictors and/or using a dimension reduction technique.
- Other descriptors are counts of certain types of aspects of the molecule. For example, one predictor is the number of Bromine atoms. The vast majority of compounds lack Bromine and this leads to a near-zero variance situation discussed previously. It might be a good idea to pre-filter these.

Also , to demonstrate the utility of recipes, suppose that we could score potential drugs on the basis of how manufacturable they might be. We might eant to build a model on the entire data set but only evaluate it on compounds that could be reasonably manufactured. For illustration, we'lll assume that, as a compounds molecule weight increases, its manufacturability *dercreases*. For this purpose, we create a new variable (`manufacturability`) that is neither an outcome or predictor but will be needed to compute performance. 

```{r}
tox <- tox %>%
  select(-Molecule) %>%
  ## Suppose the easy of manufacturability is 
  ## related to the molecular weight of the compound
  mutate(manufacturability  = 1/moe2D_Weight) %>%
  mutate(manufacturability = manufacturability/sum(manufacturability))

tox
```

For this analysis, we will compute the RMSE using weights based on the manufacturability columns such that a difficult compound has less impact on the RMSE. 
```{r}
model_stats <- function(data,lev=NULL, model=NULL){
  stats <- defaultSummary(data,lev=lev,model=model)
  
  wt_rmse <- function(pred,obs,wts,na.rm=T)
    sqrt(weighted.mean(pred-obs)^2, wts, na.rm=na.rm)
  
  res <- wt_rmse(pred=data$pred,
                 obs=data$obs,
                 wts=data$manufacturability)
  c(wRMSE=res,stats)
}
```


There is no way to include this extra variable using the default `train` method or using `train.formula`. 

Now let's create a recipe incrementally. First, we will use the formula methods to declare the outcome and predictors but change the analysis role of the `manufacturability` variabile so that it will only be available when summarizing the model fit. 
```{r}
tox_recipe <- recipe(Activity~., data=tox) %>% 
  add_role(manufacturability, new_role="performance var")

tox_recipe
```

Using this new role, the `manufacturability` column will be available when the summary function is executed and the appropriate rows of the data set will be exposed during resampling. For example, if one were debug the `model_stats` function during of a model, the `data` object might look like this:
```{r eval=FALSE}
Browse[1]> head(data)
```

More than one variable can have this role so that multiple columns can be made available. Now let's add some steps to the recipe. First, we remove sparse and unbalanced predictors:
```{r}
tox_recipe <- tox_recipe %>% 
  step_nzv(all_predictors())
tox_recipe
```

Note that we have only specified what will *happen once the recipe* is executed. This is only a specification that uses a generic declaration of `all_predictors`.

As mentioned above, there are a lot of different surface area predictors and they tend to have very high correlations with one another. We’ll add one or more predictors to the model in place of these predictors using principal component analysis. The step will retain the number of components required to capture 95% of the information contained in these 56 predictors. We’ll name these new predictors `surf_area_1`, `surf_area_2` etc.

```{r}
tox_recipe <- tox_recipe %>% 
  step_pca(contains("VSA"),prefix="surf_area_",threshold=.95)
```

Now, lets specific that the third step in the recipe is to reduce the number of predictors so that no pair has an absolute correlation greater than 0.90. However, we might want to keep the surface are principal components so we **exclude** thse from the filter (using the minus sign)

```{r}
tox_recipe <- tox_recipe %>% 
  step_corr(all_predictors(),-starts_with("surf_area_"),threshold=.90)
```

Finally, we can center and scale all of the predictors that are available at the end of the recipe:
```{r}
tox_recipe <- tox_recipe %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
tox_recipe
```

Let's use this recipe to fit a SVM model and pick the tuning parameters that minimize the weighted RMSE value:
```{r}
tox_ctrl <- trainControl(method = "cv", summaryFunction = model_stats)
set.seed(888)
tox_svm <- train(tox_recipe, tox,
                 method = "svmRadial", 
                 metric = "wRMSE",
                 maximize = FALSE,
                 tuneLength = 10,
                 trControl = tox_ctrl)
tox_svm
```

What variables were generated by the recipe?
```{r}
## originally:
ncol(tox)-2
```

```{r}
## after the recipe was executed:
predictors(tox_svm)
```

The trained recipe is available in the `train` object and now shows specific variables involved in each step:
```{r}
tox_svm$recipe
```


## Case weights
For models that accept them, case weights can be passed to the model fitting routines using a role of `"case weight"`.

# Using your own model in `train`
The package contains a large number of predictive model interfaces. However, you may want to create your own because: 

- you are testing out a novel model or the package doesn’t have a model that you are interested in
- you would like to run an existing model in the package your own way
- there are **pre-processing** or **sampling steps** not contained in the package or you just don’t like the way the package does things

You can still get the benefits of the `caret` infrastructure by creating your own model.

Currently, hen you specify the type of model that you are interested in (e.g. `type = "lda"`), the `train` function runs another function called `getModelInfo` to retrieve the specifics of that model from the existing catalog. For example:
```{r}
ldaModelInfo <- getModelInfo(model="lda",regex=FALSE)[[1]]
## Model components
names(ldaModelInfo)
```

To use your own model, you can pass a list of these components to `type`. This page will describe those components in detail.

## Illustrative example 1: SVMs with Laplacian Kernels
The package currently contains support vector machine (SVM) models using linear, polynomial and radial basis function kernels. The `kernlab` package has other functions, including the Laplacian Kernel. We will illustrate the model components for this model, which has two parameters: the standard cost parameter for SVMs and one kernel parameter (`sigma`)

## Model Components

You can pass a list of information to the `method` argument in `train`. For models that are built-in to the package, you can just pass the method name as before. 

There are some basic components of the list for custom models. A brief description is below for each then, after setting up and example, each will be described in detail. The list should have the following elements:

- `library` is a character vector of package names that will be needed to fit the model or calculate predictions. NULL can also be used.
- `type` is a simple character vector with values `"Classification"`, `"Regression"` or both.
- `parameters` is a data frame with three simple attributes for each tuning parameter (if any): the argument name (e.g. mtry), the type of data in the parameter grid and textual labels for the parameter.
- `grid` is a function that is used to create the tuning grid (unless the user gives the exact values of the parameters via tuneGrid)
fit is a function that fits the model
- `predict` is the function that creates predictions
- `prob` is a function that can be used to create class probabilities (if applicable)
sort is a function that sorts the parameter from most complex to least
- `loop` is an **optional** function for advanced users for models that can create multiple submodel predictions from the same object.
- `levels` is an **optional** function, primarily for classification models using S4 methods to return the factor levels of the outcome.
- `tags` is an **optional** character vector that has subjects associated with the model, such as `Tree-Based Model` or `Embedded Feature Selection`. This string is used by the package to create additional documentation pages on the package website.
label is an optional character string that names the model (e.g. “Linear Discriminant Analysis”).
- `predictors` is an **optional** function that returns a character vector that contains the names of the predictors that we used in the prediction equation.
- `varImp` is an **optional** function that calculates variable importance metrics for the model (if any).
- `oob` is another **optional** function that calculates out-of-bag performance estimates from the model object. Most models do not have this capability but some (e.g. random forests, bagged models) do.
- `notes` is an **optional** character vector that can be used to document non-obvious aspects of the model. For example, there are two Bayesian lasso models (`blasso` and `blassoAveraged`) and this field is used to describe the differences between the two models.
- `check` is an **optional** function that can be used to check the system/install to make sure that any atypical software requirements are available to the user. The input is pkg, which is the same character string given by the `library`. This function is run after the checking function to see if the packages specified in library are installed. As an example, the model `pythonKnnReg` uses certain python libraries and the user should have python and these libraries installed. The model file demonstrates how to check for python libraries prior to running the R model.

In the `caret` package, the subdirectory `models` has all the code for each model that `train` interfaces with and these can be used as prototypes for your model.

Let's create a new model for a classification support vector machine using the Laplacian kernel function. We will use the `kernlab` package's `ksvm` function. The kernel has two parameters: the standard cost parameter for SVMs and one kernel parameter (`sigma`). 

To start, we'll create a new list:
```{r}
lpSVM <- list(type="Classification",
              library="kernlab",
              loop=NULL)
```

This model can also be used to regression, but we will constratin things here for simplificity. For other SVM models, the type value would be `c("Classification", "Regression")`. 

The `library` value checks to see if this package is installed and loads it whenever it is needed (e.g., before modeling or prediction). **Note:** `caret` will check to see if these packages are installed but will *not* explicitly load them. As such unctions that are used from the package should be referenced by namespace. This is discussed more below when describing the `fit` function.

### The parameters element
We have to create some basic information for the parameters in the form of a data frame. The first column is the name of the parameter. The convention is to use the argument name in the model function (e.g. the `ksvm` function here). Those values are `C` and `sigma`. Each is a number and we can give them labels of `"Cost"` and `"Sigma"`, respectively. The parameters element would then be:

```{r}
prm <- data.frame(parameter=c("C","Sigma"),
                  class=rep("numeric",2),
                  label=c("Cost","Sigma"))
```

Now, we assign it to the model list:
```{r}
lpSVM$parameters <- prm
summary(lpSVM)
```

Values of `type` can indicate numeric, character or logical data types.

### The `grid` Element
This should be a function that takes parameters: `x` and `y` (for the predictors and outcome data), `len` (the number of values per tuning parameter) as well as `search`. `len` is the value of `tuneLength` that is potentially passed in through `train`. `search` can be either `"grid"` or `"random"`. This can be used to setup a grid for searching or random values for random search.

The output should be a data frame of tuning parameter combinations with a column for each parameter. The column names should be the parameter name (e.g. the values of prm$parameter). In our case, let’s vary the cost parameter on the log 2 scale. For the sigma parameter, we can use the kernlab function sigest to pre-estimate the value. Following ksvm we take the average of the low and high estimates. Here is a function we could use:

```{r}
svmGrid <- function(x,y,len=NULL, search="grid"){
  library(kernlab)
  ## This produces low, middle and high value for sigma
  ## i.e. a vector with 3 elements
  sigmas <- kernlabb::sigest(as.matrix(x),na.action=na.omit, scaled=TRUE)
  ## To use a grid search
  if (search =="grid"){
    out <- expand.grid(sigma=mean(as.vector(sigmas[-2])),
                       C=2^(1:len)-3)
  }else{
    ## For random search, define ranges for the parameters then
    ## generate random values for them
    rng <- extendrange(log(sigmas),f=.75)
    out <- data.frame(sigma=exp(runif(len,min=rng[1],max=rng[2])),
                      C=2^runif(len,min=-5,max=8))
  }
  out
}
```

Why did we use `kernelab::sigest` instead of `sigest`? As previously mentioned, `caret` will not execute `library(kernlab)` unless you explicitly code it in these functions. Since it not explicityl loaded, you have to call it *using the namespace operator* `::`.

Again, the user can pass their own grid via `train`'s `tuneGrid` option or they can use this code  to create a default grid. We assign this function to the overall model list:
```{r}
lpSVM$grid <- svmGrid
```

### The `fit` Element
Here is where we fit the model. This `fit` function has several arguments:
- `x`, `y`: the current data used to fit the model
- `wts`: optional instance weights (not applicable for this particular model)
- `param`: the current tuning parameter values
- `lev`: the class levels of the outcome (or NULL in regression)
- `last`: a logical for whether the current fit is the final fit
- `weights`
- `classProbs`: a logical for whether class probabilities should be computed.

Here is something we could use for this model:
```{r }
svmFit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) { 
  kernlab::ksvm(
    x = as.matrix(x), y = y,
    kernel = "rbfdot",
    kpar = list(sigma = param$sigma),
    C = param$C,
    prob.model = classProbs,
    ...
    )
 }
 
lpSVM$fit <- svmFit
```

A few note about this:
- Notice that the package is not loaded in the code. It is loaded prior to this function being called so it won’t hurt if you load it again (but that’s not needed).
- The `ksvm` function requires a matrix or predictors. If the original data were a data frame, this would throw and error.
- The tuning parameters are references in the `param` data frame. There is always a single row in this data frame.
- The probability model is fit based on the value of `classProbs`. This value is determined by the value given in `trainControl`.
- The three dots allow the user to pass options in from `train` to, in this case, the `ksvm` function. For example, if the use wanted to set the cache size for the function, they could list `cache = 80` and this argument will be pass from `train` to `ksvm`.
- Any pre-processing that was requested in the call to train have been done. For example, if `preProc = "center`" was originally requested, the columns of `x` seen within this function are mean centered.
- Again, the namespace operator `::` is used for `rbfdot` and `ksvm` to ensure that the function can be found.

### 13.3.4 The `predict` Element
This is a function that produces a vector or predictions. In our case these are class predictions but they could be numbers for regression models.

The arguments are:

- `modelFit`: the model produced by the `fit` code shown above.
- `newdata`: the predictor values of the instances being predicted (e.g. out-of-bag samples)
- `preProc`
- `submodels`: this an optional list of tuning parameters only used with the `loop` element discussed below. In most cases, it will be NULL.

Our function will be very simple:
```{r }
svmPred <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
   kernlab::predict(modelFit, newdata)
lpSVM$predict <- svmPred
```

The function `predict.ksvm` will automatically create a factor vector as output. The function could also produce character values. Either way, the innards of `train` will make them factors and ensure that the same levels as the original data are used.

### 13.3.5 The `prob` element

If a regression model is being used or if the classification model does not create class probabilities a value of `NULL` can be used here instead of a function. Otherwise, the function arguments are the same as the `pred` function. The output should be a matrix or data frame of class probabilities with a column for each class. The column names should be the class levels.

We can use:
```{r eval=FALSE}
svmProb <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  kernlab::predict(modelFit, newdata, type = "probabilities")
lpSVM$prob <- svmProb
```

If you look at some of the SVM examples in the models directory, the real functions used by train are much more complicated so that they can deal with model failures, probabilities that do not sum to 1 etc.

## The sort element

This is an optional function that sorts the tuning parameters from the simplest model to the most complex. There are times where this ordering is not obvious. This information is used when the performance values are tied across multiple parameters. We would probably want to choose the least complex model in those cases.

Here, we will sort by the cost value. Smaller values of `C` produce smoother class boundaries than larger values:
```{r }
svmSort <- function(x) x[order(x$C),]
lpSVM$sort <- svmSort
```

### The `levels` element

`train` ensures that classification models always predict factors with the same levels. To do this at prediction time, the package needs to know the levels from the model object (specifically, the `finalModels` slot of the `train` object).

For model functions using `S3` methods, `train` automatically attaches a character vector called `obsLevels` to the object and the package code uses this value. However, this strategy does not work for `S4` methods. In these cases, the package will use the code found in the `levels` slot of the model list.

For example, the `ksvm` function uses `S4` methods but, unlike most model functions, has a built–in function called `lev` that will extract the class `levels (if any). In this case, our levels code would be:
```{r }
lpSVM$levels <- function(x) kernlab::lev(x)
```

In most other cases, the levels will beed to be extracted from data contained in the fitted model object. As another example, objects created using the `ctree` function in the `party` package would need to use:
```{r }
function(x) levels(x@data@get("response")[,1])
```

Again, this is only used for classification models using `S4` methods.
We should now be ready to fit our model.
```{r}
library(mlbench)
data(Sonar)
  
library(caret)
set.seed(998)
inTraining <- createDataPartition(Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTraining,]
testing  <- Sonar[-inTraining,]

fitControl <- trainControl(method = "repeatedcv",
                           ## 10-fold CV...
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
  

# set.seed(825)
Laplacian <- train(Class ~ ., data = training, 
                   method = lpSVM, 
                   preProc = c("center", "scale"),
                   tuneLength = 8,
                   trControl = fitControl)
Laplacian
```

A plot of the data shows that the model doesn't change when the cost value is above 16.
```{r}
ggplot(Lapacian)+scale_x_log10()
```

## Illustrative Example 2: Something more complicated - `LogitBoost`
### The loop element
This function can be used to create custom loops for models to tune over. In most cases, the function can just retrun the existing tuning grid. 

For example, a `LogitBoost` model can be trained over the number of boosting iterationds. IN the `caTools` package, the `LogitBoost` function can be used to fit this model. For example:
```{r eval=FALSE}
library(caTools)

mod <- LogitBoost(as.matrix(x), y, nIter = 51)
```

If we were to tune the model evaluating models where the number of iterations was 11, 21, 31, 41 and 51, the grid could be
```{r}
lbGrid <- data.frame(nIter = seq(11, 51, by = 10))  
lbGrid
```

During resampling, `train` could loop over all five rows in `lbGrid` and fit five models. However, the `predict.LogitBoost` function has an argument called `nIter` that can produce, in this case, predictions from `mod` for all five models.

Instead of `train` fitting five models, we could fit a single model with `nIter`=class="hl num">51 and derive preidctions for all five models using only `mod`.

The terminology used here is that `nIter` is a sequential tuning parameter (and the other parameters would be considered *fixed*).

The `loop` argument for model is used to produce two objects
- `loop`: this is the actual loop that is used by `train`.
- `submodels` is a *list* that has as many elements as there are rows in `loop`. The list has all the extra parameter settings that can be derived for each model.

Going back to the `LogitBoost` example, we could have:
```{r}
loop <- data.frame(.nIter=51)
loop
```

```{r}
submodels <- list(data.frame(nIter=seq(11,41,by=10)))
submodels
```

For this case, `train` first fits the `nIter = 51` model. When the model is predicted, that code has a for loop that iterates over the elements of `submodel[[1]]` to get the predictions for the other 4 models.

In the end, predictions for all five models (for `nIter = seq(11, 51, by = 10)`) with a single model fit.

There are other models built-in to `caret` that are used this way. There are a number of models that have multiple sequential tuning parameters.

If the `loop` argument is left `NULL` the results of `tuneGrid` are used as the simple loop and is recommended for most situations. Note that the machinery that is used to “derive” the extra predictions is up to the user to create, typically in the `predict` and `prob` elements of the custom model object.

For the `LogitBoost` model, some simple code to create these objects would be:
```{r}
fullGrid <- data.frame(nIter=seq(11,51,by=10))

## Get the largest value of nIter to fit the "full" model
loop <- fullGrid[which.max(fullGrid$nIter),,drop=FALSE]
loop
```

```{r}
submodels <- fullGrid[-which.max(fullGrid$nIter),,drop=FALSE]
## This needs to be encased in a list in case there are more
## than one tuning parameter
submodels <- list(submodels)  
submodels
```

For the `LogitBoost` custom model object, we could use this code in the `predict` slot:
```{r}
lbPred <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
  ## This model was fit with the maximum value of nIter
  out <- caTools::predict.LogitBoost(modelFit, newdata, type="class")
  
  ## In this case, 'submodels' is a data frame with the other values of
  ## nIter. We loop over these to get the other predictions.
  if(!is.null(submodels)) {
    ## Save _all_ the predictions in a list
    tmp <- out
    out <- vector(mode = "list", length = nrow(submodels) + 1)
    out[[1]] <- tmp
    
    for(j in seq(along = submodels$nIter)) {
      out[[j+1]] <- caTools::predict.LogitBoost(
        modelFit,
        newdata,
        nIter = submodels$nIter[j])
      
    }
  }
  out                   
}
```

A few more notes:

- The code in the `fit` element does not have to change.
- The `prob` slot works in the same way. The only difference is that the values saved in the outgoing lists are matrices or data frames of probabilities for each class.
- After model training (i.e. predicting new samples), the value of `submodels` is set to `NULL` and the code produces a single set of predictions.
- If the model had one sequential parameter and one fixed parameter, the `loop` data frame would have two columns (one for each parameter). If the model is tuned over more than one value of the fixed parameter, the `submodels` list would have more than one element. If `loop` had 10 rows, then `length(submodels)` would be `10` and `loop[i,]` would be linked to `submodels[[i]]`.
- In this case, the prediction function was called by namespace too (i.e. `caTools::predict.LogitBoost`). This may not seem necessary but what functions are available can vary depending on what parallel processing technology is being used. For example, the nature of forking used by `doMC` and `doParallel` tends to have easier access to functions while PSOCK methods in doParallel do not. It may be easier to take the safe path of using the namespace operator wherever possible to avoid errors that are difficult to track down.

Here is a slimmed down version of the logitBoost code already in the package:
```{r}
lbFuncs <- list(library = "caTools",
                loop = function(grid) {            
                  loop <- grid[which.max(grid$nIter),,drop = FALSE]
                  submodels <- grid[-which.max(grid$nIter),,drop = FALSE]
                  submodels <- list(submodels)  
                  list(loop = loop, submodels = submodels)
                },
                type = "Classification",
                parameters = data.frame(parameter = 'nIter',
                                        class = 'numeric',
                                        label = '# Boosting Iterations'),
                grid = function(x, y, len = NULL, search = "grid") {
                  out <- if(search == "grid") 
                    data.frame(nIter = 1 + ((1:len)*10)) else 
                      data.frame(nIter = sample(1:500, size = len))
                  out
                },
                fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
                  caTools::LogitBoost(as.matrix(x), y, nIter = param$nIter)
                },
                predict = function(modelFit, newdata, preProc = NULL, submodels = NULL) {
                  out <- caTools::predict.LogitBoost(modelFit, newdata, type="class")
                  if(!is.null(submodels)) {                   
                    tmp <- out
                    out <- vector(mode = "list", length = nrow(submodels) + 1)
                    out[[1]] <- tmp
                    
                    for(j in seq(along = submodels$nIter)) {
                      out[[j+1]] <- caTools::predict.LogitBoost(
                        modelFit,
                        newdata,
                        nIter = submodels$nIter[j]
                        )
                    }
                  }
                  out                   
                },
                prob = NULL,
                sort = function(x) x)
```

Should you care about this? Let's tune the model over the same dataset for the SVM model above and see how long it takes:
```{r}
set.seed(825)
lb1 <- system.time(train(Class ~ ., data = training, 
                         method = lbFuncs, 
                         tuneLength = 3,
                         trControl = fitControl))
lb1
```

```{r}

```



