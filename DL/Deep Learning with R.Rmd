---
title: "Deep Learning with R"
author: "Koji Mizumura"
date: 'r Sys.Date()'
output:
  word_document:
    toc: yes
  html_notebook:
    code_folding: hide
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

# Chapter 2
## First look at DL

This notebook contains the code samples found in Chapter 2, Section 1 of [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.

***

Let's look at a concrete example of a neural network that uses the Keras R package to learn to classify hand-written digits. Unless you already have experience with Keras or similar libraries, you will not understand everything about this first example right away. You probably haven't even installed Keras yet. Don't worry, that is perfectly fine. In the next chapter, we will review each element in our example and explain them in detail. So don't worry if some steps seem arbitrary or look like magic to you! We've got to start somewhere.

The problem we're trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels) into their 10 categories (0 to 9). We'll use the MNIST dataset, a classic dataset in the machine-learning community, which has been around almost as long as the field itself and has been intensively studied. It's a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of "solving" MNIST as the "Hello World" of deep learning—it's what you do to verify that your algorithms are working as expected. As you become a machine-learning practitioner, you'll see MNIST come up over and over again, in scientific papers, blog posts, and so on. 

The MNIST dataset comes preloaded in Keras, in the form of `train` and `test` lists, each of which includes a set of images (`x`) and associated labels (`y`):

```{r}
library(keras)
library(tensorflow)

mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

`train_images` and `train_labels` form the _training set_, the data that the model will learn from. The model will then be tested on the  _test set_, `test_images` and `test_labels`. The images are encoded as as 3D arrays, and the labels are a 1D array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels.

The R `str()` function is a convenient way to get a quick glimpse at the structure of an array. Let's use it to have a look at the training data:

```{r}
str(train_images)
```

```{r}
str(train_labels)
```

Let's have a look at the test data:

```{r}
str(test_images)
```

```{r}
str(test_labels)
```

The workflow will be as follows: first we'll feed the neural network the training data, `train_images` and `train_labels`. The network will then learn to associate images and labels. Finally, we'll ask the network to produce predictions for `test_images`, and we'll verify whether these predictions match the labels from `test_labels`.

Let's build the network -- again, remember that you aren't supposed to understand everything about this example yet.

```{r}
network <- keras_model_sequential() %>% 
  layer_dense(units =512, activation = "relu", input_shape = c(28*28)) %>% 
  layer_dense(units = 10, activation = "softmax")

network
```

The core building block of neural networks is the _layer_, a data-processing module that you can think of as a filter for data. Some data comes in, and it comes out in a more useful form. Specifically, layers extract _representations_ out of the data fed into them—hopefully representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of progressive _data distillation_. A deep-learning model is like a sieve for data processing, made of a succession of increasingly refined data filters—the layers.

Here our network consists of a sequence of two layers, which are densely connected (also called _fully connected_) neural layers. The second (and last) layer is a 10-way _softmax_ layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes.

To make the network ready for training, we need to pick three more things, as part of the _compilation_ step:

- A _loss function_: How the network will be able to measure how good a job its doing on its training data, and thus how it will be able to steer itself  in the right direction.
- _An optimizer_: The mechanism through which the network will update itself based on the data it sees and its loss function.
- _Metrics_ to monitor during training and testing: —Here we'll only care about accuracy (the fraction of the images that were correctly classified).

The exact purpose of the loss function and the optimizer will be made clear throughout the next two chapters.

```{r}
network %>% 
  compile(
    optimizer = "rmsprop",
    loss      = "categorical_crossentropy",
    metrics   = c("accuracy")
  )
```

Before training, we will preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the `[0, 1]` interval. Previously, our training images, for instance, were stored in an array of shape `(60000, 28, 28)` of type integer with values in the `[0, 255]` interval. We transform it into a double array of shape `(60000, 28 * 28)` with values between 0 and 1.

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255

test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
```

We also need to categorically encode the labels, a step which we explain in chapter 3:

```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

We are now ready to train our network, which in Keras is done via a call to the `fit` method of the network; we fit the model to its training data.

```{r}
network %>% 
  fit(train_images, train_labels,
      epochs = 5, batch_size = 128)
```

Two quantities are being displayed during traing the "loss" of the network over the training data, and the accuracy of the network over the training data.

We quickly reach an accuracy of 0.989 (i.e., 98.9%) on the training data. Now let's check that our model performs wekk on the test set too.

```{r}
metrics <- network %>% 
  evaluate(test_images, test_labels, verbose = 0)
metrics
```

Our test set accuracy turns out to be 98.1% -- thats quite a bit lower than training set accuracy. This gap between training accuracy and test accuracy is an example of "overfitting", the fact that machine learning models tend to perform worse on new data than on their training data. Overfitting will be a central topic in chapter 3.

This concludes our first example -- you just saw how you can build and a train a neural network to classify handwritten digits in less than 20 lines of R code. In the next chapter, we'll go into detail about every moving piece we just previewed and clarify what's going on behind the scenes. You'll learn about tensors, the data-storing objects going into the network; about tensor operations, which layers are made of; and about gradient descent, which allows your network to learn from its training examples.

Let's generate predictions for the first 10 samples of the test set:

```{r}
network %>% 
  predict_classes(test_images[1:10, ])
```

This concludes our first example—you just saw how you can build and train a neural network to classify handwritten digits in fewer than 20 lines of R code. In the next chapter, we’ll go into detail about every moving piece we just previewed and clarify what’s going on behind the scenes. You’ll learn about tensors, the data-storing objects going into the network; tensor operations, which layers are made of; and gradient descent, which allows your network to learn from its training examples.

## Data representations for neural networks 

In the previous example, we started from data stored in multidimensional arrays, also called _tensors_.  In general, all current machine-learning systems use tensors as their basic data structure. Tensors are fundamental to the field—so fundamental that Google’s TensorFlow was named after them. So what’s a tensor?

Tensors are a generalization of vectors and matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis). In R, vectors are used to create and manipulate 1D tensors, and matrices are used for 2D tensors. For higher-level dimensions, `array` objects (which support any number of dimensions) are used.

### Scalars (0D tensors)

A tensor that contains only one number is called a scalar (or _scalar tensor_, or _zero-dimensional tensor_, or _0D tensor_). R doesn’t have a data type to represent scalars (all numeric objects are vectors, matrices, or arrays), but an R vector that’s always length 1 is conceptually similar to a scalar.

### Vectors (1D tensors)

A one-dimensional array of numbers is called a _vector_ or _1D tensor_. A 1D tensor is said to have exactly one axis. We can convert the R vector to an `array` object to insepct its dimensions:

```{r}
x <- c(12, 3, 6, 14, 10)
str(x)
# num [1:5] 12 3 6 14 10

dim(as.array(x))
```

This vector has five entries and so is called five-dimensional vector. Don’t confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its axis, whereas a 5D tensor has five axes (and may have any number of dimensions along each axis). Dimensionality can denote either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor), which can be confusing at times. In the latter case, it’s technically more correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes), but the ambiguous notation 5D tensor is common regardless.

### Matrices (2D tensors)

A two-dimensional array of numbers is a matrix, or 2D tensor. A matrix has two axes (often referred to as rows and columns). You can visually interpret a matrix as a rectanglular grid of numbers.

```{r}


x <- matrix(rep(0, 3*5), nrow = 3, ncol = 5)
x
dim(x)
# [1] 3 5
```

### 3D tensors and higher-dimensional tensors

If you pack such matrices in a new array, you obtain a 3D ensor, which you can visually interpret as a cube of numbers:

```{r}
x <- array(rep(0, 2*3*2), dim = c(2,3,2))
str(x)
# num [1:2, 1:3, 1:2] 0 0 0 0 0 0 0 0

dim(x)
# [1] 2 3 2
```

By packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learning, you’ll generally manipulate tensors that are 0D to 4D, although you may go up to 5D if you process video data.

### Key attributes 

A tensor is defined by three key attributes.

- __Number of axes (rank)__ —For instance, a 3D tensor has three axes, and a matrix has two axes.
- __Shape__ — This is an integer vector that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape `(3, 5)`, and the 3D tensor example has shape `(2, 3, 2)`. A vector has a shape with a single element, such as (5). You can access the dimensions of any array using the `dim()` function.
- __Data type__—This is the type of the data contained in the tensor; for instance, a tensor’s type could be `integer` or `double`. On rare occasions, you may see a `character` tensor. But because tensors live in preallocated contiguous memory segments, and strings, being variable-length, would preclude the use of this implementation, they’re rarely used.

To make this more concrete, let’s look back at the data we processed in the MNIST example. First, we load the MNIST dataset:

```{r}
library(keras)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

Next, we display the number of axes of the tensor `train_images`:

```{r}
length(dim(train_images))
```

Here its shape:
```{r}
dim(train_images)
```

And this is its data type:

```{r}
typeof(train_images)
```

So what we have here is a 3D tensor of integers. More precisely, its an array of 60,00 matrices of 28*28 inteers. Each such matrix is a grayscale image, with coefficients 0 and 255.

LetYs plot the fifth digit in this 3D tensor.

```{r}
digit <- train_images[5,,]
plot(as.raster(digit, max=255))
```

### Manipulating tensors in R

In the previous example, we selected a specific digit alongside the first axis using the syntax `train_images[i,,]`. Selecting specific elements in a tensor is called __tensor slicing__.

The following example selects digits #10 to #99 and puts them in an array of shape `(90, 28, 28)`:

```{r}
my_slice <- train_images[1:99,,]
dim(my_slice)
```

It is equivalent 
```{r}
my_slice <- train_images[10:99, 1:28, 1:28]
dim(my_slice)
```

In general, you may select between any two indices along each tensor axis. For instance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you do this:

```{r}
my_slice <- train_images[, 15:28, 15:28]
```

### The notion of data batches 

In general, the first axis in all data tensors you’ll come across in deep learning will be the samples axis (sometimes called the samples dimension). In the MNIST example, samples are images of digits.

In addition, deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches. Concretely, here’s one batch of our MNIST digits, with batch size of 128:

```{r}
batch <- train_images[1:128,,]
```

And here is the next batch:

```{r}
batch <- train_images[129:256,,]
```

When considering such a batch tensor, the first axis is called the __batch axis__ or __batch dimension__. This is a term you’ll frequently encounter when using Keras and other deep-learning libraries.

### Real-world examples of data tensors

Let’s make data tensors more concrete with a few examples similar to what you’ll encounter later. The data you’ll manipulate will almost always fall into one of the following categories:

- Vector data—2D tensors of shape `(samples, features)`
- Timeseries data or sequence data—3D tensors of shape `(samples, timesteps, features)`
- Images—4D tensors of shape `(samples, height, width, channels)` or `(samples, channels, height, width)`
- Video—5D tensors of shape (`samples, frames, height, width, channels)` or `(samples, frames, channels, height, width)`

### Vector data

This is the most common case. In such a dataset, each single data point can be encoded as a vector, and thus a batch of data will be encoded as a 2D tensor (that is, an array of vectors), where the first axis is the samples axis and the second axis is the features axis.

Let’s take a look at two examples:

- An actuarial dataset of people, where we consider each person’s age, ZIP code, and income. Each person can be characterized as a vector of 3 values, and thus an entire dataset of 100,000 people can be stored in a 2D tensor of shape `(100000, 3)`.
- A dataset of text documents, where we represent each document by the counts of how many times each word appears in it (out of a dictionary of 20,000 common words). Each document can be encoded as a vector of 20,000 values (one count per word in the dictionary), and thus an entire dataset of 500 documents can be stored in a tensor of shape `(500, 20000)`.

### Time series data or sequence data

Whenever time matters in your data (or the notion of sequence order), it makes sense to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D tensor (see figure 2.3).

```{r}
knitr::include_graphics("02fig03.jpg")
```

The time axis is always the second axis, by convention. Let's take a look at a few examples: 

- A dataset of stock prices. Every minute, we store the current price of the stock, the highest price in the past minute, and the lowest price in the past minute. Thus, every minute is encoded as a 3D vector, an entire day of trading is encoded as a 2D tensor of shape `(390, 3)` (there are 390 minutes in a trading day), and 250 days’ worth of data can be stored in a 3D tensor of shape `(250, 390, 3)`. Here, each sample would be one day’s worth of data.
- A dataset of tweets, where we encode each tweet as a sequence of 140 characters out of an alphabet of 128 unique characters. In this setting, each character can be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry at the index corresponding to the character). Then each tweet can be encoded as a 2D tensor of shape `(140, 128)`, and a dataset of 1 million tweets can be stored in a tensor of shape `(1000000, 140, 128)`.

### Image data

Images typically have three dimensions: height, width, and color depth. Although grayscale images (like our MNIST digits) have only a single color channel and could thus be stored in 2D tensors, by convention image tensors are always 3D, with a one-dimensional color channel for grayscale images. A batch of 128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape `(128, 256, 256, 1)`, and a batch of 128 color images could be stored in a tensor of shape `(128, 256, 256, 3) (see figure 2.4)`.

```{r fig.cap="Fig 2.4 image data tensor"}
knitr::include_graphics("02fig04.jpg")
```

There are two conventions for shapes of image tensors: the  the channels-last convention (used by TensorFlow) and the channels-first convention (used by Theano). The TensorFlow machine-learning framework, from Google, places the color-depth axis at the end: (samples, height, width, color_depth). Meanwhile, Theano places the color depth axis right after the batch axis: (samples, color_depth, height, width). With the Theano convention, the previous examples would become (128, 1, 256, 256) and (128, 3, 256, 256). The Keras framework provides support for both formats.

## The gears of neural networks: tensor operations

Much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (AND, OR, NOR, and so on), all transformations learned by deep neural networks can be reduced to a handful of tensor operations applied to tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors, and so on.

In our initial example, we were building our network by stacking dense layers on top of each other. A layer instance looks like this:

```{r eval=FALSE}
layer_dense(units = 512, activation = "relu")
```

This layer can be interpreted as a function, which takes as input a 2D tensor and returns another 2D tensor - a new representation for the input tensor. Specifically, the function is as follows (where `w` is a 2D tensor and `b` is a vector, both attribute of the layer).

```{r eval=FALSE}
output = relu(dot(W, input) +b)
```

Let's unpack this. We have three tensor operations here: a dot product (`dot`) between the input tensor and a tensor named `W`; an addition between the resulting 2D tensor and a vector `b`; and finally, a `relu` operation. `relu(x)` is `max(x,0)`. 

### Element-wise operations 

The `relu` n and addition are element-wise operations: operations that are applied
independently to each entry in the tensors being considered. This means these operations are highly amenable to massively parallel implementations (vectorized implementations, a term that comes from the vector processor supercomputer architecture from the
1970–1990 period). If you want to write a naive R implementation of an element-wise
operation, you use a for loop, as in this naive implementation of an element-wise relu
operation:

```{r}
naive_relue <- function(x){
  for (i in nrow(x))
    for (j in ncol(x))
      x[i, j] <- max(x[i,j], 0)
}
```

You do the same for addition:

```{r}
naive_add <- function(x,y){
  for (i in nrow(x))
    for (j in ncol(x))
      x[i, j] = x[i, j] + y[i, j]
}
```

On the same principle, you can do element-wise multipication, substraction and so on. 

In practice, when dealing with R arrays, these operations are available as welloptimized built-in R functions, which themselves delegate the heavy lifting to a BLAS
implementation (Basic Linear Algebra Subprograms) if you have one installed (which
you should). BLAS are low-level, highly parallel, efficient tensor-manipulation routines
typically implemented in Fortran or C.
 So in R you can do the following native element-wise operations, and they will be
blazing fast:

```{r eval=FALSE}
z <- x + y # element-wise addition
z <- pmax(z, 0)
```

### Operations involving tensors of different dimensions 

Our earlier naive implementation of `naive_add` only supports the addition of 2D tensors with identical shapes. But in the dense layer introduced earlier, we added a 2D tensor with a vector. What happens with addition when the shapes of the two tensors
being added differ?

The R `sweep()` function enables you to perform operations between higherdimension tensors and lower-dimension tensors. With `sweep()`, we could perform the matrix plus vector addition described earlier as follows:

```{r}
sweep(x, 2, y, `+`)
```

The second argument (here, 2) specifies the dimensions of x over which to sweep y. The last argument (here, +) is the operation to perform during the sweep, which should be a function of two arguments: x and an array of the same dimensions generated from y by `aperm()`.

You can apply a sweep in any number of dimensions and can apply any function that implements a vectorized operation over two arrays. The following example sweeps a 2D tensor over the last two dimensions of a 4D tensor using the `pmax()` function:

```{r} 
x <- array(round(runif(1000,0,9)), 
           dim = c(64, 3, 32, 10))
y <- array(5, dim = c(32,10))
z <- sweep(x, c(3,4), y, pmax)
```

### Tensor dot 























