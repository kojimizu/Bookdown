---
title: "UC business Analytics R programming Guide 1(2)"
author: "Koji Mizumura"
date: "`r Sys.Date()`"
output:
   rmdformats::readthedown:
    highlight: kate
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '4'
 always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
library(rmdformats)

knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```


This is a practice of [UC business analytics R programming guide](http://uc-r.github.io/).

# Predictive analytics
## Machine Learning
### Regularized regression

As discussed, linear regression is a simple and fundamental approach for supervised learning. Moreover, when the assumption required by OLS are met, the coefficients produced by OLS are unbiased and, of all unbiased linear techniques, have the lowest variance. However, in today’s world, data sets being analyzed typically have a large amount of features. As the number of features grow, our OLS assumptions typically break down and our models often overfit (aka have high variance) to the training sample, causing our out of sample error to increase. Regularization methods provide a means to control our regression coefficients, which can reduce the variance and decrease our of sample error.

#### tl;dr
This tutorial serves as an introduction to regularized and covers:

1. [Replication requirements](#RR_RR): What you’ll need to reproduce the analysis in this tutorial.
2. [Why regularize](#RR_Reg): A closer look at why regularization can improve upon ordinary least squares regression.
3. [Ridge regression](#RR_Ridge): Regularizing coefficients but keeping all features.
4. [Lasso regression](#RR_LASSO): Regularizing coefficients to perform feature selection.
5. [Elastic nets](#RR_EN): Combining Ridge and Lasso regularization. Predicting: Once you’ve found your optimal model, predict on a new data set.
6. [Other package implementations](#RR_pck_imp): Implementing regularization with other popular packages.
7. [Learning more](#RR_Learn): Where to go from here.

#### Replication Requirements: {#RR_RR}

This tutorial leverages the following packags. Most of these packages are playing a supporting role while the main emphasis will be on the `glmnet` package.

```{r}
library(tidyverse)
library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
```

To illustrate various regularization concepts, we will use the `Ames` Housing data that has been included in the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)

ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```


#### Why regularize {#RR_Reg}

The objective of ordinary least squares regression is to find the plane that minimizes the sum of squared errors (SSE) between the observed and predicted response. In Figure 1, this means identifying the plane that minimizes the grey lines, which measure the distance between the observed (red dots) and predicted response (blue plane).

More formally, this objective function is written as:
$$
minimize [{SSE = \Sigma_{i=1}^n (y_i-\hat{}y_i)^2}]
$$

The OLS objective function performs quite well when our data align to the key assumptions of OLS regression:

- Linear relationship
- Multivariate normality
- No autocorrelation
- Homoscedastic (constant variance in residuals)
- There are more observations (n) than features 

However, for many real-life data sets we have very _wide_ data, meaning we have a large number of features (_p_) that we believe are informative in predicting some outcome. As _p_ increases, we can quickly violate some of the OLS assumptions and we require alternative approaches to provide predictive analytic solutions. Specifically, as _p_ increases there are three main issues we most commonly run into:

__1. Multicollinearty__

As _p_ increases we are more likely to capture multiple features that have some multicollinearity. When multicollinearity exists, we often see high variability in our coefficient terms. For example, in our Ames data, `Gr_Liv_Area` and `TotRms_AbvGrd` are two variables that have a correlation of 0.801 and both variables are strongly correlated to our response variable (`Sale_Price`). When we fit a model with both these variables we get a positive coefficient for `Gr_Liv_Area` but a negative coefficient for `TotRms_AbvGrd`, suggesting one has a positive impact to `Sale_Price` and the other a negative impact.

```{r}
# fit with two strongly correlated variables
lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)    Gr_Liv_Area  TotRms_AbvGrd  
##       49953.6          137.3       -11788.2
```

However, if we refit the model with each variable independently, they both show a positive impact. However, the `Gr_Liv_Area` effect is now smaller and the `TotRms_AbvGrd` is positive with a much larger magnitude.

```{r}
# fit with just Gr_Liv_Area
lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Coefficients:
## (Intercept)  Gr_Liv_Area  
##       17797          108

# fit with just TotRms_Area
lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Call:
## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)  TotRms_AbvGrd  
##         26820          23731
```

This is a common result when collinearity exists. Coefficients for correlated features become over-inflated and can fluctuate significantly. One consequence of these large fluctuations in the coefficient terms is overfitting, which means we have high variance in the bias-variance tradeoff space. Although an analyst can use tools such as variance inflaction factors (Myers, 1994) to identify and remove those strongly correlated variables, it is not always clear which variable(s) to remove. Nor do we always wish to remove variables as this may be removing signal in our data.

__2. Insufficient solution__

When the number of features exceed the number of observations ($p>n$), the OLS solution matrix is _not_ invertible. This causes significant issues because it means: (1) The least-squares estimates are not unique. In fact, there are an infinite set of solutions available and most of these solutions overfit the data. (2) In many instances the result will be computationally infeasible.

Consequently, to resolve this issue an analyst can remove variables until $p < n$ and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach (Kuhn & Johnson, 2013, pp. 43-47), it can be cumbersome and prone to errors.

__3. Interpretability__

With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection. One approach to this is called _hard threshholding_ feature selection, which can be performed with linear model selection approaches. However, model selection approaches can be computationally inefficient, do not scale well, and they simply assume a feature as in or out. We may wish to use a _soft threshholding_ approach that slowly pushes a feature’s effect towards zero. As will be demonstrated, this can provide additional understanding regarding predictive signals.

__Regulaized regression__

When we experience these concerns, one alternative to OLS regression is to use regularized regression (also commonly referred to as _penalized_ models or _shrinkage_ methods) to control the parameter estimates. Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model.

The objective function of regulaized regression model is very similar to OLS regression; however, we add a penalty paramter (`P`)

$$
minimize(SSE + P)
$$

There are two main penalty parameters, which we will see shortly, but they both have a similarr effect. They constrain the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). Next, we’ll explore the most common approaches to incorporate regularization.

#### Ridge {#RR_Ridge}

Ridge regression [(Hoerl, 1970](https://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634)) controls the coefficients by adding $\lambda \Sigma_{j=1}^p \beta_j^2$ to the objective function. This penalty parameter is also referred to as ???$L_2$" as it signifies a second-order penlty being used on the coefficients.

$$ minimize (SSE + \lambda \Sigma_{j=1}^p \beta_j^2)
$$ 


This penalty parameter can take on a wide range of values, which is controlled by the tuning parameter $\lambda$. When $lambda>0$ there is no effect and our objective function equals the normal PLS regression objective function of simply minimizing SSE. 

However, as $\lambda -> \infit$, the penalty becomes large and forces our coefficients to zero. This is illustrated in Figure 2 where exemplar coefficients have been regularized with $\lambda ranging from 0 to over 8,000 (log(8103)=9).

Aothough these coefficients were scaled and centered prior to the analysis, you will notice that some are extremely large when $\lambda=0$. Furthermore, you will notice the large shrinks to zero. his is indicitive of multicollinearity and likely illustrates that constraining our coefficients with $log(\lambda)~2$ whre its then continuously shrinks to zero. This is indicative of multicollinearity and likely illustrates that constraining our coefficients with $log(\lambda)>2$ may reduce the variance, and therefore the error in our model. However, the question remains - how do we find the amount that minimizes our error? We will answer this shortly.

##### Implementation

To implement Ridge regression, we will focus on the `glmnet` package (implementation in other packages are illustrated [below](http://uc-r.github.io/regularized_regression#other)). `glmnet` does not use the formula method (`y ~ x`) so prior to modeling we need to create our feature and target set. Furthermore, we use the `model.matrix` function on our feature set, which will automatically dummy encode qualitative variables (see `Matrix::sparse.model.matrix` for increased efficiency on large dimension data). We also log transform our response variable due to its skeweness.


```{r}
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
ames_train_x <- model.matrix(Sale_Price ~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Sale_Price)

ames_test_x <- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y <- log(ames_test$Sale_Price)

# What is the dimension of of your feature matrix?
dim(ames_train_x)
## [1] 2054  307
```

To apply a ridge model we can use the glmnet::glmnet function. The alpha parameter tells glmnet to perform a ridge (`alpha = 0`), lasso (`alpha = 1`), or elastic net ($0 < alpha < 1$) model. Behind the scenes, `glmnet` is doing two things that you should be awre of:

1. It is essential that predictor variables are standardized when performing regularized regression. `glmnet` performs this for you.  If you standardize your predictors prior to `glmnet` you can turn this argument off with `standardize = FALSE`.

2. `glmnet` will perform ridge models across a wide range of $\lambda$ parameters, which are illustrated in the figure below.

```{r}
# apply ridge regression to ames data
library(glmnet)
ames_ridge <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge, xvar="lambda")
```

In fact, we can see the exact $\lambda$ values applied with `ames_ridge$lambda`. Although you can specify your own $\lambda$ values, by default `glmnet` applies 100 $\lambda$ values that are derived. Majority of the time  you will have little need to adjust the default $\lambda$ values.

We can also directly access the coefficients for a model using `coef. glmnet` stores all the coefficients for each model in order of largest to smallest $\lambda$. Due to the number of features, here I just peak at thebcoefficients for the `Gr_Liv_Area` and `TotRms_abvGrd` features for the largest $\lambda$ (279.1035) and smallest $\lambda$ (0.02791035). You can see how the largest $\lambda$ value has pushed these coefficients to nearly 0.

```{r}
# lambdas applied to penalty parameter
ames_ridge$lambda %>% head()
## [1] 279.1035 254.3087 231.7166 211.1316 192.3752 175.2851

# coefficients for the largest and smallest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]
##   Gr_Liv_Area TotRms_AbvGrd 
##  0.0001004011  0.0096383231
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 1] 
##   Gr_Liv_Area TotRms_AbvGrd 
##  5.551202e-40  1.236184e-37
```

However, at this point, we do not understand how much improvement we are experiencing in our model.

__Tuning__ 

Recall tht $\lambda$ is a tuning parameter that helps to control our model frok overfitting to the training data. However, to identify the optimal $\lambda$ value we need to perform [cross validation](http://uc-r.github.io/resampling_methods) (CV). `cv.glmnet` provides a built-in option to perform k-fold CV, and by default, performs 10-fold CV.

```{r}
# Apply CV Ridge regression to ames data
ames_ridge <- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

# plot results
plot(ames_ridge)
```


Our plot outputs above illustrates the 10-fold CV mean squared error(MSE) across the $\lambda$ values. It illustrates that we do not see substantial improvement; however, as we constrain our coefficients with $log(\lambda)>0$ penalty, the MSE rises considerably. 

The numbers at the top of the plot (299) just refer to the number of variables in the model. Ridge regression does not force any variable exactly zero so all features will remain in the model (we will see this change with `lasso` and `elastic nets`).

```{r}
ames_ridge$cvm %>% min() #minimum MSE
ames_ridge$lambda.min # lambda for this min MSE

ames_ridge$cvm[ames_ridge$lambda == ames_ridge$lambda.1se] # 1st st.error
ames_ridge$lambda.1se  # lambda for this MSE
```

The advantage of identifying the $\lambda$ with an MSE within one standard error becomes more obvious with the lasso and elastic net models.  However, for now we can assess this visually. Here we plot the coefficients across the $\lambda$ values and the dashed red line represents the largest $\lambda$ that falls within oe standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy.

```{r}
ames_ridge_min <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge_min, xvar = "lambda")
abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")

```

__Advantage & Disdvantage__

In essence, the ridge regression model has pushed many of the correlated features towards each other rather than allowing for one to be wildly positive and the other wildly negative. Furthermore, many of the non-important features have been pushed closer to zero. This means we have reduced the noise in our data, which provides us more clarity in identifying the true signals in our model.

```{r}
coef(ames_ridge, s = "lambda.1se") %>% 
  broom::tidy() %>% 
  filter(row != "(Intercept)") %>% 
  top_n(25, wt = abs(value)) %>% 
  ggplot(aes(value, reorder(row, value)))+
  geom_point()+
  ggtitle("Top 25 influential variables")+
  xlab("coefficient")+
  ylab(NULL)
```

However, __a ridge model will retain all variables__. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection. If greater interpretation is necessary where you need to reduce the signal in your data to a smaller subset then a lasso model may be preferable.

#### LASSO {#RR_LASSO}

The _least absolute shrinkage and selection operator (lasso)_ model ([Tibshirani, 1996](http://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents)) is an alternative to ridge regression that has a small modification to the penalty in the objective function. Rather than the $L_2$ penalty we use the following $L_1$ penalty $\lambda \Sigma_{j=1}^p|\beta_j|$ in the objective function

$$
minimize (SSE + \lambda \Sigma_{j=1}^p|\beta_j|)
$$

Whereas the ridge regression approach pushes variables to _approximately but not equal to zero_, the lasso penalty will actually push coefficients to zero as illustrated with Fig. 3. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection.

In Fig.3we see that when $log(\lambda)=1$ only 3 variables are retained. Consequently, when a data set has many features lasso can be used to identify and extract those features with the largest (and most consistent) signal.

__Implementation__

Implementing lasso follows the same logic as implementing the ridge model, we just need to switch `alpha=1` within `glmnet`.

```{r}
## apply lasso regression to ames data

ames_lasso <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso, xvar = "lambda")
```

Our output illustrates a quick drop in the number of features retained in the lasso model as $log(\lambda)$-> -6. In fact, we see several features that had very large coefficients for the OLS model (when $log(\lambda)=-10 -> \lambda=0$.  As before, these features are likely highly correlated with other features in the data, causing their coefficients to be excessively large. As we constrain our model, these noisy features are pushed to zero.

However, similar to the Ridge regression section, we need to perform CV to determine when the right value is for $\lambda$.

__Tuning__

To perform CV we use the same approach as we did in the ridge regression tuning section, but change our `alpha = 1`. We see that we can minimize our MSE by applying approximately $-6 < log(\lambda) < -4$ Not only does this minimize our MSE but it also reduces the number of features to $156 > p > 58$

```{r}
# Apply CV Ridge regression to ames data
ames_lasso <- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)
# plot results
plot(ames_lasso)
```

As before, we can extract our minimum and one standard error MSE and $\λ$ values.

```{r}
min(ames_lasso$cvm)       # minimum MSE
## [1] 0.02275227
ames_lasso$lambda.min     # lambda for this min MSE
## [1] 0.003521887

ames_lasso$cvm[ames_lasso$lambda == ames_lasso$lambda.1se]  # 1 st.error of min MSE
## [1] 0.02562055
ames_lasso$lambda.1se  # lambda for this MSE
## [1] 0.01180396
```

Now the advantage of identifying the $\lambda$ with an MSE within one standard error becomes more obvious. If we use the $\lambda$ that derives the minimum MSE we can reduce our feature set from 307 down to less than 160. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor.

```{r}
ames_lasso_min <-  glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso_min, xvar = "lambda")
abline(v = log(ames_lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(ames_lasso$lambda.1se), col = "red", lty = "dashed")
```

__Advantage & Disadvantage__

Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those feature most influential to predictive accuracy.

```{r}
coef(ames_lasso, s = "lambda.1se") %>% 
  broom::tidy() %>% 
  filter(row != "(Intercept)") %>% 
  ggplot(aes(value, reorder(row, value), color = value >0))+
  geom_point(show.legend = FALSE)+
  ggtitle("Influential variables")+
  xlab("Coefficient")+
  ylab(NULL)
```

However, often when we remove features we sacrifice accuracy. Consequently, to gain the refined clarity and simplicity that lasso provides, we sometimes reduce the level of accuracy. Typically we do not see large differences in the minimum errors between the two. So practically, this may not be significant but if you are purely competing on minimizing error (i.e. Kaggle competitions) this may make all the difference!

```{r}
# minimum Ridge MSE
min(ames_ridge$cvm)
## [1] 0.02147691

# minimum Lasso MSE
min(ames_lasso$cvm)
## [1] 0.02275227
```


#### Elastic Nets {#RR_EN}

A generalization of the ridge and lasso models is the elastic net (Zou and Hastie, 2005), which combines the two penalties.

$$
minimize(SSE + \lambda_1\Sigma_{j=1}^p \beta_j^2 + \lambda_2\Sigma_{j=1}^p |\beta_j|)
$$

Although lasso models perform feature selection, a result of their penalty parameter is that typically when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically reducing correlated features together. Consequently, the advantage of the elastic net model is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty.

__Implementation__

We implement an elastic net the same way as the ridge and lasso models, which are controlled by the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas `alpha` $???0$  will have a heavier ridge penalty applied and `alpha` $???1$ will have a heavier lasso penalty.

```{r}
library(glmnet)
lasso    <- glmnet(ames_train_x, ames_train_y, alpha = 1.0) 
elastic1 <- glmnet(ames_train_x, ames_train_y, alpha = 0.25) 
elastic2 <- glmnet(ames_train_x, ames_train_y, alpha = 0.75) 
ridge    <- glmnet(ames_train_x, ames_train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
```

__Tuning__

In ridge and lasso models $\lambda$ is our primary tuning parameter. However, with elastic nets, we want to tune the $\lambda$ and the `alpha` parameters. To set up our tuning, we create a common `fold_id`, which just allows us to apply the same CV folds to each model.

We then create a tuning grid that searches across a range of `alpha`s from 0-1, and empty columns where we will dump our model results into.

```{r}
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(ames_train_y), replace=TRUE)

# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
```

Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error MSE values and their respective $\lambda$ values.

```{r}
for(i in seq_along(tuning_grid$alpha)) {
  
  # fit CV model for each alpha value
  fit <- cv.glmnet(ames_train_x, ames_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid
## # A tibble: 11 x 5
##    alpha mse_min mse_1se lambda_min lambda_1se
##    <dbl>   <dbl>   <dbl>      <dbl>      <dbl>
##  1 0      0.0217  0.0241    0.136       0.548 
##  2 0.100  0.0215  0.0239    0.0352      0.0980
##  3 0.200  0.0217  0.0243    0.0193      0.0538
##  4 0.300  0.0218  0.0243    0.0129      0.0359
##  5 0.400  0.0219  0.0244    0.0106      0.0269
##  6 0.500  0.0220  0.0250    0.00848     0.0236
##  7 0.600  0.0220  0.0250    0.00707     0.0197
##  8 0.700  0.0221  0.0250    0.00606     0.0169
##  9 0.800  0.0221  0.0251    0.00530     0.0148
## 10 0.900  0.0221  0.0251    0.00471     0.0131
## 11 1.00   0.0223  0.0254    0.00424     0.0118
```

If we plot the MSE +- one standard erro for the optimal $\lambda$ value for each `alpha` setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model with $lambda=0.02062776$, gain the benefits of its feature selection capability and reasonably assume no loss in accuracy.

```{r}
tuning_grid %>% 
  mutate(se = mse_1se - mse_min) %>% 
  ggplot(aes(alpha, mse_min))+
  geom_line(size = 2)+
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha= .25)+
  ggtitle("MSE +- one standard error")
```

__Advantage & Disadvantage__

As previously stated, the advantage of the elastic net model is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. Effectively, elastic nets allow us to control multicollinearity concerns, perform regression when 
$p>n$, and reduce excessive noise in our data so that we can isolate the most influential variables while balancing prediction accuracy.

However, elastic nets, and regularization models in general, still assume linear relationships between the features and the target variable. And although we can incorporate non-additive models with interactions, doing this when the number of features is large is extremely tedious and difficult. When non-linear relationships exist, its beneficial to start exploring non-linear regression 

#### Predicting

Once you have identified your preferred model, you can simply use `predict` to predict the same model on a new data set. The only caveat is you need to supply `predict` an `s` parameter with the preferred models $\lambda$ value. For example, here we create a lasso model, which provides me a minimum MSE of 0.022. I use the minimum $\lambda$ value to predict on the unseen test set and obtain a slightly lower MSE of 0.015.

```{r}
# some best model
cv_lasso   <- cv.glmnet(ames_train_x, ames_train_y, alpha = 1.0)
min(cv_lasso$cvm)
## [1] 0.02279668

# predict
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, ames_test_x)
mean((ames_test_y - pred)^2)
## [1] 0.01488605
```

#### Other package implementation {##RR_pck_imp}

glmnet is not the only package that can perform regularized regression. The following also shows how to implement with the popular `caret` and `h2o` packages. For brevity, I show the code but not the output.

```{r}
## caret package
library(caret)

train_control <- trainControl(method = "cv", number = 10)

caret_mod <- train(
  x = ames_train_x,
  y = ames_train_y,
  method = "glmnet",
  preProc = c("center", "scale", "zv", "nzv"),
  trControl = train_control,
  tuneLength = 10
)

caret_mod
```

```{r}
## h2o package
library(h2o)
h2o.init()

# convert data to h2o object
ames_h2o <- ames_train %>%
  mutate(Sale_Price_log = log(Sale_Price)) %>%
  as.h2o()

# set the response column to Sale_Price_log
response <- "Sale_Price_log"

# set the predictor names
predictors <- setdiff(colnames(ames_train), "Sale_Price")


# try using the `alpha` parameter:
# train your model, where you specify alpha
ames_glm <- h2o.glm(
  x = predictors, 
  y = response, 
  training_frame = ames_h2o,
  nfolds = 10,
  keep_cross_validation_predictions = TRUE,
  alpha = .25
  )

# print the mse for the validation data
print(h2o.mse(ames_glm, xval = TRUE))

# grid over `alpha`
# select the values for `alpha` to grid over
hyper_params <- list(
  alpha = seq(0, 1, by = .1),
  lambda = seq(0.0001, 10, length.out = 10)
  )

# this example uses cartesian grid search because the search space is small
# and we want to see the performance of all models. For a larger search space use
# random grid search instead: {'strategy': "RandomDiscrete"}

# build grid search with previously selected hyperparameters
grid <- h2o.grid(
  x = predictors, 
  y = response, 
  training_frame = ames_h2o, 
  nfolds = 10,
  keep_cross_validation_predictions = TRUE,
  algorithm = "glm",
  grid_id = "ames_grid", 
  hyper_params = hyper_params,
  search_criteria = list(strategy = "Cartesian")
  )

# Sort the grid models by mse
sorted_grid <- h2o.getGrid("ames_grid", sort_by = "mse", decreasing = FALSE)
sorted_grid

# grab top model id
best_h2o_model <- sorted_grid@model_ids[[1]]
best_model <- h2o.getModel(best_h2o_model)

```


#### Learning More {#RR_Learn}

This serves as an introduction to regularized regression; however, it just scrapes the surface. Regularized regression approaches have been extended to other parametric generalized linear models (i.e. logistic regression, multinomial, poisson, support vector machines). Moreover, alternative approaches to regularization exist such as `Least Angle Regression` and The `Bayesian Lasso`. The following are great resources to learn more (listed in order of complexity):

- Applied Predictive Modeling
- Introduction to Statistical Learning
- The Elements of Statistical Learning
- Statistical Learning with Sparsity

### Multivariate Adaptive regression splines (MARS) {#MARS}

Several previous tutorials (i.e., ) discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects); however, to do so you must know the specific nature of the nonlinearity a priori. Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy.

This tutorial discusses multivariate adaptive regression splines (MARS), an algorithm that essentially creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of linear regression and other intrinsically linear models.

- [Prerequisites](#MARS_PR)
- [Basic Idea](#MARS_BI)
- [Multivariage regression splines](#MARS)
- [Fitting a basic MARS model](#MARS_Fit)
- [Tuning](#MARS_TUNE)
- [Feature interpretation](#MARS_FI)
- [Final thoughts](#MARS_SUM)
- [Learning more](#MARS_LM)


#### Prerequisites {#MARS_PR}

For this tutorial we will use the following packages:
```{r}
library(rsample)   # data splitting 
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
library(vip)       # variable importance
library(pdp)       # variable relationships
```

To illustrate various MARS modeling concepts we will use `Ames Housing` data, which is available via the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

#### The basic idea {#MARS_BI}

Some previous tutorials have focused on linear models. In those tutorials we illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients. However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictive accuracy.

We can extend linear models to capture non-linear relationships. Typically, this is done by explicitly including polynomial parameters or step functions. Polynomial regression is a form of regression in which the relationship between the independent variable $x$ and the dependent variable y is modeled as an $n^{th}$ degree polynomial of $x$. For example, Equation 1 represents a polynomial regression function where y is modeled as a function of $x$ with $d$ degrees. Generally speaking, it is unusual to use d greater than 3 or 4 as the larger d becomes, the easier the function fit becomes overly flexible and oddly shapened…especially near the boundaries of the range of x values

$$
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2+...+\beta_dx_i^d+\epsilon_i
$$

An alternative to polynomial regression. is step function regression. Whereas polynomial functions impose a global non-linear relationship, step functions break the range of $x$ into bins, and fit a different constant for each bin. This amounts to converting a continuous variable into an ordered categorical variable such that our linear regression function is converted to Equation 2

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + ... + \beta_d C_d(x_i) + \epsilon_i
$$

where $C_1(x)$ represents $x$ value ranging from $c_1 < x < c_2$, $C_2(x)$ represents $x$ value ranging from $c_2<x<c_3$, ..., $C_d(x)$ represents $x$ values ranging from $c_{d-1}<x<c_d$. Figure 1 illustrate polynomial and step function fits for `Sale_Price` asa a function of `Year_Build` in our `ames` data.

```{r fig.cap="Figure 1: Blue line represents predicted Sale_Price values as a function of Year_Built for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional nonlinear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function fitting cutting Year_Built into three categorical levels."}
# polynomial degree of 1
p1 <- ames_train %>% 
  ggplot(aes(Year_Built, Sale_Price))+
  geom_point()+
  geom_smooth(method = "gam", formula = y ~ x)

# polynomial degree of 2
p2 <- ames_train %>% 
  ggplot(aes(Year_Built, Sale_Price))+
  geom_point()+
  geom_smooth(method = "gam", formula = y ~ poly(x,2))

# polynomial degree of 3
p3 <- ames_train %>% 
  ggplot(aes(Year_Built, Sale_Price))+
  geom_point()+
  geom_smooth(method = "gam", formula = y ~ poly(x,3))

# step function

gridExtra::grid.arrange(p1, p2, p3, ncol=3)
```

Although useful, the typical implementation of polynomial regression and step functions require the user to explicitly identify and incorporate which variables should have what specific degree of interaction or at points of a variable $x$ should cut points to be made for the step functions. Considering many data sets today can easily contain 50, 100, or more features, this would require an enormous and unncessary time commitment from an analyst to determine these explicit non-linear settings.

### Multivariage regression splines {#MARS}

Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinearity aspect of polynomial regression by assessing cutpoints ( _knots_) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). For example, consider our simple model of `Sale_Price` ~ `Year_Built`. The MARS procedure will first look for the single point across the range of Year_Built values where two different linear relationships between `Sale_Price` and `Year_Built` achieve the smallest error. 

What results is known as hinge function ($h(x-a)$ where $a$ is the cutpoint value). For a single knot, our hinge function is $h(Year_Built - 1968$ such that our two linear models for `Sale_Price` are

$$
Sale\_Price = 136091.022 (for \ Year\_Built < 1968)
136091.022 + 3094.208 (Year\_Built-1968) (for Year\_Built > 1968)
$$

Once the first knot has benn found, the search continues for a second knot which is found at 2006 (Figure 2 (B)). This results in three linear models for `Sale_Price`:

```{r eval=FALSE}
mars0 <- ames_train %>% 
  earth::earth(Sale_Price ~ Year_Built, data=.) 

mars0 %>% print()
```

This procedure can continue until many knots are found, producing a highly non-linear pattern. Although including many knots may allow us to fit a really good relationship with our training data, it may not generalize very well to new, unseen data. For example, Figure 3 includes nine knots but this likley will not generalize very well to our test data.

Consequently, once the full set of knots have been created, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as “pruning??? and we can use cross-validation, as we have with the previous models, to find the optimal number of knot

#### Fitting a basic MARS model {#MARS_Fit}

We can fit a MARS model with the __earth__ package. By default, `earth::earth()` will assess all potential knots across all supplied features and then will prune to the optimal number of knots on an expected change in $R^2$ (for the training data) of less than 0.001. This calculation is performed by the Generalized cross-validation procedure (GCV statistic), which is a computational shortcut for linear models that produces an error value that approximates leave-one-out cross-validation (see [here](http://w3.atomki.hu/~efo/hornyak/Tikhonov_references/Technometrics_Golub_Heath_Wahba) for technical details).

The following applies a basic MARS model to our ames data and performs a search for required knots across all features. The results show us the final models GCV statistic, generalized $R^2$ (GRSq) and more.

```{r}
# Fit a basic MARS model
mars1 <- earth::earth(
  Sale_Price ~.,
  data = ames_train
)

# Print model summary
print(mars1)
## Selected 37 of 45 terms, and 26 of 307 predictors
## Termination condition: RSq changed by less than 0.001 at 45 terms
## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, ...
## Number of terms at each degree of interaction: 1 36 (additive model)
## GCV 521186626    RSS 995776275391    GRSq 0.9165386    RSq 0.92229
```

It also shows us that 38 of 41 terms were used from 27 of the 307 original predictors. But what does this mean? If we were to look at all the coefficients, we would see that there are 38 terms in our model (including the intercept). These terms include hinge functions produced from the original 307 predictors (307 predictors because the model automatically dummy encodes our categorical variables). Looking at the first 10 terms in our model, we see that `Gr_Liv_Area` is included with a knot at 2945 (the coefficients for $h(2945 - Gr_Liv_Area$ is $49.85$), `Year_Built` is included with a knot at 2003, etc.

```{r}
mars1 %>% .$coefficients %>% head()

##                              Sale_Price
## (Intercept)                301399.98345
## h(2945-Gr_Liv_Area)           -49.84518
## h(Year_Built-2003)           2698.39864
## h(2003-Year_Built)           -357.11319
## h(Total_Bsmt_SF-2171)        -265.30709
## h(2171-Total_Bsmt_SF)         -29.77024
## Overall_QualExcellent       88345.90068
## Overall_QualVery_Excellent 116330.48509
## Overall_QualVery_Good       36646.55568
## h(Bsmt_Unf_SF-278)            -21.15661
```

The plot method for MARS model objects provide convenient performance and residual plots.Figure 4 illustrates the model selection plot that graphs the GCV $R^2$  (left-hand y-axis and solid black line) based on the number of terms retained in the model (x-axis) which are constructed from a certain number of original predictors (right-hand y-axis). The vertical dashed lined at 37 tells us the optimal number of non-intercept terms retained where marginal increases in GCV 

```{r eval=FALSE, fig.cap="Figure 4: Model summary capturing GCV $R^2$ (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 37 non-intercept terms were retained which are based on 26 predictors. Any additional terms retained in the model, over and above these 37, results in less than 0.001 improvement in the GCV $R^2$."}
plot(mars1, which =1)
```

In addition to pruning the number of knots, `earth::earth()` allows us to also assess potential interactions between different hinge functions. The following illustrates by including a `degree = 2` argument. You can see that now our model includes interaction terms between multiple hinge functions (i.e. `h(Year_Built-2003)*h(Gr_Liv_Area-2274)`) is an interaction effect for those houses built prior to 2003 and have less than 2,274 square feet of living space above ground).

```{r}
# Fit a basic MARS model
library(earth)
mars2 <- earth(
  Sale_Price ~ .,  
  data = ames_train,
  degree = 2
)

# check out the first 10 coefficient terms
summary(mars2) %>% .$coefficients %>% head(10)
##                                             Sale_Price
## (Intercept)                               242611.63686
## h(Gr_Liv_Area-2945)                          144.39175
## h(2945-Gr_Liv_Area)                          -57.71864
## h(Year_Built-2003)                         10909.70322
## h(2003-Year_Built)                          -780.24246
## h(Year_Built-2003)*h(Gr_Liv_Area-2274)        18.54860
## h(Year_Built-2003)*h(2274-Gr_Liv_Area)       -10.30826
## h(Total_Bsmt_SF-1035)                         62.11901
## h(1035-Total_Bsmt_SF)                        -33.03537
## h(Total_Bsmt_SF-1035)*Kitchen_QualTypical    -32.75942
```

#### Tuning {#MARS_TUNE}

Since there are two tuning parameters associated with our MARS model: the degree of interactions and the number of retained terms, we need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of cross-validated performance on the training data rather than an actual k-fold cross validation process). As in previous tutorials, we will perform a cross-validated grid search to identify the optimal mix. Here, we set up a search grid that assesses 30 different combinations of interaction effects (`degree`) and the number of terms to retain (`nprune`).

```{r}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3,
  nprune = seq(2, 100, length.out = 10) %>% floor()
)
```

We can use __caret__ to perform a grid search using 10-fold cross-validation. The model that provides the optimal combination includes second degree interactions and retain 34 terms. The cross-validated RMSE for these models are illustrated in Figure 5 and the optimal model's cross validated RMSE is $24,021.68$

```{r Fig.cap="Figure 5: Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains 34 terms and includes up to 2nd degree interactions."}
# for reproducibility
set.seed(123)

# cross validation model
library(caret)
tuned_mars <- train(
  x = subset(ames_train, select= -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# best model
tuned_mars$bestTune
#    nprune degree
# 14     34      2

# plot residuals
ggplot(tuned_mars)
```

The above grid search helps to focus where we can further refine our model tuning. As a next step, we could perform a grid search that focuses in on a refined grid space for `nprune` (i.e. comparing 25-40 terms retained). However, for brevity we will leave this as an exercise for the reader.

So how does this compare to some other linear models for the Ames housing data? The following table compares the cross-validated RMSE for our tuned MARS model to a regular multiple regression model along with tuned principal component regression (PCR), partial least squares (PLS), and regularized regression (elastic net) models. By incorporating non-linear relationships and interaction effects, the MARS model provides a substantial improvement over the previous linear models that we have explored.

> Notes: Notice that we standardize the features for the linear model but we did not for the MARS model. Whereas linear models tend to be sensitive to the scale of the features. MARS models are not.

```{r}
# multiple regression
set.seed(123)
cv_model1 <- train(
  Sale_Price ~.,
  data = ames_train,
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method ="cv", number =10),
  preProcess = c("zv", "center", "scale")
)

# prncipal component regression
set.seed(123)
cv_model2 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# partial least squares regression
set.seed(123)
cv_model3 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# regularized regression
set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 10
)

# extract out of sample perfromance measrues

library(kableExtra)
summary(resamples(list(
  Multiple_regression = cv_model1, 
  PCR = cv_model2, 
  PLS = cv_model3,
  Elastic_net = cv_model4,
  MARS = tuned_mars
  )))$statistics$RMSE %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

```

#### Feature interpretation {#MARS_FI}

MARS models via `earth::earth()` include a backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. This total reduction is used as the variable importance measure (`value = "gcv`). Since MARS will automatically include and exclude terms during the pruning process, , it essentially performs automated feature selection. If a predictor was never used in any of the MARS basis functions in the final model (after pruning), it has an importance value of zero. This is illustrated in Figure 6 where 27 features have $>0$ importance values while the rest of the feature have an importance value of zero since they were not included in the final model.

Alternatively, you can also monitor the change in the residual sum of squares (RSS) as terms are added (`value = "rss`); however, you will see very little difference between these methods.

```{r}
# variable importance plots
library(vip)
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value ="gcv")+ ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value ="rss")+ ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

It is important to realize that variable importance will only measure the impact of the prediction error as features are included; however, it does not measure the impact for particular hinge functions created for a given feature. For example,  in Figure 6, we see that `Gr_Liv_Area` and `Year_Built` are the two most influential variables; however, variable importance does not tell us how our model is treating the non-linear patterns for each feature. 

Also, if we look at the interaction terms, our model retained, we see interactions between different hinge functions for `Gr_Liv_Area` and `Year_Built`.

```{r}
coef(tuned_mars$finalModel) %>% 
  broom::tidy() %>% 
  filter(str_detect(names, "\\*"))
## # A tibble: 16 x 2
##    names                                                   x
##    <chr>                                               <dbl>
##  1 h(Year_Built-2003) * h(Gr_Liv_Area-2274)           18.7  
##  2 h(Year_Built-2003) * h(2274-Gr_Liv_Area)          -10.9  
##  3 h(Total_Bsmt_SF-1035) * Kitchen_QualTypical       -33.1  
##  4 NeighborhoodEdwards * h(Gr_Liv_Area-2945)        -507.   
##  5 h(Lot_Area-4058) * h(3-Garage_Cars)                -0.791
##  6 h(2003-Year_Built) * h(Year_Remod_Add-1974)         7.00 
##  7 Overall_QualExcellent * h(Total_Bsmt_SF-1035)     104.   
##  8 NeighborhoodCrawford * h(2003-Year_Built)         424.   
##  9 h(Lot_Area-4058) * Overall_CondFair                -3.29 
## 10 Overall_QualAbove_Average * h(2003-Year_Built)    136.   
## 11 h(Lot_Area-4058) * Overall_CondGood                 1.35 
## 12 Bsmt_ExposureNo * h(Total_Bsmt_SF-1035)           -22.5  
## 13 NeighborhoodGreen_Hills * h(5-Bedroom_AbvGr)    27362.   
## 14 Overall_QualVery_Good * Bsmt_QualGood          -18641.   
## 15 h(2003-Year_Built) * Sale_ConditionNormal         192.   
## 16 h(Lot_Area-4058) * h(Full_Bath-2)                   1.61
```


To better understand the relationship between these features and `Sale_Price`, we can create partial dependence plot (PDPs) for each feature individually and also an interaction PDF. The individual PDPs illustrate that our model found that one knot in each feature provides the best fit.

For `Gr_Liv_Area`, as homes exceed 2,945 square feet, each additional square foot demands a higher marginal increase in sale price than homes with less that 2,945 square feet. Similarly, for homes built after 2003, there is a greter marginal effect on sales price based on the age of the home than for homes built priot to 2003. 

The interaction plot (far right plot) illustrates the strong effect these two features have when combined.

```{r eval=FALSE, fig.cap="Figure 7: Partial dependence plots to understand the relationship between `Sale_Price` and the `Gr_Liv_Area` and `Year_Built` features. The PDPs tell us that as Gr_Liv_Area increases and for newer homes"}

p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% ggplot2::autoplot()
p2 <- partial(tuned_mars, pred.var = "Year_Built", grid.resolution = 10) %>% ggplot2::autoplot()

# p3 <- partial(tuned_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>% 
#   plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

#### Final thoughts {#MARS_SUM}

MARS provides a great stepping stone into nonlinear modeling and tends to be fairly intuitive due to being closely related to multiple regression techniques. They are also easy to train and tune. This tutorial illustrated how incorporating non-linear relationships via MARS modeling greatly improved predictive accuracy on our Ames housing data. The following summarizes some of the advantages and disadvantages discussed regarding MARS modeling:

__Advantages__:

- Accurate if the local linear relationships are correct.
- Quick computation.
- Can work well even with large and small data sets.
- Provides automated feature selection.
- The non-linear relationship between the features and response are fairly intuitive.
- Can be used for both regression and classification problems.
- Does not require feature standardization.

__Disadvantages__:
- Not accurate if the local linear relationships are incorrect.
- Typically not as accurate as more advanced non-linear algorithms (random forests, gradient boosting machines).
- The earth package does not incorporate more advanced spline features (i.e. Piecewise cubic models).
- Missing values must be pre-processed.

#### Learning more {#MARS_LM}
This will get you up and running with MARS modeling. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:

- An Introduction to Statistical Learning, Ch. 7
- Applied Predictive Modeling, Ch. 7
- Elements of Statistical Learning, Ch. 5
- [Notes on the earth package by Stephen Milborrow](http://www.milbo.org/doc/earth-notes.pdf)


### Regression trees & bagging

Basic regression trees partition a dataset into small groups and then fit a simple model for each subgroup. Unfortunately, a single tree model tends to be highly unstable and a poor predictor. However, by bootstrap aggregating ( __bagging__) regression trees, this technique can become quite powerful and effective. Moreover, this provides the fundamental basis of more complex tree-based models such as random forests and _gradient boosting machines_. This tutorial will get you started with regression trees and bagging.

#### tl;dr

This tutorial services as an introduction to the Regression Decision Trees. This tutorail will cover the following material:

- [Replication Requirements](#RT_RR): What you’ll need to reproduce the analysis in this tutorial.
- The idea: A quick overview of how regression trees work.
- [Basic implementation](#RT_IMP): Implementing regression trees in R.
- [Tuning](#RT_TUNE): Understanding the hyperparameters we can tune.
- [Bagging](#RT_BAGG): Improving performance by fitting many trees.
- [Learning more](#RT_MORE): Where you can learn more.

#### Replication Requirements {#RT_RR}

This tutorial leverages the following packages. Most of these packages are playing a supportive role while the main emphasis will be  

```{r}
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
```

To illustrate various regularization concepts we will use the Ames Housing data that has been included in the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop=.7)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```



#### Basic Implementation {#RT_IMP}

__The Idea__

There are many methodologies for constructing regression trees but one of the oldest is known as the classification and regression tree (CART) approach developed by Breiman et al. (1984). This tutorial focuses on the regression part of CART. Basic regression trees partition a data set into smaller subgroups and then fit a simple constant for each observation in the subgroup. The partitioning is achieved by successive binary partitions (aka _recursive partitioning_) based on the different predictors. The constant to predict is based on the average response values for all observations that fall in that subgroup.

or example, consider we want to predict the miles per gallon a car will average based on cylinders (`cyl`) and horsepower (`hp`). All observations go through this tree, are assessed at a particular node, and proceed to the left if the answer is ?gyes?h or proceed to the right if the answer is ?gno?h. So, first, all observations that have 6 or 8 cylinders go to the left branch, all other observations proceed to the right branch. Next, the left branch is further partitioned by horsepower. Those 6 or 8 cylinder observations with horsepower equal to or greater than 192 proceed to the left branch; those with less than 192 hp proceed to the right. These branches lead to terminal nodes or leafs which contain our predicted response value. Basically, all observations (cars in this example) that do not have 6 or 8 cylinders (far right branch) average 27 mpg. All observations that have 6 or 8 cylinders and have more than 192 hp (far left branch) average 13 mpg.

This simple example can be generalized to state we have a continuous response variable $Y$ and two inputs $X_1$ and $X_2$. The recursive partitioning results in three regions ($R_1, R_2, R_3$) where the model predicts $Y$ with a constanct $c_m$ for region $R_m$:

$$
\hat{f}(X) = \Sigma_{m=1}^3 I(X_1, X_2) \in R_m...(1)

$$

Howeer, an important question remains of how to grow a regression tree.

__Deciding on splits__

First, it is important to realize the partitioning of variables are done in a top-down, _greedy_ fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. But how are these partions made? The model begins with the entire data set, S, and searches every distinct value of every input variable to find the predictor and split value that partitions the data into two regions ($R_1 and $_2$) such that the overall sums of squares error are minimized:

$$
minimize \ (SSE = \Sigma_{i \in R_1}(y_i-c_1)^2+
\Sigma_{i \in R_2}(y_i-c_2)^2)...(2)
$$

Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. This process is continued until some stopping criterion is reached. What results is, typically, a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, leading to poor performance on unseen data.

For example, using the well-known Boston housing data set, I create three decision trees based on three different samples of the data. You can see that the first few partitions are fairly similar at the top of each tree; however, they tend to differ substantially closer to the terminal nodes. These deeper nodes tend to overfit to specific attributes of the sample data; consequently, slightly different samples will result in highly variable estimate/predicted values in the terminal nodes. By pruning these lower level decision nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data.

___Cost complexity criterion__

There is often a balance to be achieved in the depth and complexity of the tree to optimize predictive performance on some unseen data. To find this balance, we typically grow a very large tree as defined in the previous section and then prune back to find an _optimal subtree_.

We find the optimal subtree by using a _cost complexity parameter_ ($\alpha$) that penalizes our objective function in Eq.2 for the number of terminal nodes of the tree (T) as Eq.3.

$$
minimize (SSE + \alpha|T|)
$$

For a given value of $\alpha$, we find the smallest pruned tree that has the lowest penalized error. If you are familiar with [regularized regression](http://uc-r.github.io/regularized_regression), you will realize the close association to the [lasso $L_1$ norm penalty](http://uc-r.github.io/regularized_regression#lasso). 

As with these regularization methods, smaller penalties tend to produce more complex models, which result in larger trees. Whereas larger penalties result in much smaller trees.

consequently, as a tree grows larger, the reduction in the SSE must be greater than the cost compexity penalty. Typically, we evaluate multiple models across a spectrum of $\alpha$ and use cross-validation to identify the optimal $\alpha$ and, therefore, the optimal subtree.

__Strenghts and weakness__

There are several __advantages__ to regression trees:

- They are very interpretable.
- Making prediction is fast (no compicated calculations, just looking up constants in the trees)
- It?fs easy to understand what variables are important in making the prediction. The internal nodes (splits) are those variables that most largely reduced the SSE.
- If some data is missing, we might not be able to go all the way down the tree to a leaf, but we can still make a prediction by averaging all the leaves in the sub-tree we do reach.
- The model provides a non-linear ?gjagged?h response, so it can work when the true regression surface is not smooth. If it is smooth, though, the piecewise-constant surface can approximate it arbitrarily closely (with enough leaves).
- There are fast, reliable algorithms to learn these trees.

But there are also some significant __weaknesses__:

- Single regression trees have high variance, resulting in unstable predictions (an alternative subsample of traiinng data can significantly change the terminal nodes)
- Due to high variance, single regreesion trees have poor predictive accuracy

__Implementation details__

We can fit a regression tree using `rpart` and then visualize it using `rpart.plot`. The fitting process and the visual output of regression trees and classification trees are very similar. Both use the formula method for expressing the model (similar to `lm`). 

However, when fitting a regression tree, we need to set `method = "anova"`. By default, `rpart` will make an intelligent guess as to what the method value should be based on the data type of your response column, but it is recommended that you explicitly set the method for reproducibility reasons (since the auto-guess may change in the future).

```{r}
m1 <- rpart::rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova"
)

m1 %>% rpart.plot::rpart.plot()
```

Once we have fit our we can take a peak at the `m1` output. This just explains steps of the splits. For example, we start with 2051 observations at the root node (very beginning) and the first variables we split on (the first variable that optimizes a reduction in SSE) is `Overall_Qual`. We see at the first node all observations with `Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good` go to the 2nd (`2`)) branch. The total number of observations that follow this branch ($1699$), their average sales price ($156147.10$) and SSE ($4.001092e+12$) are listed. If you look for the 3rd branch (`3`)) you will see that $352$ observations with Overall_Qual=Very_Good,Excellent,Very_Excellent follow this branch and their average sales prices is 304571.10 and the SEE in this region is 2.874510e+12. Basically, this is telling us the most important variable that has the largest reduction in SEE initially is `Overall_Qual` with those homes on the upper end of the quality spectrum having almost double the average sales price.

```{r}
m1
```

We can visualize our model with `rpart.plot`. `rpart.plot` has many plotting options, which we will leave to the reader to explorer. However, in the default print, it will show the percentage of data that fall to that node and the average sales price for that branch. One thing you may notice is that this tree contains 11 internal nodes resulting in 12 terminal nodes. Basically, this tree is partitioning on 11 variables to produce its model. However, there are 80 variables in `ames_train`. So what happened?

```{r}
library(rpart.plot)
rpart.plot(m1)
```

Behind the scenes, `rpart` is automatically applying a range of cost compexity ($\alpha$ values to prune the tree). To compare the error for each $\alpha$ value, `rpart` performs a 10-fold cross validation so that the error associated with a given $\alpha$ value is computed on the hold-out validation data. In this example we find diminishing returns after 12 terminal nodes as illustrated below (y-axis is cross validation error, lower x-axis is cost complexity $\alpha$ value, upeer x-axis is the number of terminal nodes (tree size =$|T|$). You may also notice the dashed line which goes through the point $|T|=9$. Breiman et al. (1984) suggested that in actual practice, it is common to instead use the smallest tree within 1 standard deviation of the minimum cross validation error (aka the 1-SE rule).  Thus, we could use a tree with 9 terminal nodes and reasonably expect to experience similar results within a small margin of error.

```{r}
rpart::plotcp(m1)
```

To illustrate the point of selecting a tree with 12 terminal nodes (or 9 if you go by the 1-SE rule), we can force `rpart` to generate a full tree by using `cp=0` (no penalty results in a fully grown tree).  we can see that after 12 terminal nodes, we see diminishing returns in error reduction as the tree grows deeper. Thus, we can significantly prune our tree and still achieve minimal expected error.

```{r}
library(rpart)

m2 <- rpart(
  formula = Sale_Price ~.,
  data = ames_train,
  method = "anova",
  control = list(cp = 0, xval = 10)
)

plotcp(m2)
abline(v = 12, lty = "dashed")
```

So, by default, `rpart` is performing some automated tuning, with an optimal subtree of 11 splits, 12 terminal nodes, and a cross-validated error of 0.272 (note that this error is equivalent to the PRESS statistic but not the MSE). However, we can perform additional tuning to try improve model performance.

```{r}
m1$cptable %>% broom::tidy() 
```


#### Tuning {#RT_TUNE}

In addition to the cost complexity paramter($\alpha$), it is also common to tune:

- `minsplit`: the minimum nunber of data points required to attempt a split before it is forced to create a terminal node. The default is 20. Making this smaller allows for terminal nodes that may contain only a handful of observations to create the predicted value.

- `maxdepth`: the maximum number of internal nodes between the root node and the terminal nodes. The default is 30, which is quite liberal and allows for fairly large trees to be built.

`rpart` uses special `control` argument where we provide a list of hyperparameter values. For example, if we wanted to assess a model with `minsplit`=10 and `maxdepth`=12, we could execute the following:

```{r}
m3 <- rpart(
  formula = Sale_Price ~.,
  data = ames_train,
  method = "anova",
  control = list(minsplit=10, maxdepth=12, xval=10)
)

m3$cptable %>% broom::tidy()
```

Although useful, this approach requires you to manually assess multiple models. Rather, we can perform a grid search to automatically search across a range of differently tuned models to identify the optimal hyperparameter setting.

To perform a grid search we first create our hyperparameter grid. In this example, I search a range of `minsplit` from 5-20 and vary `maxdepth` from 8-15 (since our original model found an optimal depth of 12). What results is 128 different combinations, which requires 128 different models.

```{r}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)

head(hyper_grid)
```

To automate the modeling we simply set up a `for` loop and iterate through each minsplit and maxdepth combination. We save each model into its own list item.

```{r}
models <- list()

for (i in 1:nrow(hyper_grid)){
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  # train a model and store in the list
  models[[i]] <- rpart(
    formula = Sale_Price~.,
    data = ames_train,
    method = "anova",
    control = list(minsplit = minsplit,
                   maxdepth = maxdepth)
  )
}

# option - purrr::map2()

h <- hyper_grid %>% 
  mutate(models=map2(minsplit, maxdepth,
    ~rpart(
      formula = Sale_Price~.,
      data = ames_train,
      method = "anova",
      control = list(minsplit = .x,
                     maxdepth = .y)
    ),
    tidy = broom::tidy(models)))

h$models[[1]]$cptable[,"xerror"]
```

We can now create a function to extract the minimum error associated with the optimal cost complexity $\alpha$ value for each model. After a little data wrangling to extract the optimal $\alpha$ value and its respective error, adding it back to our grid, and filter for top 5 minimal error values we see that the optimal model makes a slight improvement over our ealier model (xerror of 0.242 versus 0.272).

```{r eval=FALSE}
# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"]}


h %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)) %>% 
  arrange(error) %>%
  top_n(-5, wt = error)
##   minsplit maxdepth        cp     error
## 1       15       12 0.0100000 0.2419963
## 2        5       13 0.0100000 0.2422198
## 3        7       10 0.0100000 0.2438687
## 4       17       13 0.0108982 0.2468053
## 5       19       13 0.0100000 0.2475141
```

If we were satisfied with these results, we could apply this final optimal model and predict on our test set. The final RMSE is $39145.39$ which suggests that, on average, our predicted sales prices are about $39,145 off from the actual sales price.

```{r}
optimal_tree <- rpart(
  formula = Sale_Price ~.,
  data = ames_train,
  method = "anova",
  control = list(minsplit = 11,
                 maxdepth = 8,
                 cp       = 0.01)
)

pred <- predict(optimal_tree, newdata = ames_test)
RMSE(pred = pred, obs = ames_test$Sale_Price)
```


#### Bagging {#RT_BAGG}

__The Idea__

As previously mentioned, single tree models suffer from high varianace. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploit the variability of single trees in a way that can significantly improve performance over and above that of _Bootstrap Aggregate (Bagging)_ is one such approach (oroginally proposed by [Breiman, 1996](https://link.springer.com/article/10.1023%2FA%3A1018054314350))

Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any tone tree and reduces overfitting, which imrpoves predictive performance. Bagging follows three simple steps:

1. Create m [bootstrap samples](http://uc-r.github.io/bootstrapping) from the training data.Boostrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.
2. For each boostrap sample train a single, unpruned regression tree.
3. Average individual predictions from each tree to create an overall average predicted value.

```{r eval=FALSE}
knitr::include_graphics("C:/Protected/Data Science/UC Business Analytics/image/bagging3.png")
```

This process can actually be applied to any regression or classification model; however, it provides the greatest improvement for models that have high variance. For example, more stable parametric models such as linear regression and multi-adaptive regression splines tend to experience less improvement in predictive performance.

One benefit of bagging is that, on average, a bootstrap sample will contain 63% of the training data. This leaves about 33% of the data out of the bootstrapped sample. We call this is the __out-of-bag (OOB)__ sample. We can use the OOB observations to estimate the model's accuracy, creating a natural cross-validation process.

__Bagging with `ipred`__

Fitting a bagged tree model is quite simple. Instead of using `rpart` we use `opred::bagging`. We use `coob=TRUE` to use the OOB sample to estimate the test error. We see that our initial estimate error is close to $3K less than the test error we achieved with our singple optimal tree (36543 vs 39145).

```{r}
# make boostrapping reproducible
set.seed(123)

# train bagged model
bagged_m1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  coob = TRUE
)

bagged_m1
## 
## Bagging regression trees with 25 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     coob = TRUE)
## 
## Out-of-bag estimate of root mean squared error:  36543.37

```

One thing to note is that typically, the more trees the better. As we add more trees we are averaging over more high variance single trees. What result is that early on, we see a dramatic reduction in variance (and hence our error) and eventually the reduction in error will flatline signaling an appropriate number of trees to create a stable model.

Rarely you need more than 50 trees to stabilize the error.

By default, `bagging` performs 25 booststrap samples and trees but we may require more. We can assess the error versus number of trees as below. We see that the error is stabilizing at about 25 trees so we will likely not gain much improvement by simply bagging more trees.

```{r}
# assess 10-50 bagged trees
ntree <- 10:50

# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length=length(ntree))

for (i in seq_along(ntree)){
  # reproducibility
  set.seed(123)
  
  # perform bagged model
  model <- bagging(
    formula = Sale_Price ~.,
    data = ames_train,
    coob = TRUE,
    nbagg = ntree[i]
  )
  
  # get OOB error
  rmse[i] <- model$err
}

plot(ntree, rmse, type = 'l', lwd = 2)
abline(v = 25, col = "red", lty = "dashed")
```

__Bagging with `caret`__

Bagging with `ipred` is quite simple; however, there are some additional benefits of bagging with `caret`.

1. It is easier to perform cross-validation. Although we can use the OOBE error, performing cross validation will provide a more robust understanding of the true expected test error.
2. We can assess variable important across the bagged trees.

Here, we performa a 10-fold corss-validated model. We see that the cross-validated RMSE is $36,477$. We also assess the top 20 variables from our model.

Variable importance for regression trees is measured by assessing the total amount SSE is decreased by splits over a given predictor, averaged over all $m$ trees. The predictors with the largest average impact to SSE are considered most important. The importance value is simple the relative mean decrease in SSE compared to the most important variables.

```{r}
# specify 10-fold cross validation
library(caret)
ctrl <- trainControl(method = "cv", number = 10)

# CV bagged model
bagged_cv <- train(
  Sale_Price ~.,
  data = ames_train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
)

# assess results
bagged_cv

## Bagged CART 
## 
## 2051 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1845, 1847, 1845, 1846, 1847, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   36477.25  0.8001783  24059.85

# plot most important variables
plot(varImp(bagged_cv), 20)  

```



#### Learning more {#RT_MORE}

Decision trees provide a very intuitive modeling approach that have several, flexible benefits. Unfortunately, they suffer from high variance; however, when you combine them with bagging you can minimize this drawback. Moreover, bagged trees provides the fundamental basis for more complex and powerful algorithms such as random forests and gradient boosting machines. To learn more I would start with the following resources listed in order of complexity:

- An Introduction to Statistical Learning
- Applied Predictive Modeling
- [Classification and Regression Trees](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)
- The Elements of Statistical Learning
- [Bagging Predictors](https://link.springer.com/article/10.1023%2FA%3A1018054314350)

## Bagging

We learned about botstrapping as a resampling procedure,  which creates b new bootstrap samples by drawing samples with replacement of the original training data. This chapter illustrates how we can use bootstrapping to create an ensemble of predictions. Bootstrap aggregating, also called bagging, is one of the first ensemble algorithms28 machine learning practitioners learn and is designed to improve the stability and accuracy of regression and classification algorithms. By model averaging, bagging helps to reduce variance and minimize overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.

### Prerequisites

```{r}
# Helper packages
library(dplyr)       # for data wrangling
library(ggplot2)     # for awesome plotting
library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees
```

We will continue to illustrate the main concepts with the `ames_train` data set created in Section 2.7.

### Why and when bagging works

_Bootstrap aggregating_ (bagging) prediction models is a general method for fitting multiple versions of a prediction model and then combining (or ensembling) them into an aggregated prediction (Breiman 1996a). Bagging is a fairly straight forward algorithm in which b bootstrap copies of the original training data are created, the regression or classification algorithm (commonly referred to as the base learner) is applied to each bootstrap sample and, in the regression context, new predictions are made by averaging the predictions together from the individual base learners. When dealing with a classification problem, the base learner predictions are combined using plurality vote or by averaging the estimated class probabilities together.

This is represented in Equation (10.1) where $X$ is the recored for which we want to generate a prediction $\hat{f_{bag}}$ is the bagged prediction, and $\hat{f_b(X)}$ are the prediction from the individual base learners.

$$
\hat{f_{bag}} = \hat{f_1(X)}+...+\hat{f_b(X)}...(10.1)
$$

Because of the aggregation process, bagging effectively reduces the variance of an individual base learner (i.e., averaging reduces variance); however, bagging does not always improve upon an individual base learner. As discussed in Section 2.5, some models have larger variance than others. Bagging works especially well for unstable, high variance base learners—algorithms whose predicted output undergoes major changes in response to small changes in the training data (Dietterich 2000b, 2000a). This includes algorithms such as decision trees and KNN (when k is sufficiently small). However, for algorithms that are more stable or have high bias, bagging offers less improvement on predicted outputs since there is less variability (e.g., bagging a linear regression model will effectively just return the original predictions for large enough $b$).

The general idea behind bagging is referred to as the “wisdom of the crowd” effect and was popularized by Surowiecki (2005). It essentially means that the aggregation of information in large diverse groups results in decisions that are often better than could have been made by any single member of the group. The more diverse the group members are then the more diverse their perspectives and predictions will be, which often leads to better aggregated information. Think of estimating the number of jelly beans in a jar at a carinival. While any individual guess is likely to be way off, you’ll often find that the averaged guesses tends to be a lot closer to the true number.

This is illustrated in Figure 10.1, which compares bagging $b=100$ polynomial regression moels, MARS models and CART decision trees. You can see that the low variance base learner (polynomial regression) gains very little from bagging while the higher variance learner (decision trees) gains significantly more. Not only does bagging help minimize the high variability (instability) of single trees, but it also helps to smooth out the prediction surface.

```{r echo=FALSE, eval=FALSE, fig.cap="Figure 10.1: The effect of bagging 100 base learners. High variance models such as decision trees (C) benefit the most from the aggregation effect in bagging, whereas low variance models such as polynomial regression (A) show little improvement."}

knitr::include_graphics("bagging-multiple-models-1.png")

```

Optimal performance is often found by bagging 50–500 trees. Data sets that have a few strong predictors typically require less trees; whereas data sets with lots of noise or multiple strong predictors may need more. Using too many trees will not lead to overfitting. However, it’s important to realize that since multiple models are being run, the more iterations you perform the more computational and time requirements you will have. As these demands increase, performing k-fold CV can become computationally burdensome.

A benefit to creating ensembles via bagging, which is based on resampling with replacement, is that it can provide its own internal estimate of predictive performance with the out-of-bag (OOB) sample (see Section 2.4.2). The OOB sample can be used to test predictive performance and the results usually compare well compared to k-fold CV assuming your data is sufficiently large (say $n>1000$). Consequently, as your data sets become larger and your bagging iterations increase, it is common to use the OOB error estimate as a proxy for predictive performance. 

### Implementation

 Chapter 9, we saw how decision trees performed in predicting the sales price for the Ames housing data. Performance was subpar compared to the MARS (Chapter 7) and KNN (Chapter 8) models we fit, even after tuning to find the optimal pruned tree. Rather than use a single pruned decision tree, we can use, say, 100 bagged unpruned trees (by not pruning the trees we’re keeping bias low and variance high which is when bagging will have the biggest effect). As the below code chunk illustrates, we gain significant improvement over our individual (pruned) decision tree (RMSE of 26,462 for bagged trees vs. 41,019 for the single decision tree).

The` bagging()` function comes from the ipred package and we use nbagg to control how many iterations to include in the bagged model and coob = TRUE indicates to use the OOB error rate. By default, `bagging()` uses `rpart::rpart()` for decision tree base learners but other base learners are available. Since bagging just aggregates a base learner, we can tune the base learner parameters as normal. Here, we pass parameters to rpart() with the control parameter and we build deep trees (no pruning) that require just two observations in a node to split.

```{r}
# make bootstrapping reproducible
set.seed(123)

# train bagged model
ames_bag1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag1
## 
## Bagging regression trees with 100 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     nbagg = 100, coob = TRUE, control = rpart.control(minsplit = 2, 
##         cp = 0))
## 
## Out-of-bag estimate of root mean squared error:  25528.78
```
One thing to note is that tyipically, the more trees the better. As we add more trees we’re averaging over more high variance decision trees. Early on, we see a dramatic reduction in variance (and hence our error) but eventually the error will typically flatline and stabilize signaling that a suitable number of trees has been reached. Often, we need only 50–100 trees to stabilize the error (in other cases we may need 500 or more). For the Ames data we see that the error is stabilizing with just over 100 trees so we’ll likely not gain much improvement by simply bagging more trees.

Unfortunately, `bagging()` does not provide the RMSE by tree so to produce this error curve we iterated over nbagg values of 1–200 and applied the same `bagging()` function above.

```{r echo=FALSE, eval=FALSE, fig.cap="Figure 10.2: Error curve for bagging 1-200 deep, unpruned decision trees. The benefit of bagging is optimized at 187 trees although the majority of error reduction occurred within the first 100 trees."}

knitr::include_graphics("image/n-bags-plot-1.png")

```

We can also apply bagging within __caret__ and use 10-fold CV to see how well our ensemble will generalize. We see that the cross-validated RMSE for 200 trees is similar to the OOB estimate (difference of 495). However, using the OOB error took 58 seconds to compute wheres performing the following 10 fold CV took 26 minutes on our machine!

```{r}
ames_bag2 <- train(
  Sale_Price ~., 
  data = ames_train,
  meth0d = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,
  control = rpart.control(minsplit = 2, cp = 0)
)

ames_bag2
```

### Easily Parallelize

As stated in Section 10,2, bagging can become computationally 

## Random forests 

[Bagging (bootstrap aggregating) regression trees](http://uc-r.github.io/regression_trees) is a technique that can turn a single tree model with high variance and poor preditive power into a fairly accurate prediction function.

However, bagging trees typically suffers from tree correlation, which reduces the overall performance of the model.

Random forests are a modification of bagging that builds a large collection of _de-correlated_ trees and have become a very popular "out-of-the-box" learning algorithm that enjoys good predictive peformance. This tutotiral will cover the fundamentals of random forests.

#### tl;dr

This tutorial serves as an introduction to the random forests. This tutorial will cover the following material:

- [Replication Requirements](#RF_RR): What you?fll need to reproduce the analysis in this tutorial.
- [The idea](#RF_Idea): A quick overview of how random forests work.
- [Basic implementation](#RF_BI): Implementing regression trees in R.
- [Tuning](#RF_Tune): Understanding the hyperparameters we can tune and performing grid search with ranger & h2o.
- [Predicting](#RF_Pred): Apply your final model to a new data set to make predictions.
- [Larning more](#RF_LM): Where you can learn more

#### Replication Requirements {#RF_RR}

This tutorial leverages the following packages. Some of these packages play a supporting role; however, we demonstrate how to implement random forests with several different packages and discuss the pros and cons to each.

```{r}

library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform

```

To illustrate various regularization concepts we will use the Ames Housing data that has been included in the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)

amessplit <- initial_split(AmesHousing::make_ames(), prop=.7)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```

#### The idea {#RF_Idea}

Random forests are built on the same fundamental principles as decision trees and bagging (check out this [tutorial](http://uc-r.github.io/regression_trees) if you need a refresher on these techniques). 

Bagging trees introduces a random component in to the tree building process that reduces the variance of a single tree's prediction and improve predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships.

For example, if we create six decision trees with different bootstrapped samples of the Boston housing data, we see that the top of the trees all have a very similar structure. Although there are 15 predictor variables to split on, all six trees have both `lstat` and `rm` variables driving the first few splits.

This characteristic is known as _tree correlation_ and prevents bagging from optimally reducing variance of the predictive values. In order to reduce variance further, we need to minimize the amount of correlation between the trees. 

This can be achieved by injecting more randomness into the tree-growing process. random forecast achieve this in two ways:

1. __Bootstrap__: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and somewhat decorrelates them.

2. __Split-variable randomization__: each time a split is to be performed, the search for the split variable is limited to a random subset of _m_ of the _p_ variables. 

For regression trees, typical default values are $m = \frac{p}{3}$, but this should be considere a tuning parameter. When $m=p$, the randomization amounts to using only step 1 and is the same as __bagging__.

The basic algorthim for a regression random forest can be generalized to the following:

```{r eval=FALSE}

1.  Given training data set
2.  Select number of trees to build (ntrees)
3.  for i = 1 to ntrees do
4.  |  Generate a bootstrap sample of the original data
5.  |  Grow a regression tree to the bootstrapped data
6.  |  for each split do
7.  |  | Select m variables at random from all p variables
8.  |  | Pick the best variable/split-point among the m
9.  |  | Split the node into two child nodes
10. |  end
11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune)
12. end

```

since the algorithm randomly selects a bootstrap sample to traini on __and__ predictors to use at each split, tree correlation will be lessened beyond bagged trees.

__OOB error vs test set error__

Similar to bagging, a natural benefit of the bootstrap resampling process is that random forests have an out-of-bag (OOB) sample that provides an eifficient and reasonable approximation of the test error. This provides a built-in validation set without an extra work on your part, and you do not need to sacrifice any of your training data to use for validation.

This makes identifying the number of trees required to stabilize the error rate during tuning more efficient; however, as illustrated below, some difference between the OOB erro and test error are expected.

Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. If you are comparing multiple models to one-another, you?fd want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages. So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation.

__Advantages & Disadvantages__
_Advantages:_

- Typically have very good performance
- Remarkably good ?gout-of-the box?h - very little tuning required
- Built-in validation set - don?ft need to sacrifice data for extra validation
- No pre-processing required
- Robust to outliers

_Disdvantages:_

- Can become slow on large data sets
- Although accurate, often cannot compete with advanced boosting algorithms
- Less interpretable


#### Basic implementation {#RF_BI}

There are over 20 random forest packages in R.1 To demonstrate the basic implementation we illustrate the use of the `randomForest` package, the oldest and most well known implementation of the Random Forest algorithm in R. However, as your data set grows in size `randomForest` does not scale well (although you can parallelize with `foreach`). Moreover, to explore and compare a variety of tuning parameters we can also find more effective packages. Consequently, in the [Tuning](#RF_Tune) section we illustrate how to use the `ranger` and `h2o` packages for more efficient random forest modeling.

`randomForest::randomForest` can use the formula or separate x, y matrix notation for specifying our model. Below we apply the default `randomForest` model using the formulaic specification.

The default random forest performs 500 trees and $\frac{features}{3} = 26$ randomly selected predictor variables at each split. Averaging across all 500 trees provides an OOB $MSE=659550782 (RMSE=25682)$

```{r}
# for reproducibility
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = Sale_Price ~.,
  data = ames_train
)

# m1.caret <- caret::train(
#   Sale_Price ~., 
#   data=ames_train, 
#   method = "rf",
#   preProcess = c("center", "scale", "zv"),
#   trcontrol = trainControl(method="cv"))
# https://www.rdocumentation.org/packages/caret/versions/6.0-82/topics/preProcess

# predicted value computation
# predRF <- predict(m1.caret, ames_test)

# RMSE computation
# sqrt(sum((ames_train$Sale_Price - predRF)^2))
```

Plotting the model will illustrate the error rate as we average across more trees and shows that our error rate stabilizies with around 100 trees but continues to decrease slowly around 300 or so trees.

```{r}
plot(m1)
```

The plotted error rate above is based on the OOB sample error and can be accessed directly at `m1$mse`. Thus, we can find which number of trees providing the lowest error rate, which is 344 trees providing an average home sales price error of $25,673$

```{r}
# number of trees with lowest MSE
which.min(m1$mse)

# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])

```

`randomForest` also allows us to use a validatio set to measure predictive accuracy if we did not want to use the OOB samples. Here we split our training set further to create a training and validation set.

We then supply the validation data in the `xtest` and `ytest` arguments. 

```{r}
# create training and validation data
set.seed(123)
valid_split <- initial_split(ames_train, .8)

# training data
ames_train_v2 <- analysis(valid_split)

# validation data
ames_valid <- assessment(valid_split)
x_test <- ames_valid[setdiff(names(ames_valid), "Sale_Price")]
y_test <- ames_valid$Sale_Price

rf_oob_comp <- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train_v2,
  xtest   = x_test,
  ytest   = y_test
)

# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare errror rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```

Random forests are one of the best "out-of-the-box" machine learning algorithms. They typically perform remarkably well with very little tuning required. For example, as we saw above,  we were able to get an RMSE of less than `$30K` without any tuning which is over a `$6K` reduction to the RMSE achieved with a fully-tuned [bagging model](http://uc-r.github.io/regression_trees#bag) and $4K reduction to to a fully-tuned [elastic net model](http://uc-r.github.io/regularized_regression#elastic). However, we can still seek improvement by tuning our random forest model.


#### Tuning {#RF_Tune}

Random forests are fairly easy to tune since there are only a handful of tuning parameters. Typically, the primary concern when starting out is tuning the number of candidate variables to select from each split. However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present:

- `ntree`: number of trees. We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets.

- `mtry`: the number of variables to randomly sample as candiates at each split. When `mtry`=p, the model equates to bagging. When `mtry`=1, the split variable is completely random, so all variable get a chance btu can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to $p$.

- `sampsize`:  the number of samples to train on. The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range.

- `nodesize`: minimum number of samples within the terminal nodes. Controls the complexity of the trees. Smaller node size allows for deeper, more complex trees and smaller node results in shallower trees. This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data).

- `maxnodes`: maximum number of terminal nodes. Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees.

__Initial tuning with randomForest__

If we are interested with just starting out and tuning the `mtry` parameter we can use `randomForest::tuneRF` for a quick and easy tuning assessment. 

`tuneRf` will start at a value of `mtry` that you supply and increase by a certain step factor until the OOB error stops improving be a specified amount. For example, the below starts with `mtry=5` and increases by a factor of 1.5 until the OOB error stops improving by 1%. Note that `tuneRF` requires a separate `xy` specification. We see that the optimal `mtry` value in this sequence is very close to the default `mtry` value of $\frac{features}{3}=26$.

```{r}
# names of features
features <- setdiff(names(ames_train), "Sale_Price")

set.seed(123)

m2 <- tuneRF(
  x = ames_train[features],
  y = ames_train$Sale_Price,
  ntreeTry = 500,
  mtryStart = 5,
  stepFactor = 1.5,
  improve = 0.01,
  trace = FALSE # to not show real-time progress
)

## -0.02973818 0.01 
## 0.0607281 0.01 
## 0.01912042 0.01 
## 0.02776082 0.01 
## 0.01091969 0.01 
## -0.01001876 0.01

```

__Full grid search with ranger__

To perfrom a larger grid search across several hyperparameters, we will need to create a grid and loop through each hyperparameter combination and evaluate the model. Unfortunately, this is where `randomForest` becomes quite inefficient since it does not scale well. Instead, we can use `ranger` wihch is a C++ implementation of Brieman's random forest algorithm, and as the folliwing illustrates, is over 6 times faster than `randomForest`.

```{r}
#randomForest speed
system.time(
  ames_randomForest <- randomForest(
    formula = Sale_Price ~.,
    data = ames_train,
    ntree = 500,
    mtry = floor(length(features)/3)
  )
)
##    user  system elapsed 
##  55.371   0.590  57.364

# ranger speed
system.time(
  ames_ranger <- ranger(
    formula = Sale_Price ~.,
    data = ames_train,
    num.tree = 500,
    mtry = floor(length(features)/3)
  )
)
##    user  system elapsed 
##   9.267   0.215   2.997
```

To perform the grid search, first we want to construct our grid of hyperparameters.We are going to search across 96 different models with varying `mtry`, minimum node size and sample size.

```{r}
# hyperparameter grid srearch

hyper_grid <- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sample_size = c(.55, .632, .7, .8),
  OOB_RMSE = 0
)
  
# total number of combinations
nrow(hyper_grid)
```

We look through each yperparameter combination and apply 500 trees since our previous examples illustrated that 500 was plenty to achieve a stable error rate. Also note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. Our OOB RMSE ranges between ~26,000-27,000. Our top 10 performing models all have RMSE values right around 26,000 and the results show that models with slighly larger sample sizes (70-80%) and deeper trees (3-5 observations in an terminal node) perform best. We get a full range of `mtry` values showing up in our top 10 so is does not look like that is over influential.

```{r}

for (i in 1:nrow(hyper_grid)){
  # train model
  model <- ranger(
    formula = Sale_Price ~.,
    data = ames_train,
    num.trees = 500,
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123)
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>% 
  head()
```

Although, random forests typically perform quite well with categorical variables in their original columnar form, it is worth checking to see if alternative encodings can increase performance. For example, the following one-hot encodes our categorical variables which produces 353 predictor variables versus the 80 we were using above. We adjust our `mtry` parameter to search from 50-200 random predictor variables at each split and re-perform our grid search. The results suggest that one-hot encoding does not improve performance.

```{r}
# one-hot encode our categorical variables
# caret package dummyVars()
one_hot <- dummyVars(~., ames_train, fullRank=FALSE)
ames_train_hot <- predict(one_hot, ames_train) %>% as.data.frame()

# makr ranger compatible names
names(ames_train_hot) <- make.names(names(ames_train_hot), allow_=FALSE)

#hyperparameter grid search --> same as above but with increased mtry values
hyper_grid_2 <- expand.grid(
  mtry       = seq(50, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = Sale.Price ~ ., 
    data            = ames_train_hot, 
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_2$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_2 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
##    mtry node_size sampe_size OOB_RMSE
## 1    50         3        0.8 26981.17
## 2    75         3        0.8 27000.85
## 3    75         5        0.8 27040.55
## 4    75         7        0.8 27086.80
## 5    50         5        0.8 27113.23
## 6   125         3        0.8 27128.26
## 7   100         3        0.8 27131.08
## 8   125         5        0.8 27136.93
## 9   125         3        0.7 27155.03
## 10  200         3        0.8 27171.37

```

Currently, the best random forest model we have found retains columnar categorical variables and uses `mtry` = 24, terminal node size of 5 observations, and a sample size of 80%. Lets repeat this model to get a better expectation of our error rate. We see that our expected error ranges between ~25,800-26,400 with a most likely just shy of 26,200.

```{r}
OOB_RMSE <- vector(mode="numeric", length=100)

for (i in seq_along(OOB_RMSE)){
  
  optimal_ranger <- ranger(
    formula = Sale_Price ~.,
    data = ames_train,
    num.trees = 500,
    mtry = 24,
    min.node.size = 5,
    sample.fraction =.8,
    importance = "impurity"
  )
   OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)
```

Furthermore, you may have noticed we set `imporance ="impurity` in the above modeling, which allows us to assess variable importance. Variable importance is measured by recording the decrease in MSE each time a variable is used as a node split in a tree.

The remaining error left in predictive accuracy after a node split is known as __node impurity__ and a variable that reduces this impurity is considered more imporant than those variables that do not. Consequently, we accumulate the reduction in MSE for each variable across all the trees and the variable with the greatest accumulated impact is considered the more important, or impactful. We see that `Overall_Qual` has the greatest impact in reducing MSE across our trees, followed by `Gr_Liv_Area`, `Garage_Cars`, etc.

```{r}
optimal_ranger$variable.importance %>% 
  tidy() %>% 
  dplyr::arrange(desc(x)) %>% 
  dplyr::top_n(25) %>% 
  ggplot(aes(reorder(names,x), x))+
  geom_col()+
  coord_flip()+
  ggtitle("Top 25 imporant variables")
```

__Full grid search with H2O__

If you ran the grid search code above you probably noticed the code took a while to run. Although `ranger` is computationally efficient, as the grid search space expands, the manual `for` loop process becomes less efficient. `h2o` is a powerful and efficient java-based interface that provides parallel distributed algorithms. Moreover, `h2o` allows for different optimal search paths in our grid search. This allows us to be more efficient in tuning our models. Here, I demonstrate how to tune a random forest model with `h2o`. Lets go ahead and start up `h2o`:

```{r eval=FALSE}
# start up h2o (I turn off progress bars when creating reports/tutorials)
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

First, we can try a comprehensive ( __full cartesian__) grid search, which means we will examine every combination of hyperparameter settings that we specify in `hyper_grid.h2o`. Here, we search across 96 models but since we perform a full cartesian search this process is not any faster than that which we did above. However, note that the best performing model has an OOB RMSE of 24504 ($\sqrt{6.0046E8}$), which is lower than what we achieved previously. This is because some of the default settings regarding minimum node size, tree depth, etc. are more ?ggenerous?h than ranger and randomForest (i.e. h2o has a default minimum node size of one whereas ranger and randomForest default settings are 5).

```{r eval=FALSE}
# create feature names
y <- "Sale_Price"
x <- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o <- as.h2o(ames_train)

# hyperparameter grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 30, by = 2),
  sample_rate = c(.55, .632, .70, .80)
)

# build grid search
grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x,
  y =y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  searchsearch_criteria = list(strategy = "Cartesian")
)

# collect the results and sor by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "rf_grid",
  sort_by = "mse",
  decreasing = FALSE
)

print(grid_perf)

```

Because of the combinatorial explosion, each additional hyperparameter that gets added to our grid search has a huge effect on the time to complete. Consequently, `h2o` provides an additional grid search path called __?gRandomDiscrete?h__, which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model.

For example, the following code searches a large ggrid search of 2,025 hyperparameter combinations. e create a random grid search that will stop if none of the last 10 models have managed to have a 0.5% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 600 seconds (30 minutes). Our grid search assessed 190 models and the best model (`max_depth` = 30, `min_rows` = 1, `mtries` = 25, `nbins` = 30, `ntrees` = 200, `sample_rate` = .8) achived an RMSE of 24686 ($\sqrt{6.094E8}$).

```{r eval=FALSE}
#hyperparameter grid search criteria

search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 30*60
)

# build grid search
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  search_criteria = search_criteria
)

# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2", 
  sort_by = "mse", 
  decreasing = FALSE
  )
print(grid_perf2)
## H2O Grid Details
## ================
## 
## Grid ID: rf_grid2 
## Used hyper parameters: 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  nbins 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 190 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   max_depth min_rows mtries nbins ntrees sample_rate          model_ids
## 1        30      1.0     25    30    200         0.8 rf_grid2_model_114
## 2        30      1.0     30    30    400         0.8  rf_grid2_model_60
## 3        25      1.0     20    25    200         0.8  rf_grid2_model_62
## 4        20      1.0     20    15    400         0.8  rf_grid2_model_48
## 5        20      1.0     15    15    350        0.75 rf_grid2_model_149
##                   mse
## 1  6.09386451519276E8
## 2 6.141013192008269E8
## 3 6.143676001174936E8
## 4 6.181798579219993E8
## 5 6.182797259475644E8
## 
## ---
##     max_depth min_rows mtries nbins ntrees sample_rate          model_ids
## 185        30      5.0     30    15    500        0.55 rf_grid2_model_126
## 186        35      5.0     15    10    300        0.55  rf_grid2_model_84
## 187        25      5.0     15    20    200        0.55  rf_grid2_model_20
## 188        35      5.0     15    10    200        0.55 rf_grid2_model_184
## 189        40      1.0     15    20    400        0.55 rf_grid2_model_127
## 190        30      1.0     25    20    500       0.632 rf_grid2_model_189
##                      mse
## 185  7.474602079646143E8
## 186  7.530174943920757E8
## 187  7.591548840980767E8
## 188  7.721963479865576E8
## 189 1.0642428537243171E9
## 190 1.4912496290688899E9
```

Once we've identifed the best model we can get that model and apply it to our hold-out test set to compute our final test error. We see that we?fve been able to reduce our RMSE to near $23,000, which is a $10K reduction compared to elastic nets and bagging.

```{r eval=FALSE}
#grab the model id for the top model, chosen by validation error

best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now let?fs evaluate the model performance on a test set
ames_test.h2o <- as.h2o(ames_test)
best_model_perf <- h2o.performance(model = best_model, newdata = ames_test.h2o)

# RMSE of best model
h2o.mse(best_model_perf) %>% sqrt()
## [1] 23104.67
```

#### Predicting {#RF_Pred}

Once we?fve identified our preferred model we can use the traditional `predict` function to make predictions on a new data set. We can use this for all our model types (`randomForest`, `ranger`, and `h2o`); although the outputs differ slightly. Also, not that the new data for the `h2o` model needs to be an h2o object.

```{r}
# randomForest
pred_randomForest <- predict(ames_randomForest, ames_test)
head(pred_randomForest)
##        1        2        3        4        5        6 
## 128266.7 153888.0 264044.2 379186.5 212915.1 210611.4

# ranger
pred_ranger <- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)
## [1] 128440.6 154160.1 266428.5 389959.6 225927.0 214493.1

# h2o - commented
# pred_h2o <- predict(best_model, ames_test.h2o)
# head(pred_h2o)
##    predict
## 1 126903.1
## 2 154215.9
## 3 265242.9
## 4 381486.6
## 5 211334.3
## 6 202046.5
```

#### Learning more {#RF_LM}

Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. To learn more I would start with the following resources listed in order of complexity:

- An Introduction to Statistical Learning
- Applied Predictive Modeling
- Computer Age Statistical Inference
- The Elements of Statistical Learning
