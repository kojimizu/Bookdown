---
title: "UC business Analytics R programming Guide"
author: "Koji Mizumura"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```


This is a practice of [UC business analytics R programming guide](http://uc-r.github.io/).

# Predictive Analysis
## Gradient boosting machines (GBM)

Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the learning methods for winning Kaggle competitions. Whereas [random forest](http://uc-r.github.io/random_forests) build anc ensemble of deepe independent trees, GBMs build an ensemple of shallow and weak successive trees with each tree leading and improving on the previous. 

When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms. This tutorial will cover the fundamentals of GBMs for regression problems.

### tl;dr

This tutotrial serves as an introduction to the GBMs. This tutorial will cover the following material:

- [Replication Requirements](#GBM_RR): What you’ll need to reproduce the analysis in this tutorial.
- [Advantages & Disadvantages](#GBM_feature): Primary strengths and weaknesses of GBMs.
- [The idea](#GBM_idea): A quick overview of how GBMs work.
- [gbm](#GBM_gbm_pkg): Training and tuning with the gbm package
- [xgboost](#GBM_xgboost_pkg): Training and tuning with the xgboost package
- [h2o](#GBM_h2o_pkg): Training and tuning with the h2o package
- [Learning more](#GBM_Learn): Where you can learn more.

### Replication requirements {#GBM_RR}

This tutorial leverages the following packages. Some of these packages play a supporting role; however, we demonstrate how to implement GBMs with several different packages and discuss the pros and cons to each.

```{r}
library(rsample)      # data splitting 
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
```

To illustrate various GBM concepts, we will use the Ames housing data that has been included in the `AmesHousing` package.

```{r}
# create training(70%) and test (30%) sets 
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

__Important notice__: tree-based methods tend to perform well on unprocessed data (i.e., without normalizing, centering, scaling features). In this tutorial, I focus on how to implement GBMs with various packages. Although I do not pre-process the data, realize that you __can__ improve model performance by spending time processing variable attributes.

### Advantages & Disadvantages {#GBM_feature}

__Advantages__ 
- Often provides predictive accuracy that cannot be beat.
- Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.
- No data pre-processing required - often works great with categorical and numerical values as is.
- Handles missing data - imputation not required.

__Disadvantages__
- GBMs will continue improving to minimize all errors. This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.
- Computationally expensive - GBMs often require many trees (>1000) which can be time and memory exhaustive.
- The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.
- Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).

### The idea {#GBM_idea}

Several supervised machine learning models are founded on a single predictive model (i.e., liner regression, penalized models, naive Bayes, support vector machines). Alternatively, other approaches such as bagging and random forests are built on the idea of building an ensemble of models where each individal model predicts the outcome and then the ensemble simply averages the predictive values. The family of boosting methods is based on a different, constructive strategy of ensemble formation.

The main idea of boosting is to add new models to the ensemble __sequentially__. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.

```{r}
knitr::include_graphics("C:/Protected/Data Science/UC Business Analytics/image/boosted-trees-process.png")
```

Let’s discuss each component of the previous sentence in closer detail because they are important.

__Base-learning models__: Boosting is a framework that iteratively improves any weak learning model. Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner. Consequently, this tutorial will discuss boosting in the context of regression trees.

__Training weak models__:A weak model is one whose error rate is only slightly better than random guessing. The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors. With regards to decision trees, shallow trees represent a weak learner. Commonly, trees with only 1-6 splits are used. Combining many weak models (versus strong ones) has a few benefits:

- Speed: Constructing weak models is computationally cheap.  
- Accuracy improvement: Weak models allow the algorithm to learn slowly; making minor adjustments in new areas where it does not perform well. In general, statistical approaches that learn slowly tend to perform well.  
- Avoids overfitting:Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).

__Sequential training with respect to errors__: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. The basic algorithm for boosted regression trees can be generalized to the following where x represents our features and y represents our response:

1. Fit a decision tree to the data: $F_1(x) = y$
2. We then fit the next decision tree to the residuals of the previous: $h_1(x) = y-F_1(x)$
3. Add this new tree to our algorithm: $F_2(x)=F_1(x) + h_1(x)$,
4. Fir the next decision tree to the residuals of $F_2$: $h_2(x)=y-F_2(x)$
5. Add this new tree to our algorithm: $F_3(x) = F_2(x) + h_1(x)$
6. Continue this process until some mmechasim (i.e., cross-validation) tells us to stop.

The basic algorithm for boosted regression trees can be generalized to the following where the final model is simply a stagewise additive model of $b$ individual regression trees:

$$
f(x) = \Sigma_{b=1}^B{f^b(x)}
$$

To illustrate the behaivor, assume the following $x$ and $y$ observations.  The blue sine wave represents the true underlying function and the points represent observations that include some irriducible error (noise). The boosted prediction illustrates the adjusted predictions after each additional sequential tree is added to the algorithm. Initially, there are large errors which the boosted algorithm improves upon immediately but as the predictions get closer to the true underlying function you see each additional tree make small improvements in different areas across the feature space where errors remain. Towards the end of the gif, the predicted values nearly converge to the true underlying function.

__Gradient descent__

Many algorithms, including decision trees, focus on minimizing the residuals and, therefore, emphasize the MSE loss function. The algorithm discussed in the previous section outlines the approach of sequentially fitting regression trees to minimize the errors. This specific approach is how gradient boosting minimizes the mean squared error (MSE) loss function. 

However, often we wish to focus on other loss functions such as mean absolute error (MAE) or to be able to apply the method to a classification problem with a loss function such as deviance. The name __gradient__ boosting machines come from the fact that this procedure can be generalized to loss functions other than MSE.

Gradient boosting is considered a __gradient descent__ algorithm. Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of a gradient descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are a downhill skier racing your friend. 

A good strategy to beat your friend to the bottom is to take the path with the steepest sloep. This is is exactly what gradient descent does - it measures the local gradient of the loss (cost) function for a given set of parameter ($\phi$) and takes steps in the direction of the descending gradient. Once the gradient is zero, we have reached the minimum.

Gradient descent can be performed on any loss function that is differentiable. Consequently, this allows GBMs to optimize different loss functions as desired (see ESL, p. 360 for common loss functions). An important parameter in gradient descent is the size of the steps which is determined by the _learning rate_. If the learning rate is too small, then the algorithm will take many iterations to find the minimum. On the other hand, if the learning rate is too high, you might jump cross the minimum and end up further away than when you started.

Moreover, not all cost functions are convex (bowl shaped). There may be local minimas, plateaus, and other irregular terrain of the loss function that makes finding the global minimum difficult. _Stochastic gradient descent_ can help us address this problem by sampling a fraction of the training observations (typically without replacement) and growing the next tree using that subsample. 

This makes the algorithms faster but the stochastic nature of random sampling also adds some random nature in descending the loss function gradient. Although this randomness does not allow the algorithm to find the absolute global minimum, it can actually helpt the algorithm jump out of local minima and off plateus get near the global minimum.

As we’ll see in the next section, there are several hyperparameter tuning options that allow us to address how we approach the gradient descent of our loss function.

#### Tuning

Part of the beauty and challenges of GBM is that they offer several tuning parameters. The beauty in this is GBMs are highly flexible. The challenge is that they can be time consuming to tune and find the optimal combination of hyperparamters. The most common hyperparameters that you will find in most GBM implementations include:

- __Number of trees__: The total number of trees to fit. GBMs often require many trees; however, unlike random forests GBMs can overfit so the goal is to find the optimal number of trees that minimize the loss function of interest with cross validation. 

- __Depth of trees__; The number $d$ of splits in each tree, which controls the complexity of the boosted ensembles. Often $d=1$ works well, in which case each tree is a stump consisting of a single split. More commonly, $d$ is greater than 1 but is unlikely $d>10$ will be required.

- __Learning rate__: Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called _shrinkage_.

- __Subsampling__: Controls whether or not you use a fraction of the available training observations. Using less than 100% of the training observations means you are implementing stochastic gradient descent. This can help to minimize overfitting and keep from getting stuck in a local minimum or plateau of the loss function gradien

Throughout this tutorial you’ll be exposed to additional hyperparameters that are specific to certain packages and can improve performance and/or the efficiency of training and tuning models.

#### Package implementation

There are many packages that implement GBMs and GBM variants. You can find a fairly comprehensive list [here](https://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html#gbm-software-in-r) and at the [CRAN Machine Learning Task View](https://cran.r-project.org/web/views/MachineLearning.html). However, the most popular implementations which we will cover in this plot include:

- [gbm](https://cran.r-project.org/web/packages/gbm/index.html): The original R implementation of GBMs
- [xgboost](https://cran.r-project.org/web/packages/xgboost/index.html): A fast and efficient gradient boosting framework (C++ backend)
- [h2o](https://cran.r-project.org/web/packages/gamboostLSS/index.html): A powerful java-based interface that provides parallel distributed algorithms and efficient productionalization.


#### gbm {#GBM_gbm_pkg}

The `gbm` R package is an implementation of extensions to Freund and Schapire’s [AdaBoost algorithm](http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf) and Friedman’s [gradient boosting machine.](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf) This is the original R implementation of GBM. A presentation is available [here](https://www.slideshare.net/mark_landry/gbm-package-in-r) by Mark Landry.

Features include:

- Stochastic GBM.
- Supports up to 1024 factor levels.
- Supports Classification and regression trees.
- Can incorporate many loss functions.
- Out-of-bag estimator for the optimal number of iterations is provided.
- Easy to overfit since early stopping -functionality is not automated in this package.
- If internal cross-validation is used, this can be parallelized to all cores on the machine.
- Currently undergoing a major refactoring & rewrite (and has been for some time).
- GPL-2/3 License.

#### Basic implementation

`gbm` has two primary training functions - `gbm::gbm` and `gbm::gbm.fit`. The primary difference is that `gbm::gbm` uses the formula interface to specify your model whereas `gbm::gbm.fit` requires the separated x and y matrices. When working with many variables it is more efficient to use the matrix rather than formula interface.

The default settings in `gbm` includes a learning rate (`shrinkage`) of 0.001. This is a very small learning rate and typically requires a large number of trees to find the minimum MSE. However, `gbm` uses a default number of trees of 100, which is rarely sufficient. Consequently, I crank it up to 10,000 trees. The default depth of each tree (`interaction.depth`) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include `cv.folds` to perform a 5 fold cross validation. The model took about 90 seconds to run and the results show that our MSE loss function is minimized with 10,000 trees.

```{r}
# for reproducibility
set.seed(123)

# train GBM model
gbm.fit <- gbm(
  formula = Sale_Price ~.,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 10000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
)

# print results
print(gbm.fit)
# gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train, 
##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, 
##     cv.folds = 5, verbose = FALSE, n.cores = NULL)
## A gradient boosted model with gaussian loss function.
## 10000 iterations were performed.
## The best cross-validation iteration was 10000.
## There were 80 predictors of which 45 had non-zero influence.
```

The output object is a list containing several modeling and results information. We can access this information with regular indexing; I recommend you take some time to dig around in the object to get comfortable with its components. Here, we see that the minimum CV RMSE is 29133 (this means on average our model is about $29,133 off from the actual sales price) but the plot also illustrates that the CV error is still decreasing at 10,000 trees.

```{r}
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
## [1] 29133.33

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
```

In this case, the small learning rate is resulting in very small incremental improvements which means many trees are required. In fact, for the default learning rate and tree depth settings it takes 39,906 trees for the CV error to minimize (~ 5 minutes of run time)!

#### Tuning

However, rarely do the degfault settings suffice. We could tune parameters one at a time to see how the results change. for example, here, I increase the learning rate to take large steps down the gradient descent, reduce the number of trees (since we are reducing the learning rate), and increase the depth of each tree from using a single split to 3 splits. This model takes about 90 seconds to run and achieves a significantly lower RMSE than our initial model with only 1,260 trees.

```{r}
# for reproducibility
set.seed(123)

# train GBM model
gbm.fit2 <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 5000,
  interaction.depth = 3,
  shrinkage = 0.1,
  cv.folds = 5,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  

# find index for n trees with minimum CV error
min_MSE <- which.min(gbm.fit2$cv.error)

# get MSE and compute RMSE
sqrt(gbm.fit2$cv.error[min_MSE])
## [1] 23112.1

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit2, method = "cv")
```

However, a better option than manually tweaking hyperparameters one at a time is to perform a grid search which iterates over every combination of hyperparameter values and allows us to assess which combination tends to perform well. To perform a manual grid search, first we want to construct our grid of hyperparameter combinations. We’re going to search across 81 models with varying learning rates and tree depth. I also vary the minimum number of observations allowed in the trees terminal nodes (`n.minobsinnode`) and introduce stochastic gradient descent by allowing `bag.fraction < 1`.

```{r}
# create hyperparameter grid

hyper_grid <- expand.grid(
  shrinkage = c(0.01, .1, .3),
  interaction.depth = c(1,3,5),
  n.minosinnode = c(5,10,15),
  bag.fraction = c(.65, .8,1),
  optimal_trees = 0,        # a place to dump results
  min_RMSE = 0              # a place to dump results
)

# total number of combinations
nrow(hyper_grid)

```

We loop through each hyperparameter combination and apply 5,000 trees. However, to speed up the tuning process, instead of performing 5-fold CV I train on 75% of the training observations and evaluate performance on the remaning 25%. __Important note__: when using `train.fraction` it will take the first Xx % of the data so its important to randomize your rows in case this is any logic behind the ordering of the data (i.e., ordered by neighborhood)

After about 30 minutes of training time our grid search ends and we see a few important results pop out. First, our top model has better performance than our previously fitted model above, with the RMSE nearly $3,000 lower. Second, looking at the top 10 models we see that:

- none of the top models used a learning rate of $0.3$; small incremental steps down the gradient descent appears to work best,
- none of the top models used stumps (`interaction.depth = 1`); there are likely stome important interactions that the deeper trees are able to capture,
- adding a stochastic component with bag.fraction < 1 seems to help; there may be some local minimas in our loss function gradient,
- none of the top models used `n.minobsinnode = 15`; the smaller nodes may allow us to capture pockets of unique feature-price point instances,
- in a few instances we appear to use nearly all 5,000 trees; maybe we should increase this parameter in our next search?

```{r}
# randomize data
random_index <- sample(1:nrow(ames_train), nrow(ames_train))
random_ames_train <- ames_train[random_index,]

# grid search
for (i in 1:nrow(hyper_grid)){
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = Sale_Price ~.,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 5000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minosinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default,
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)

```


These results help us to zoom into areas where we can refine our search. Let’s adjust our grid and zoom into closer regions of the values that appear to produce the best results in our previous grid search. This grid contains 81 combinations that we’ll search across.

```{r}
# modify hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1),
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 7, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# total number of combinations
nrow(hyper_grid)
## [1] 81
```

We can use the same `for` loop as before and perform our grid search. We get pretty similar results as before and, actually, our best model is the same as the best model above with an RMSE just above $20K.

```{r}
# grid search

for (i in 1:nrow(hyper_grid)){
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = Sale_Price ~.,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 6000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = 0.75,
    n.cores = NULL, # will use all cores by default,
    verbose = FALSE
  )
  
  # add min trainining error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
  }

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>% 
  head(10)
##    n.trees shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees min_RMSE
## 1     6000      0.10                 5              5         0.65           483 20407.76
## 2     6000      0.01                 5              7         0.65          4999 20598.62
## 3     6000      0.01                 5              5         0.65          4644 20608.75
## 4     6000      0.05                 5              7         0.80          1420 20614.77
## 5     6000      0.01                 7              7         0.65          4977 20762.26
## 6     6000      0.10                 3             10         0.80          1076 20822.23
## 7     6000      0.01                 7             10         0.80          4995 20830.03
## 8     6000      0.01                 7              5         0.80          4636 20830.18
## 9     6000      0.10                 3              7         0.80           949 20839.92
## 10    6000      0.01                 5             10         0.65          4980 20840.43
```

Once we have found our top model we train a model with those specific parameters. And since the model coveraged at 483 trees I train a cross validated model (to provide a more robust errro estimate) with 1000 trees. The cross-validated error of ~$22K is a better representation of the error we might expect on a new unseen data set.

```{r}
# for reproducibility
set.seed(123)

# train GBM model
gbm.fit.final <- gbm(
  formula = Sale_Price ~ .,
  distribution = "gaussian",
  data = ames_train,
  n.trees = 483,
  interaction.depth = 5,
  shrinkage = 0.1,
  n.minobsinnode = 5,
  bag.fraction = .65, 
  train.fraction = 1,
  n.cores = NULL, # will use all cores by default
  verbose = FALSE
  )  
```


#### Visualizing

_variable importance_

After re-running our final model we likely want to understand the variables that have the largest influence on sale price. The `summary` method for `gbm` will output a data frame and a plot that shows the most influential variables. `cBars` allows you to adjust the number of variables to show (in order of influence). The default method for computing variable importance is with relative influence

1. `method=relative.influence`: At each split in each tree, `gbm` computes the improvement in the split-criterion (MSE for regression). `gbm` then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important.

2. `method = permutation.test.gbm`: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important.

```{r}
par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit.final, 
  cBars = 10,
  method = relative.influence, # also can use permutation.test.gbm
  las = 2
  )
```


An alternative approach is to use the underdevelopment `vip` package, which provides `ggplot2` plots. `vip` also provides an additional measure of variable importance based on partial dependence measures and is a common variable importance plotting framework for many machine learning models.

```{r}
# devtools::install_github("koalaverse/vip")
vip::vip(gbm.fit.final)
```

_Partial dependence plots_

After the most relevant variables have been identified the next step is to attemp to understand how the response variable changes based on these variables. For this, we can use __partial dependence plots (PDPs)__ and __individual conditional expectation (ICE) curves__.

PDPs plot the change in the average predicted values as specified feature(s) vary over their marginal distribution. for example, consider the `Gr_Liv_Area` variable. The PDP plot below displays the average change in predicted sales price as we vary Gr_Liv_Area while holding all other variables constant. This is done by holding all variables constant for each observation in our training data set but then apply the unique values of Gr_Liv_Area for each observation. We then average the sale price across all the observations. This PDP illustrates how the predicted sales price increases as the square footage of the ground floor in a house increases.

```{r}
gbm.fit.final %>% 
  partial(pred.var = "Gr_Liv_Area", 
          n.trees  = gbm.fit.final$n.trees,
          grid.resolution = 100) %>% 
  autoplot(rug = TRUE, train=ames_train)+
  scale_y_continuous(labels=scales::dollar)
```

ICE curves are an extension of PDP plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. Below shows the regular ICE curve plot (left) and the centered ICE curves (right). When the curves have a wide range of intercepts and are consequently “stacked” on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results. The resuts show that most observations follow a common trend as `Gr_Liv_Area` increases; however, the centered ICE plot highlights a few observations that deviate from the common trend.

```{r}

ice1 <- gbm.fit.final %>% 
  partial(
    pred.var = "Gr_Liv_Area",
    n.trees  =  gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  ggtitle("Non-centered") +
  scale_y_continuous(labels = scales::dollar)

ice2 <- gbm.fit.final %>%
  partial(
    pred.var = "Gr_Liv_Area", 
    n.trees = gbm.fit.final$n.trees, 
    grid.resolution = 100,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  ggtitle("Centered") +
  scale_y_continuous(labels = scales::dollar)

gridExtra::grid.arrange(ice1, ice2, nrow=1)
```

_LIME_

LIME is a newer procedure for understanding why a prediction resulted in a given value for a single observation. You can read more about LIME [here](http://uc-r.github.io/lime). To use the `lime` package on a `gbm` model we need to define model type and prediction methods.

```{r}
model_type.gbm <- function(x, ...) {
  return("regression")
}

predict_model.gbm <- function(x, newdata, ...) {
  pred <- predict(x, newdata, n.trees = x$n.trees)
  return(as.data.frame(pred))
}
```

We can now apply to our two observations. The results show the predicted value (Case 1:118K, Case 2:161K),local model fit (both are relatively poor), and the most influential variables driving the predicted value for each observation.

```{r}
# get a few observations to perform local interpreation on 
local_obs <- ames_test[1:2,]

# apply LIME
explainer <- lime(ames_train, gbm.fit.final)
explanation <- explain(local_obs, explainer, n_features = 5)
plot_features(explanation)
```

#### Predicting

Once you have decided on a final model you will likely want to use the model to predict on new observations. Like most models, we simply use the `predict` function; however, we also need to supply the number of trees to use (see`?predict.gbm` for details). we see that our RMSE for our test set is very close to the RMSE we obtained to our best `gbm` model.

```{r}
# predict values for test set
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, ames_test)

# results
caret::RMSE(pred, ames_test$Sale_Price)
## [1] 20681.88
```


### xgboost {#GBM_xgboost_pkg}

The `xgboost` R package proviedes an R API to "Extreme Gradient Boosting", which is an efficient implementation of gradient boosting framework (approxmately faster than `gbm`). The [xgboost/demo](https://github.com/dmlc/xgboost/tree/master/demo) repository provides a wealth of information. You can also find a fairly comprehensive parameter tuning guide [here](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/). The xgboost package has been quite popular and successful on Kaggle for data mining competitions.

Features include:

- Provide built-in k-fold cross-validation
- Stochastic GBM with column and row sampling (per split and per tree) for better generalization.
- Includes efficient linear model solver and tree learning algorithms.
- Parallel computation on a single machine.
- Supports various objective functions, including regression, classification and ranking.
- The package is made to be extensible, so that users are also allowed to define their own objectives easily.
- Apache 2.0 License.

#### Basic implementation

XGBoost only works with matrices that contain all numeric variables; consequentl, we need to one hot encode our data. There are different ways to do this in R (i.e., `Matrix::sparse.model.matrix`, `caret::dummyVars`). but here we will use the `vtreat` package. vtreat is a robust package for data prep and helps to eliminate problems caused by missing values, novel categorical levels that appear in future data sets that were not in the training data, etc. However, `vtreat` is not very intuitive. I will not explain the functionalities but you can find more information :

- https://arxiv.org/abs/1611.09477
- https://www.r-bloggers.com/a-demonstration-of-vtreat-data-preparation/
- https://github.com/WinVector/vtreat

The following applies `vtreat` to one-hot encode the training and testing data sets.

```{r}
# variable names
features <- setdiff(names(ames_train), "Sale_Price")

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(ames_train, features, verbose = FALSE)

# Get the "clean" variable naems from the scoreFrame
new_vars <- treatplan %>% 
  magrittr::use_series(scoreFrame) %>% 
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)

# Prepare the training data
features_train <- vtreat::prepare(treatplan, ames_train, varRestriction = new_vars) %>% as.matrix()
response_train <- ames_train$Sale_Price

# Prepare the test data
features_test <- vtreat::prepare(treatplan, ames_test, varRestriction = new_vars) %>% as.matrix()
response_test <- ames_test$Sale_Price

# dimensions of one-hot encoded data
dim(features_train)
## [1] 2051  208
dim(features_test)
## [1] 879 208

```

`xgboost` provides different training functions (i.e. `xgb.train` which is just a wrapper for `xgboost`). However, to train an XGBoost we typically want to use `xgb.cv`, which incorporates cross-validation. The following trains a basic 5-fold cross validated XGBoost model with 1,000 trees. There are many parameters available in `xgb.cv` but the ones you have become more familiar with in this tutorial include the following default values:

- learning date(`eta`): 0.3
- tree depth(`max_depth`):6
- minimum node size(`min_child_weight`):1
- Percent of training data to sample for each tree (`subsample` –> equivalent to `gbm`’s bag.fraction): 100%

```{r}
# reproducibility
set.seed(123)

xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0               # silent,
)
```

The xgb.fit1 object contains lots of good information. In particular we can assess the `xgb.fit1$evaluation_log` to identify the minimum RMSE and the optimal number of trees for both the training data and the cross-validated error. We can see that the training error continues to decrease to 965 trees where the RMSE nearly reaches zero; however, the cross validated error reaches a minimum RMSE of $27,572 with only 60 trees.

```{r}
# get number of trees that minimize error
xgb.fit1$evaluation_log %>% 
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean ==min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test    = min(test_rmse_mean)
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          965  0.5022836          60  27572.31

# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")

```

#### Tuning

To tune the XGBoost model we pass parameters as a list object to the `params` argument.
The common parameters include:

- `eta`: controls the learning rate
- `max_depth`: tree depth
- `min_child_weight`: minimum number of observations required in each terminal node
- `subsample`: percent of training data to sample for each tree
- `colsample_bytrees`: percent of columns to sample from for each tree

For example, if we wanted to specify specific values for these parameters we would extend the above model with the following parameters.

```{r}
# create parameter list
  params <- list(
    eta = .1,
    max_depth = 5,
    min_child_weight = 2,
    subsample = .8,
    colsample_bytree = .9
  )


# reproducibility
set.seed(123)
# train model
xgb.fit3 <- xgb.cv(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear", # for regression models
  verbose = 0, # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# assess results
xgb.fit3$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          180   5891.703         170  24650.17
```

To perform a large search grip, we can follow the same procedure we did with `gbm`. We create our hyperparameter search grid along with columns to dump our results in. Here, I create a pretty large search grid consisting of 576 different hyperparmeter combinations to model.

```{r}
# create hyperparameter grid
hyper_grid <- expand.grid(
  eta = c(.01, .05, .1, .3),
  max_depth = c(1, 3, 5, 7),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.65, .8, 1),
  colsample_bytree = c(.8, .9, 1),
  optional_trees = 0, # a place to dump results
  min_RMSE = 0 # a place to dump results
)
nrow(hyper_grid)
# [1] 576
```

Now I apply the same for loop procedure to loop through and apply a XGBoost model for each hyperparameter combination and dump the results in the `hyper_grid` data frame combination and dump the results in the `hyper_grid` data frame. __Important note__: if you plan to run this code, be prepared to run it before going out to eat or going to bed as it is the full seach grid taking 6 hours to run.

```{r eval=FALSE}
# grid search
for (i in 1:nrow(hyper_grid)){
  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  
  # reproducibility
  set.seed(123)
  
  # train omdel
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train, 
    label = response_train,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:linear", # for regression model
    verbose = 0, # silent
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
}
hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>% 
  head(10)
##     eta max_depth min_child_weight subsample colsample_bytree optimal_trees min_RMSE
## 1  0.01         5                5      0.65                1          1576 23548.84
## 2  0.01         5                3      0.80                1          1626 23587.16
## 3  0.01         5                3      0.65                1          1451 23602.96
## 4  0.01         5                1      0.65                1          1480 23608.65
## 5  0.05         5                3      0.65                1           305 23743.54
## 6  0.01         5                1      0.80                1          1851 23772.90
## 7  0.05         3                3      0.65                1           552 23783.55
## 8  0.01         7                5      0.65                1          1248 23792.65
## 9  0.01         3                3      0.80                1          1923 23794.78
## 10 0.01         7                1      0.65                1          1070 23800.80
```

After assessing the results you would like to perform a few more grid searches to hone in on the parameters that appear to influence the model the most. In fact, [here is a link](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) to a greate blo post that discusses a strategic approach to tuning with `xgboost`. However, for brevity, we will just assume the top model in the above search is the global optimal model. Once you have found the optimal model, we can fit our final model with `xgb.train`.

```{r}
#  parameter list
params <- list(
  eta = 0.01,
  max_depth = 5,
  min_child_weight = 5,
  subsample = 0.65,
  colsample_bytree = 1
)
# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1576,
  objective = "reg:linear",
  verbose = 0
)
```

#### Visualizing
__variable importance__

`xgboost` provides built-in variable importance plotting. First, you need to create the importance matrix with `xgb.importance` and then feed this matrix into `xgb.plot.importance`. There are 3 variable importance measures:

- Gain: the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. This is synonymous with `gbm`’s `relative.influence`.
- Cover:  the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose feature1 is used to decide the leaf node for 10, 5, and 2 observations in tree1, tree2 and tree3 respectively; then the metric will count cover for this feature as 10+5+2 = 17 observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features’ cover metrics.
- Frequency: the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if feature1 occurred in 2 splits, 1 split and 3 splits in each of tree1, tree2 and tree3; then the weightage for feature1 will be 2+1+3 = 6. The frequency for feature1 is calculated as its percentage weight over weights of all features.

```{r}
# create importance matrix
importance_matrix <- xgb.importance(
  model = xgb.fit.final)
# variable importance plot
xgb.plot.importance(importance_matrix, top_n=10, measure = "Gain")
```

__Partial dependence plots__

PDP and ICE plots work similarly to how we implemented them with `gbm`. The only difference is you need to incorporate data within the `partial` function.

```{r}
pdp <- xgb.fit.final %>% 
  partial(pred.var = "Gr_Liv_Area", 
          n.trees = 1576, 
          grid.resolution = 100, 
          train = features_train) %>% 
  autoplot(rug = TRUE, train = features_train)+
  scale_y_continuous(labels = scales::dollar)+
  ggtitle("PDP")

ice <- xgb.fit.final %>% 
  partial(pred.var = "Gr_Liv_Area",
          n.trees = 1576, 
          grid.resolution = 100,
          train = features_train,
          ice = TRUE) %>% 
  autoplot(rug = TRUE, 
           train = features_train,
           alpha = .1,
           center = TRUE)+
  scale_y_continuous(labels = scales::dollar)+
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

__LIME__

LIME provides built-in functionality for `xgboost` objects (see `?model_type`). However, just keep in mind that the local observations being analyzed need to ne one-hot encoded in the same manner, as we prepared the raining and test data. Also, when you feed the training data into the `lime::lime` function be sure that you coerce it from a matrix to a data frame.

```{r}
# one-hot encode the local observations to be assessed.
local_obs_onehot <- vtreat::prepare(treatplan, local_obs, 
                                    varRestriction = new_vars)
# install.packages("vtreat")
# apply LIME
explainer <- lime(data.frame(features_train), xgb.fit.final)
explanation <- explain(local_obs_onehot,  explainer, n_features = 5)
plot_features(explanation)
```

__Predicting__

Lastly, we use `predict` to predict on new observations. owever, unlike `gbm` we do not need to provide the number of trees. Our test set RMSE is only about $600 different than that produced by our `gbm` model.

```{r}
# predict values for test data
pred <- predict(xgb.fit.final, features_test)
# results
caret::RMSE(pred, response_test)
```

### h2o {#GBM_h2o_pkg}

The `h2o` package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with fairly comprehensive [online resource](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html) that includes methodology and code documentation along with tutorial.

Features include:

- Distributed and parallelized computation on either a single node or a multi-node cluster.
- Automatic early stopping based on convergence of user-specified metrics to user-specified relative tolerance.
- Stochastic GBM with column and row sampling (per split and per tree) for better generalization.
- Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace).
- Grid search for hyperparameter optimization and model selection.
- Data-distributed, which means the entire dataset does not need to fit into memory on a single node, hence scales to any size training set.
- Uses histogram approximations of continuous variables for speedup.
- Uses dynamic binning - bin limits are reset at each tree level based on the split bins’ min and max values discovered during the last pass.
- Uses squared error to determine optimal splits.
- Distributed implementation details outlined in a blog post by Cliff Click.
- Unlimited factor levels.
- Multiclass trees (one for each class) built in parallel with each other.
- Apache 2.0 Licensed.
- Model export in plain Java code for deployment in production environments.

#### Basic implementation

Lets go ahead and start up h2o:

```{r}
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

`h2o.gbm` allows us to perform a GBM with H20. However, prior to running our initial model, we need to convert our training datato an h2o object. By default, `h2o.gbm` applies a GBM model with the following parameters:

- number of trees (`ntrees`): 50
- learning rate (`learn_rate`): 0.1
- tree depth(`max_depth`): 5
- minimum observations in a terminal node(`min_rows`):10
- no sampling of observations or columns

```{r}
# create feature names
y <- "Sale_Price"
x <- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o <- as.h2o(ames_train)

# training basic GBM model with defaults
h2o.fit1 <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5
)

# assess the model results
h2o.fit1
```

Similar to XGBoost, we can incorporate automated stopping so that we can crank up the number of trees but terminate training once model improvement decreases or stops. There is also an option to terminate training after so much time has passed (see `max_runtime_secs`). In this example, I train a default model with 5,000 trees but stop training after 10 consecutive trees have no improvement on the cross-validated error. In this case, training stops after 3828 trees and has a cross validated RMSE of 24,684.

```{r}
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)

# model stopped after xx trees
h2o.fit2@parameters$ntrees
## [1] 3828

# cross validated RMSE
h2o.rmse(h2o.fit2, xval = TRUE)
## [1] 24684.09
```

#### Tuning

H2o provides many parameters that can be adjusted. It is well worth your time to check out the available documentation at [H2O.ai](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#gbm-tuning-guide). For this tutorial, we will focus on the more common hyperparametes that I typically apply. This includes:

- tree complexity:
  - `ntrees`: number of trees to train
  - `max_depth`: depth of each tree
  - `min_rows`: Fewest observations allowed in a terminal node
  
- Learning rate:
  - `learn_rate` rate to descend the loss function gradient
  - `learn_rate_annealing`: allow you to have a high initial `learn_rate`, then gradually reduce as trees are added (speeds up training)
  
- Adding stochastic nature:
  - `sample rate`: row sample rate per tree
  - `col_sample_rate`: columns sample rate per tree (synonymous with `xgboost`'s `colsample_bytree`).
  
Note that there are parameters that control how categorical and continuous variables are encoded, binned and split. The defaults tend to perform quite well, but I have been able to gain small improvements in certain circmustances by adjusting these. I will not cover them but the are work reviewing.

To perform grid search tuning with H2O we have two options: perform a _full_ or _random discrete grid search_.

__Full grid search__

A __full cartesian grid search__ examines every combination of hyperparameter settings that we specify in a tuning grid. This has been the type of tuning we have been performing with our manual for loops with gbm and xgboost. However, to speed up training with H2O I’ll use a validation set rather than perform k-fold cross validation. The following creates a hyperparameter grid consisting of 486 hyperparameter combinations. We apply h2o.grid to perform a grid search while also incorporating stopping parameters to reduce training time. Total grid search time was about 90 minutes.

A few characteristics pop out when we assess the results - models with trees deeper than one split with a low learning rate, no annealing, and stochastic observation sampling tend to perform best. 

```{r eval=FALSE}
# create training & validation sets
split <- h2o.splitFrame(train.h2o, 
                         ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]

# create hyperparameter grid
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(.99, 1),
  sample_rate = c(.5, .75, 1),
  col_sample_rate = c(.8, .9, 1)
)

# perform grid search
grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid1",
  x = x,
  y = y,
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)

# collect the results and sort by our model performance metric of choice

grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid1",
  sort_by = "mse",
  decreasing = FALSE
)

grid_perf
## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid1 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 486 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             1.0       0.01                  1.0         3     10.0        0.75 gbm_grid1_model_299 3.6209830674536294E8
## 2             0.8       0.01                  1.0         3     10.0        0.75 gbm_grid1_model_297 3.6380633209494674E8
## 3             0.8       0.01                  1.0         3      1.0         0.5  gbm_grid1_model_27 3.6672773986842275E8
## 4             0.8       0.01                  1.0         5      1.0         0.5  gbm_grid1_model_45  3.683498830618852E8
## 5             0.9       0.01                  1.0         3     10.0        0.75 gbm_grid1_model_298  3.686060225554216E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 481             0.9       0.01                 0.99         1     10.0         1.0 gbm_grid1_model_433  2.824716768094968E9
## 482             0.9       0.01                 0.99         1      1.0         1.0 gbm_grid1_model_325  2.824716768094968E9
## 483             0.9       0.01                 0.99         1      5.0         1.0 gbm_grid1_model_379  2.824716768094968E9
## 484             1.0       0.01                 0.99         1      5.0         1.0 gbm_grid1_model_380 2.8252384874380198E9
## 485             1.0       0.01                 0.99         1      1.0         1.0 gbm_grid1_model_326 2.8252384874380198E9
## 486             1.0       0.01                 0.99         1     10.0         1.0 gbm_grid1_model_434 2.8252384874380198E9
```

We can check out more details of the best performing model. The top model achieves a validation RMSE of 19,029.

```{r eval=FALSE}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  362098307
## RMSE:  19028.88
## MAE:  12427.99
## RMSLE:  0.1403692
## Mean Residual Deviance :  362098307
```

#### Random discrete grid search

Because of the combinatiorial explosion, each additional hyperparameter that gets added to our grid search has a hue effect on the time to complete. Consequently, `h2o` provides an additional grid search path called "RandomDiscrete", which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amo
unt of time has been exceeded, or certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model.

The following performs a random discrete using the same hyperparameter grid we used above. However, in this example we add a search criteria (which is preferred when using a random search) that stops the grid search if none of the last 10 models have managed to have a 0.5% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 3600 seconds (60 minutes). In this example, our search went for the entire 60 minutes and evaluated 291 of the 486 potential models.

```{r eval=FALSE}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtimes_secs = 60*60
)

# perform grid search
grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid2",
  x = x,
  y = y,
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  search_criteria = search_criteria, # add this
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)

# collect the result and sort by our model performance metric of choice

grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid2",
  sort_by = "mse",
  decreasing = FALSE
)

grid_perf
## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid2 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 291 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 1             0.8       0.05                  1.0         3     10.0         1.0  gbm_grid2_model_74  5.150720254988258E8
## 2             0.9       0.01                  1.0         3      5.0         0.5 gbm_grid2_model_146 5.1889115659740096E8
## 3             0.9       0.05                  1.0         3      5.0         0.5 gbm_grid2_model_114 5.2062049083883923E8
## 4             0.8       0.05                  1.0         3      5.0        0.75  gbm_grid2_model_37 5.2124226584496534E8
## 5             0.9       0.05                  1.0         3     10.0         1.0 gbm_grid2_model_157  5.212796449846914E8
## 
## ---
##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse
## 286             0.9       0.01                 0.99         1     10.0         1.0 gbm_grid2_model_179  3.323851889022955E9
## 287             1.0       0.01                 0.99         1     10.0         1.0 gbm_grid2_model_260 3.3243159009633546E9
## 288             0.9       0.01                 0.99         1      5.0         0.5 gbm_grid2_model_199 3.3243216930611935E9
## 289             0.8       0.01                 0.99         1     10.0         0.5  gbm_grid2_model_80 3.3244630344508557E9
## 290             0.8       0.01                 0.99         1      1.0         0.5  gbm_grid2_model_71 3.3244630344508557E9
## 291             0.8       0.01                 0.99         1      5.0         0.5 gbm_grid2_model_227 3.3244630344508557E9
```

In this example, the best model obtained a cross-validated RMSE of 22,695. Not quite as good as the full grid search; however, often the results come much closer.

```{r eval=FALSE}
# Grab the model id for the top model, chosen by validation error

best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
## H2ORegressionMetrics: gbm
## ** Reported on validation data. **
## 
## MSE:  515072025
## RMSE:  22695.2
## MAE:  13841.13
## RMSLE:  0.1427291
## Mean Residual Deviance :  515072025
```

Once we’ve found our preferred model, we’ll go ahead and retrain a new model with the full training data. I’ll use the best model from the full grid search and perform a 5-fold CV to get a robust estimate of the expected error.

```{r eval=FALSE}
# train final model

h2o.final <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 5,
  ntrees = 10000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 3,
  min_rows = 10,
  sample_rate = 0.75,
  col_sample_rate = 1,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 123
)

# model stopped after xx trees
h2o.final@parameters$ntrees
## [1] 9385

# cross validated RMSE
h2o.rmse(h2o.final, xval = TRUE)
## [1] 23218.45
```

__Visualizing__
__Variable importance__

`h2o` provides a built function that plots variable importance. It only has one measure of variable importance, relative importance, which measures the average impact each variable has across all the trees on the loss function. The variable with the largest is most importance and the impact of all other variables are provided relative to the most important variable. The `vip` package also works with h2o objects to plot variable importance.

```{r eval=FALSE}
h2o.varimp_plot(h2o.final,
                num_of_features = 10)

# vip package usage - to be checked
vip::vip(h2o.final)
```

__Partial dependence plots(PDP)__

We can also create similar PDP and ICE plots as before. We only need to incorporate a specialty function that converts the supplied data to an `h2o` object and then formats the predicted output as a data frame. We feed this into the `partial` function and the rest is standard.

```{r eval=FALSE}
pfun <- function(object, newdata) {
  as.data.frame(predict(object, newdata = as.h2o(newdata)))[[1L]]
}

pdp <- h2o.final %>%
  partial(
    pred.var = "Gr_Liv_Area", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = ames_train
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- h2o.final %>%
  partial(
    pred.var = "Gr_Liv_Area", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = ames_train,
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

`h2o` does not provide built-in ICE plots but it does provide a PDP plot that plots not only the mean marginal impact (as in a normal PDP) but also one standard error to show the variability.

```{r eval=FALSE}
h2o.partialPlot(h2o.final, data = train.h2o, cols = "Overall_Qual")
```

Unfortunately, `h2o`’s function plots the categorical levels in alphabetical order whereas `pdp` will plot them in their specified level order making inference more intuitive.

```{r eval=FALSE}
pdp <- h2o.final %>%
  partial(
    pred.var = "Overall_Qual", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = as.data.frame(ames_train)
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("PDP")

ice <- h2o.final %>%
  partial(
    pred.var = "Overall_Qual", 
    pred.fun = pfun,
    grid.resolution = 20, 
    train = as.data.frame(ames_train),
    ice = TRUE
    ) %>%
  autoplot(rug = TRUE, train = ames_train, alpha = .1, center = TRUE) +
  scale_y_continuous(labels = scales::dollar) +
  ggtitle("ICE")

gridExtra::grid.arrange(pdp, ice, nrow = 1)
```

__LIME__

LIME also provides built-in functionality for `h2o` objects (see `?model_type`).

```{r eval=FALSE}
# apply LIME
explainer <- lime(ames_train, h2.final)
explanation <- explain(local_obs, explainer, n_features = 5)
plot_features(explanation)
```

#### Predicting

Lastly, we use `h2o.predict` or `predict` to predict on new observations and we can also evaluate the performance of our model on our test set easily with `h2o.performance`. Results are quite similar to both `gbm` and `xgboost`.

```{r eval=FALSE}
# convert test set to h2o object
test.h2o <- as.h2o(ames_test)

# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)
## H2ORegressionMetrics: gbm
## 
## MSE:  407532539
## RMSE:  20187.44
## MAE:  12683.01
## RMSLE:  0.100829
## Mean Residual Deviance :  407532539

# predict with h2o.predict
h2o.predict(h2o.final, newdata = test.h2o)
##    predict
## 1 130114.9
## 2 162136.7
## 3 263438.5
## 4 484853.0
## 5 219152.9
## 6 208616.2
## 
## [879 rows x 1 column]

# predict values with predict
predict(h2o.final, test.h2o)
##    predict
## 1 130114.9
## 2 162136.7
## 3 263438.5
## 4 484853.0
## 5 219152.9
## 6 208616.2
## 
## [879 rows x 1 column]
```


### Learning more {#GBM_Learn}

GBMs are one of the most powerful ensemble algorithms that are often first-in-class with predictive accuracy. Although they are less intuitive and more computationally demanding than many other machine learning algorithms, they are essential to have in your toolbox. To learn more I would start with the following resources:

Traditional book resources:

An Introduction to Statistical Learning
Applied Predictive Modeling
Computer Age Statistical Inference
The Elements of Statistical Learning
Alternative online resources:

- [Trevor Hastie - Gradient Boosting & Random Forests at H2O World 2014](https://koalaverse.github.io/machine-learning-in-R/%20//www.youtube.com/watch?v=wPqtzj5VZus&index=16&list=PLNtMya54qvOFQhSZ4IKKXRbMkyL%20Mn0caa) (YouTube)
- [Trevor Hastie - Data Science of GBM (2013)](http://www.slideshare.net/0xdata/gbm-27891077) (slides)
- [Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015](https://www.youtube.com/watch?v=9wn1f-30_ZY) (YouTube)
- [Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn at PyData London 2014](https://www.youtube.com/watch?v=IXZKgIsZRm0) (YouTube)
- [Alexey Natekin1 and Alois Knoll - Gradient boosting machines, a tutorial (blog post)](http://journal.frontiersin.org/article/10.3389/fnbot.2013.00021/full)


## Linear & Quadratic Discriminant Analysis

In the previous tutorial, you learned that logistic regression is a classification algorithm traditionally limited to only two-class classification problems (i.e., _default_ = _Yes_ or _No_). However, if you have more than two classes then linear (and its cousin Quadratic) Discriminant Analysis (LDA & QDA) is an often-preferred classification technique. Discriminant analysis models the distribution of the predictors X separately in each of the response classes (i.e., default = _"Yes"_, default = _"No"_), and then uses [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes'_theorem) to flip these around into estimates for the probability of the response category given the value of X.

### tl;dr
This tutorial covers an introduction to LDA & QDA and covers:

1. [Replication requirements](#DA_RR): What you’ll need to reproduce the analysis in this tutorial
2. [Why use discriminant analysis](#DA_Why): Understand why and when to use discriminant analysis and the basics behind how it works
3. [Preparing our data](#LDA_Data): Prepare our data for modeling
4. [Linear discriminant analysis](#DA_LDA): Modeling and 5. classifying the categorical response `Y` with a linear combination of predictor variables `X`.
5. [Quadratic discriminant analysis](#DA_QDA): Modeling and classifying the categorical response `Y` with a non-linear combination of predictor variables `X`.
6. [Prediction Performance](#DA_Pred): How well does the model fit the data? Which predictors are most important? Are the predictions accurate?
7. [A comparison](#DA_Comp): An example comparing logistic regression & Discriminant Analysis
8. [Additional resources](#DA_Resource): Additional resources to help you learn more

### Replication requirements {#DA_RR}

This tutorial primarily leverages the `Default` data provided by the `ISLR` package. This is a simulated data set containing information on ten thousand customers such as whether the customer defaulted, is a student, the average balance carried by the customer and the income of the customer. We will also use a few packages that provide data manipulation, visualization, pipeline modeling functions, and model outout tidying functions.

```{r}
library(tidyverse)  # data manipulation and visualization
library(MASS)       # provides LDA & QDA model functions

# data set from ISLR package
library(ISLR)

# Load data 
(default <- as_tibble(ISLR::Default))
## # A tibble: 10,000 × 4
##    default student   balance    income
##     <fctr>  <fctr>     <dbl>     <dbl>
## 1       No      No  729.5265 44361.625
## 2       No     Yes  817.1804 12106.135
## 3       No      No 1073.5492 31767.139
## 4       No      No  529.2506 35704.494
## 5       No      No  785.6559 38463.496
## 6       No     Yes  919.5885  7491.559
## 7       No      No  825.5133 24905.227
## 8       No     Yes  808.6675 17600.451
## 9       No      No 1161.0579 37468.529
## 10      No      No    0.0000 29275.268
## # ... with 9,990 more rows
```

### Why use discriminant analysis {#DA_Why}

In the previous tutorial, we saw that a logistic regression model does a fairly good job classifying customers that default. So why do we need another classification method beyond logistic regression? There are several reasons:

- When the classes of the reponse variable Y (i.e. _default = “Yes”_, _default = “No”_) are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. LDA & QDA do not suffer from this problem.

- If _n_ is small and the distribution of the predictors _X_ is approximately normal in each of the classes, the LDA & QDA models are again more stable than the logistic reression model.

- LDA & QDA are often preferred over logistic regression when we have more than two non-ordinal response classes (i.e., _stroke_, _drug overdose_ and _epileptic seizure_).

However, it is important to note that LDA & QDA have assumptions that are often more restictive than logistic regression:

- Both LDA and QDA assume the the predictor variables _X_ are drawn from a multivariate Gaussian (aka _normal_) distribution.
- LDA assumes equality of covariances among the predictor variables X across each all levels of _Y_. This assumption is relaxed with __the QDA model__.
- LDA and QDA require the number of predictor variables (`p`) to be less then the sample size (`n`). Furthermore, its important to keep in mind that performance will severely decline as `p` approaches `n`. A simple rule of thumb is to use LDA & QDA on data sets where $n≥5×p$.

Also, when considering between LDA & QDA its important to know that __LDA__ is a much __less flexible__ classifier than QDA, and so has substantially _lower variance_. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the the predictor variable share a common variance across each Y response class is badly off, then LDA can suffer from _high bias_. Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very _large_, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix is clearly untenable.

### Preparing our data {#DA_Data}

As we’ve done in the previous tutorials, we’ll split our data into a training (60%) and testing (40%) data sets so we can assess how well our model performs on an out-of-sample data set.

```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(default), replace = T, prob = c(0.6,0.4))
train <- default[sample, ]
test <- default[!sample, ]
```


### Linear discriminant analysis {#DA_LDA}

LDA computes "discriminat scores" for each observation to classify what response variable class it is in (i.e., default or not default). These scores are obtained by finding linear combination of the independent variables. For a single predictor variable $X = x$ the LDA classifier is estimated as 

$$
\hat{\delta}_k(x) = x*\frac{\hat{\mu_k}}{\sigma^2} -\frac{\hat{\mu_k^2}}{2\sigma^2}+log(\hat{\pi_k})...(1) 
$$

where:

- $\hat{\delta}_k(x)$  is the estimated discriminant score that the observation will fall in the kth class within the response variable (i.e. default or not default) based on the value of the predictor variable x
- $\hat{\mu}_k$ is the average of all the training observations from the kth class
- $\hat{\sigma}^2$ is the weighted average of the sample variances for each of the K classes
- $\phi_k$ is the prior probability that an observation belongs to the kth class (not to be confused with the mathematical constant $\phi$
≈3.14159.

This clssifier assigns an observation to the kth class of $Y_k$ for which discriminant score $\delta_k(x)$ is largest. For example, lets assume there are two clases  (A and B) for the response variable $Y$. Based on the predictor variable, LDA is going to compute the probability distribution of being classified as class $A$ or $B$. The linear decision boundary between the probability distribution is represented by the dashed line. Discriminat scores to the left of the dashed line will be classified as $A$ and scores to the right will be classified as $B$.

When dealing with more than one predictor variable, the LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N (\mu_k, \Sigma)$, where $mu_k$ is class-specific mean vector, and $\Sigma$ is a covariance matrix that is common to all $K$ classes. Incorporate this into the LDA classifier results in:

$$
\delta_k(x) = x^T\Sigma^{-1}\hat{\mu}_k-\frac{1}{2} \hat{\mu}_k^T\Sigma^{-1} - \mu_k + log(\hat{\pi_k})
$$

,where an observation will be assigned to class $k$ where the discriminant score $\delta_k(x)$ is largest.

#### Estimate & Understand Model

In R, we fit a LDA model using the `lda` function, which is part of the `MASS` library. Notice that the syntax for the `lda` is identical to that of `lm` (as seen in the linear regression tutorial), and to that of `glm` (as seen in the logistic regression tutorial) except for the absence of the family option.

```{r}
(lda.m1 <- lda(default ~ balance + student, data = train))
```

The LDA output indicates that our prior probabilities are $\hat{\pi_1} = 0.968$ and $\hat{\pi_2}=0.032$: in other words, 96.8% of the training observations are customers who did not default and 3% represent those that defaulted. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of $mu_k$. These suggest that customers that tend to default have, on average, a credit card balance of $1,777 and are more likely to be students then non-defaulters (29% of non-defaulters are students whereas 39% of defaulters are). However, as we learned from the last tutorial this is largely because students tend to have higher balances then non-students.

The _coefficients of linear disciminants_ provide he linear combination of balance and student=Yes that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in Eq 1 & 2. If 0.0022 × balance − 0.228 × student is large, then the LDA classifier will predict that the customer will default, and if it is small, then the LDA classifier will predict the customer will not default. We can use plot to produce plots of the linear discriminants, obtained by computing 0.0022 × balance − 0.228 × student for each of the training observations. As you can see, when 
$0.0022×balance-0.228*student<0$  the probability increases that the customer will not default and when $0.0022×balance-0.228*student>0$ the probability increases that the customer will default.

```{r}
plot(lda.m1)
```

#### Make predictions

We can use `predict` for LDA much like we did with logistic regression. I will illustrate the output that `predict` provides based on this simple data set.

```{r}
train

df <- tibble(balance = rep(c(1000, 2000), 2), 
       student = c("No", "No", "Yes", "Yes"))
df <- df %>% 
  mutate_at(vars(student), as.factor)
df
```

Below we see that `predict` returns a list with three elements. The first element, `class` contains LDA's predictions about the customer defaulting. Here we see that the second observation (non-student with balance of $2,000) is the only one kthat is predicted to default. The second element, `posterior` is a matrix that contains the posterior probability that the corresponding observations will or will not default. Here we see that the only observation to have a posterior probability of defaulting greater than 50% is observation 2, which is why the LDA model predicted this observation will default. 

However, we also see that observation 4 has a 42% probability of defaulting. Right now, the model is predicting that this observation will not default because this probability is less than 50%; however, we will see shortly how we can make adjustments to our posterior probability thresholds. Finally, `x` contains the linear discriminant values, described earlier.

```{r}
(df.pred <- predict(lda.m1, df, type="response"))
```

As previously mentioned, the default setting is to use a 50% threshold for the posterior probabilities. We can recreate the predictions contained in the `class` element above:

```{r}
# number of non-defaulters
sum(df.pred$posterior[,1]>.5)

# number of defaulters
sum(df.pred$posterior[,2]>.5)
```

If we wanted to use a posterior probability threshold other than 50% in order to make predictions, then we could easily do so. For instance, suppose that a credit card company is extremely risk-averse and want to assume that a customer with 40% or greater probability is a high-risk customer. We can easily assess the number of high-risk customers.

```{r}
# number of high-risk customers with 40$ probability of defaulting
df.pred$posterior
df.pred$posterior[,2] %>% sum(. > .4)
sum(df.pred$posterior[, 2] > .4)
```

### Quadratic discriminant analysis] {#DA_QDA} 
As previously mentioned, LDA asssumes that the observations within each class are drawn from a multivatiate Gaussian distribution and the covariance of the predictor variables are common across all $k$ levels of the response variable $Y$.

Quadratic discriminant analysis (QDA) provides an alternative approach. Like LDA the QDA classifier assumes that the observations from each class of $Y$ are drawn from a Gaussian distribution.

However, unlike LDA, QDA assumes that each class has its own covariance matrix. In other words, the predictor variables are not assumed to have common variance across each of the $k$ levels in $Y$. Mathemetically, it assumes that an observation from the $k$th class is of the form $X \sim N(\mu_k, \Sigma_k)$, where $\Sigma_k$ is a covariance matrix for the $k$th class. Under this assumption, the classifier assings an observation to the class for which:

$$
\hat{\delta_k(x)} = -\frac{1}{2}x^T\Sigma^{-1}_kx + x^T \Sigma^{-1}_k \hat{\mu_k} - \frac{1}{2} \hat{\mu}_k^T \Sigma^{-1}_k \hat{\mu_k} - \frac{1}{2} log|\Sigma_k|+log(\hat{\pi}_k)
$$

is largest. Why is this important? Consider the image below. In trying to classify the observation into the three (color coded) classes, LDA provides linear decision boundaries that are based on the assumption that the observations vary consistently across all classes.

However, when looking at the data, it becomes apparent that the variability of the observations within each class differ. Consequently, QDA (right plot) is able to capture the differing covariances and provide more accurate non-linear classification decision boundaries.

```{r eval=FALSE}
knitr::include_graphics("C:/Protected/Data Science/UC Business Analytics/image/QDA.png")
```

#### Estimate & Understand model

Similar to `lda`, we can use the `MASS` library to fit a QDa model. Here we use the `qda` function. The output is very similar to the `lda` output. It contains the prior probabilities and the group means. But it does not contain the coefficients of the linear disciminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors.

```{r}
(qda.m1 <- MASS::qda(default ~ balance + student, data = train))
```

#### Make predictions

The `predict` function works in exactly the same fashion as for LDA except it does not return the linear disciminant values. In comparing this simple prediction example to that seen in the LDA section we see minor changes in the posterior probabilities. Most notably, the posterior probability that observation 4 will default increased by nearly 8% points.

```{r}
predict(qda.m1, df, prob = TRUE)

## $class
## [1] No  Yes No  No 
## Levels: No Yes
## 
## $posterior
##          No         Yes
## 1 0.9957697 0.004230299
## 2 0.4381383 0.561861660
## 3 0.9980862 0.001913799
## 4 0.5148050 0.485194962
```


### Prediction Performance {#DA_Pred} 

Now that we understand the basics of evaluating our model and making predictions. Let's assess how well our two models (`lda.m1`, `qda.m1`) perform on our test data set. First, we need to apply our models to the test data.

```{r}
test.predicted.lda <- predict(lda.m1, newdata = test)
test.predicted.qda <- predict(qda.m1, newdata = test)
```

Now, we can evaluate how well our model predicts by assessing the different classification rates discussed in the logistic regression tutorial. First, we will look at the confusion matrix in a percentage form.

The below results show that the models perform in a very similar manner. 96% of the predicted observations are true negatives and about 1% are true positives. Both models have a type II error of less than 3% in which the model predicts the customer will not default but they actually did. And both models have a type I error of less than 1% in which the models predict the customer will default but they never did.

```{r}
lda.cm <- table(test$default, test.predicted.lda$class)
qda.cm <- table(test$default, test.predicted.qda$class)

list(LDA_model = lda.cm %>% prop.table() %>% round(3),
     QDA_model = qda.cm %>% prop.table() %>% round(3))

## $LDA_model
##      
##          No   Yes
##   No  0.964 0.002
##   Yes 0.028 0.007
## 
## $QDA_model
##      
##          No   Yes
##   No  0.963 0.002
##   Yes 0.026 0.009
```

Furtheremore, we can estimate the overall error rates. Here we see that the QDA model reduces the error rate by just a hair.

```{r}
test %>% 
  mutate(lda.pred = (test.predicted.lda$class),
         qda.pred = (test.predicted.qda$class)) %>% 
  summarise(
    lda.error = mean(default != lda.pred),
    qda.error = mean(default != qda.pred)
  )
## # A tibble: 1 × 2
##    lda.error  qda.error
##        <dbl>      <dbl>
## 1 0.02909183 0.02782697
```

However, as we discussed in the last tutorial, the overall error may be less important then understanding the _precision_ of our model. If we look at the raw number of our _confusion matrix_ we can compute the precision:

- LDA model: $29/(109+29)=21$%
- QDA model: $35/(103+35)=25$%

So our QDA model has a slightly higer precision than the LDA model; however, both of them are lower than the logistic regression model precision of 29%.

```{r}
list(LDA_model = lda.cm,
     QDA_model = qda.cm)
```

If we are concerned with increasing the precision of our model we can tune our model by adjusting the posterior probability threshold. For instance, we might label any customer with a posterior probability of default above 20% as high-risk. 

Now the precision of our QDA model improves to $83/(83+55)=60$%. However, the overall error rate has increased to 4%. But a credit card company may consider this slight increase in the total error rate to be a small price to pay for more accurate identification of individuals who do indeed default. It is important to keep in mind these kinds of trade-offs, which are common with classification models - tuning models can improve certain classification rates while worsening others.

```{r}
# create adjusted predictions
lda.pred.adj <- ifelse(test.predicted.lda$posterior[,2]>.2,"Yes","No")
qda.pred.adj <- ifelse(test.predicted.qda$posterior[,2]>.2, "Yes", "No")

# create new confusion matrices
list(LDA_model = table(test$default, lda.pred.adj),
     QDA_model = table(test$default, qda.pred.adj))
## $LDA_model
##      lda.pred.adj
##         No  Yes
##   No  3731   84
##   Yes   69   69
## 
## $QDA_model
##      qda.pred.adj
##         No  Yes
##   No  3699  116
##   Yes   55   83
```

We can also assess the ROC curve for our models as we did in the logistic regression tutorial and compute the AUC.

```{r}
#ROC curve
library(ROCR)

par(mfrow=c(1,2))

prediction(test.predicted.lda$posterior[,2], test$default) %>% 
  performance(measure = "tpr", x.measure = "fpr") %>% 
  plot()

prediction(test.predicted.qda$posterior[,2], test$default) %>% 
  performance(measure = "tpr", x.measure = "fpr") %>% 
  plot()
```

```{r}
# model 1 AUC
prediction(test.predicted.lda$posterior[,2], test$default) %>%
  performance(measure = "auc") %>%
  .@y.values
## [[1]]
## [1] 0.9420727

# model 2 AUC
prediction(test.predicted.qda$posterior[,2], test$default) %>%
  performance(measure = "auc") %>%
  .@y.values
## [[1]]
## [1] 0.9420746
```

### Comparison of logistic regression & discriminant analysis {#DA_Comp}

The logistic regression and LDA methods are closely connected and differ primarily in their fitting procedure. Consequently, the two often produce similar results. However, LDA assumes that the observations are drawn from a _Gaussian distribution__ with a common covariance matrix across each class of $Y$, and so can provide some improvements over logistic regression when this assumption approximately holds.

Conversely, logistic regression can outperform LDA if these Gaussian assumptions are not met. Both LDA and logistic regression produce linear decision boundaries so when the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well. QDA, on the other hand, provides a non-linear quadratic decision boundary. Thus, when the decision boundary is moderately non-linear, QDA may give better results (we will see other non-linear classifiers in later tutorials).

What is important to keep in mind is that non one method will dominate the others in every situation. And, often we want to compare multiple approaches to see how they compare. To illustrate, we will examine stock market (`Smarket`)  data provided by the ISLR package.

This data sset consists of percentage returs for the S&P stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, percentage returns for each of the five previous trading days, _Lag1_ through _Lag5_ are provided. In addition, _Volume_ (the number of shares traded on the previous day, in billions), _Today_ (the percentage return on the date in question), and _Direction_(whether the market was Up or down on this date) are provided.

```{r}
dim(ISLR::Smarket)
head(ISLR::Smarket)
```

Let's model this data with logistic regression, LDA and QDA to assess well each model does in predicting the direction of the stock market based on previous data returns. We will use 2001-2004 data to train our models and then test these models on 2005 data.

```{r}
train <- subset(ISLR::Smarket, Year < 2005)
test <- subset(ISLR::Smarket, Year == 2005)
```

#### Logistic regression

Here we fit a logistic regression model to the training data. Looking at the summary our modle does not look too convincing considering no coefficients are statistically significant and our residual deviance has barely been reduced.

```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
               data = train,
               family = binomial)

glm.fit %>% summary()
glm.fit %>% broom::tidy()
```

Now we comupte the predictions for 2005 and compare them to the actual movements of the market over that time period with a confusion matrix. The results are rather disappointing: the test error rate is 52%, which is worse than random guessing! Furthermore, our precision is only 31%. However, this should not be surprising considering the lack of statistical significance with our predictors.

```{r}
# predictions
glm.probs <- predict(glm.fit, test, type="response")

# confusion matrix
table(test$Direction, ifelse(glm.probs >0.5, "Up", "Down"))

# accracy rate
mean(ifelse(glm.probs > 0.5, "Up", "Down")==test$Direction)

# error rate
mean(ifelse(glm.probs > 0.5, "Up", "Down")!=test$Direction)
```

Remeber that using predictors that have no relationship with the response tends to cause a deterioration in the test error rate(since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. The variables that appear to have the highest importance rating are _Lag1_ and _Lag2_.

```{r}
caret::varImp(glm.fit)
```

Lets re-fit with just these two variables and reassess performance. We don’t see much improvement within our model summary.

```{r}
glm.fit <- glm(Direction ~ Lag1 + Lag2, 
               data = train,
               family = binomial)

summary(glm.fit)

```

However, our prediction classification rates have improved slightly. Our error rate has decreased to 44% (accuracy = 56%) and our precision has increased to 75%. However, its worth noting that the market moved up 56% of the time in 2005 and moved down 44% of the time. Thus, the logistic regression approach is no better than a naive approach!

```{r}
# predictions
glm.probs <- predict(glm.fit, test, type = "response")

# confusion matrix
table(test$Direction, ifelse(glm.probs > 0.5, "Up", "Down"))
##       
##        Down  Up
##   Down   35  76
##   Up     35 106

# accuracy rate
mean(ifelse(glm.probs > 0.5, "Up", "Down") == test$Direction)
## [1] 0.5595238

# error rate
mean(ifelse(glm.probs > 0.5, "Up", "Down") != test$Direction)
## [1] 0.4404762
```

#### Linear disciminant analysis (LDA)

Now we will perform LDA on the stock market data. Our summary shows that our prior probabilities of market movements are  49% (down) and 51% (up). The group means indicate that there is a tendency for the previous 2 days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines.

```{r}
lda.fit <- lda(Direction ~ Lag1 + Lag2, data = train)
lda.fit
```

When we predict with our LDA model and assess the confusion matrix we see that our prediction rates mirror those produced by logistic regression. The overall error and the __precision__ of our LDA and logistic regression models are the same.

```{r}
# predictions
test.predicted.lda <- predict(lda.fit, newdata = test)

# confusion matrix
table(test$Direction, test.predicted.lda$class)

# accuracy rate
mean(test.predicted.lda$class == test$Direction)
## [1] 0.5595238

# error rate
mean(test.predicted.lda$class != test$Direction)
## [1] 0.4404762

# visualize the results
test %>% 
  ggplot(aes(Lag1, Direction))+
  geom_point(aes(col=Direction))+
  geom_smooth(method = "lda", formula = y~x)
```

#### Quadratic disciminant analysis

Lastly, we will predict with a QDA model to see if we can improve our performance.

```{r}
(qda.fit <- qda(Direction ~ Lag1 + Lag2, data = train))
```

Surprisingly, the QDA predictions are acrurate almost 60% of the time! Furthermore, the precision of the model is 86%. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurarately. This uggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by the LDA and logstic regression.

```{r}
# predictions
test.predicted.qda <- predict(qda.fit, newdata = test)

# confusion matrix
table(test$Direction, test.predicted.qda$class)
##       
##        Down  Up
##   Down   30  81
##   Up     20 121

# accuracy rate
mean(test.predicted.qda$class == test$Direction)
## [1] 0.5992063

# error rate
mean(test.predicted.qda$class != test$Direction)
## [1] 0.4007937

```

We can see how our models differ with a ROC curve. Although you can't tell, the logistic regressio and LDA ROC curve sit directly on top of one another. However, we can see how the QDA (green) differs slightly.

```{r}
# ROC curve
library(ROCR)

p1 <- prediction(glm.probs, test$Direction) %>% 
  performance(measure = "tpr", x.measure = "fpr")

p2 <- prediction(test.predicted.lda$posterior[,2], test$Direction) %>% 
  performance(measure = "tpr", x.measure = "fpr")

p3 <- prediction(test.predicted.qda$posterior[,2], test$Direction) %>% 
  performance(measure = "tpr", x.measure = "fpr")

plot(p1, col = "red")
plot(p2, add = TRUE, col = "blue")
plot(p3, add = TRUE, col = "green")
```

The difference is subtle. You can see where we experince increases in the true positive predictions (where the green line go above the red and blue lines). An although our precision increases, overall AUC is not that much higher.

```{r}
# logistic regression AUC
prediction(
  glm.probs, test$Direction) %>% 
  performance(measure = "auc") %>% 
  .@y.values

# LDA AUC
prediction(test.predicted.lda$posterior[,2], test$Direction) %>%
  performance(measure = "auc") %>%
  .@y.values
## [[1]]
## [1] 0.5584308

# QDA AUC
prediction(test.predicted.qda$posterior[,2], test$Direction) %>%
  performance(measure = "auc") %>%
  .@y.values
## [[1]]
## [1] 0.5620088
```

Although we get some improvements with the QDA model we probabily want to continue tuning our models or assess other techniques to improve our classification performance before hedging any bets. But this ullustrates the usefulness of assessing multiple classification models.

### Additional resources {#}

This will get you up and running with LDA and QDA. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:

- An Introduction to Statistical Learning
- Applied Predictive Modeling
- Elements of Statistical Learning


## Support vector machine
The advent of computers brough on rapid advances in the field of statistical classification, one of which is the _Support Vector Machine_, or SVM. The goal of an SVM is to take groups of observations and construct boundaries to predict which group future observations belong to based on their measurements. The different groups that must be separated will be called "classes". SVMs can handle any number of classes, as well as observations of any dimension. SVMs can take almost any shape (including linear, radial, and polynomial, among others), and are generally flexible enough to be used in almost any classification endeavor that the user chooses to undertake.

### tl;dr

1. [Replication Requirements](#SVM_RR): What you’ll need to reproduce the analysis in this tutorial
2. [Maximal Margin Classifier](#SVM_MM_Classifier): Constructing a classification line for completely separable data
3. [Support Vector Classifiers](#SVM_Classifier): Constructing a classification line for data that is not separable
4. [Support Vector Machines](#SVM_Overview): Constructing a classification boundary, whether linear or nonlinear, for data that may or may not be separable
5. [SVMs for Multiple Classes](#SVM_Multi_Class): SVM techniques for more than 2 classes of observations


### Replication Requirements {#SVM_RR}

In this tutorial, we will leverage the `tidyverse` package to perform data manipulation, the `kernlab` and `e1071` packages to perform calculkating and produce visualization related to SVMs, and the `ISLR` package to load a real world data set and demonstrate the functionality of SVM.


```{r}
# set pseudorandom number generator
set.seed(10)

# Attach Packages
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(ISLR)         # contains example data set "Khan"
library(RColorBrewer) # customized coloring of plots

```

The data sets used in the tutorial (with the exception of `Khan`) will be generated using built-in R commands. The Support Vector Machine methodology is sound for any number of dimensions, but becomes difficult to visualize for more than 2. As previously mentioned, SVMs are robust for any number of classes, but we will stick to no more than 3 for the duration of this tutorial.

### Maximum Margin Classifier {#SVM_MM_Classifier}

If the classes are separable by a linear boundary, we can use a _Maximal Margin Classifier_ to find the classification boundary. To visualize an example of separated data, we generate 40 random observations and assign them to two classes. Upon visual inspection, we can see that infinitely many lines exist that split the two classes.

```{r}
# Construct sample data set - completely separated
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] <- x[y==1,] + 3/2
dat <- data.frame(x=x, y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

The goal of the maximum margin classifier is to identify the linear boundary that maximizes the total distance between the line and the closeset point in each class. We can use the `svm()` function in the `e1071` to find this boundary.

For the aesttheic of the `svm()` function, there are four types of kernel availabile:

1. linear: $u'v$
2. polynomial: $(\gamma u' v + coef0)^{degree}$
3. radial basis:  $e^{(-\gamma |u - v|)^2}$
4. sigmoid: $tanh(\gamma u'v + coef0)$

```{r}
# Fit Support Vector Machine model to data set
svmfit <- svm(y ~ ., 
              data = dat,
              kernel = "linear", 
              scale = FALSE)

# Plot Results
plot(svmfit, dat)
```

In the plot, points that are represented by an "X" are the __support vectors__, or the points that directly affect the classification line. The points marked with an "o" are the other points, which don't affect the calculation of the line. This principle will lay the foundation for support vector machines. The same plot can be generated using the `kernlab` package, with the following results:

```{r}
# fit model and produce plot
kernfit <- ksvm(x, y, type = "C-svc", kernel = 'vanilladot')
plot(kernfit, data = x)
```

But how do we decide how costly these missclasifications actually are? Instead of specifying a cost up front, we can use the `tune()` function from `e1071` to test various costs and identify which value produces the best fitting model.

```{r}
# find optimal cost of misclassification
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)
## 
## Call:
## best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, 
##     0.01, 0.1, 1, 5, 10, 100)), kernel = "linear")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  0.1 
##       gamma:  0.5 
## 
## Number of Support Vectors:  16
```

For our data set, the optimal cost (from amongst the choices we provided), is calculated to be 0.1, which does not penalize the model for miscclasified observations. Once this model has been identified, we can construct a table of predicted classes against true classes using the `predict()` command as follows.

```{r}
# Create a table of misclassified observations
ypred <- predict(bestmod, dat)
(misclass <- table(predict = ypred, truth = dat$y))

##        truth
## predict -1 1
##      -1  9 3
##      1   1 7
```

Using this support vector classifier, 80% of the observations were correctly classified, which matches what we see in the plot. If we wanted to test our classifier more rigorously, we could split our data into training and testing sets and then see how our SVC performed with the observations not used to construct the model. We will use this training-testing method later in this tutorial to validate our SVMs.

### Support Vector Machines {#SVM_Overview}

Support Vector Classifiers are a subset of the group of classification structures known as Support Vector Machines. Support Vector Machines can construct classification boundaries that are nonlinear in shape. The options for classification structures using the `svm()` command from the `e1071` package are linear, polynomial, radial, and sigmoid. To demonstrate a nonlinear classification boundary, we will construct a new data set.

```{r}
# construct larger random data set
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2.5
x[101:150,] <- x[101:150,] - 2.5
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))

# Plot data
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000", "#FF0000")) +
  theme(legend.position = "none")
```

Notice that the data is not linearly separable, and furthermore, isn’t all clustered together in a single group. There are two sections of class 1 observations with a cluster of class 2 observations in between. To demonstrate the power of SVMs, we’ll take 100 random observations from the set and use them to construct our boundary. We set `kernel = "radial"` based on the shape of our data and plot the results.

```{r}
# set pseudorandom number generator
set.seed(123)
# sample training data and fit model
train <- base::sample(200,100, replace = FALSE)
svmfit <- svm(y~., data = dat[train,], kernel = "radial", gamma = 1, cost = 1)
# plot classifier
plot(svmfit, dat)
```

The same procedure can be run using the `kernlab` package, which has far more `kernel` options than the corresponding function in `e1071`. In addition to the four choices in `e1071`, this package allows use of a hyperbolic tangent, Laplacian, Bessel, Spline, String, or ANOVA RBF kernel. To fit this data, we set the cost to be the same as it was before, 1.

```{r}
# Fit radial-based SVM in kernlab
kernfit <- ksvm(x[train,],y[train], type = "C-svc", kernel = 'rbfdot', C = 1, scaled = c())
# Plot training data
plot(kernfit, data = x[train,])
```

We see that, at least visually, the SVM does a reasonable job of separating the two classes. To fit the model, we used `cost = 1`, but as mentioned previously, it isn’t usually obvious which cost will produce the optimal classification boundary. We can use the `tune()` command to try several different values of cost as well as several different values of $γ$, a scaling parameter used to fit nonlinear boundaries.

```{r}
# tune model to find optimal cost, gamma values
tune.out <- tune(svm, y~., data = dat[train,], kernel = "radial",
                 ranges = list(cost = c(0.1,1,10,100,1000),
                 gamma = c(0.5,1,2,3,4)))
# show best model
tune.out$best.model
## 
## Call:
## best.tune(method = svm, train.x = y ~ ., data = dat[train, ], 
##     ranges = list(cost = c(0.1, 1, 10, 100, 1000), gamma = c(0.5, 
##         1, 2, 3, 4)), kernel = "radial")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  0.5 
## 
## Number of Support Vectors:  34
```

The model that reduces the error the most in the training data uses a cost of 1 and $\gamma$ value of 0.5. We can now see how well the SVM performs by predicting the class of the 100 testing observations:

```{r}
# validate model performance
(valid <- table(true = dat[-train,"y"], pred = predict(tune.out$best.model,newx = dat[-train,])))
##     pred
## true  1  2
##    1 58 19
##    2 16  7
```

Our best-fitting model produces 65% accuracy in identifying classes. For such a complicated shape of observations, this performed reasonably well. We can challenge this method further by adding additional classes of observations.

### SVMs for Multiple Classes {#SVM_Multi_Class}

The procedure does not change for data sets that involve more than two classes of observations. We construct our data set the same way as we have previously, only now specifying three classes instead of two:

```{r}
# construct data set
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0,50))
x[y==0,2] <- x[y==0,2] + 2.5
dat <- data.frame(x=x, y=as.factor(y))
# plot data set
ggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + 
  geom_point(size = 2) +
  scale_color_manual(values=c("#000000","#FF0000","#00BA00")) +
  theme(legend.position = "none")
```

The commands don’t change for the `e1071` package. We specify a cost and tuning parameter $\gamma$ and fit a support vector machine. The results and interpretation are similar to two-class classification.

```{r}
# fit model
svmfit <- svm(y~., data = dat, kernel = "radial", cost = 10, gamma = 1)
# plot results
plot(svmfit, dat)
```

We can check to see how well our model fit the data by using the `predict()` command, as follows:

```{r}
#construct table
ypred <- predict(svmfit, dat)
(misclass <- table(predict = ypred, truth = dat$y))
##        truth
## predict   0   1   2
##       0  38   2   4
##       1   8 143   4
##       2   4   5  42
```

As shown in the resulting table, 89% of our training observations were correctly classified. However, since we didn’t break our data into training and testing sets, we didn’t truly validate our results.

The `kernlab` package, on the other hand, can fit more than 2 classes, but cannot plot the results. To visualize the results of the `ksvm` function, we take the steps listed below to create a grid of points, predict the value of each point, and plot the results:

```{r}
# fit and plot
kernfit <- ksvm(as.matrix(dat[,2:1]),dat$y, type = "C-svc", kernel = 'rbfdot', 
                C = 100, scaled = c())

# Create a fine grid of the feature space
x.1 <- seq(from = min(dat$x.1), to = max(dat$x.1), length = 100)
x.2 <- seq(from = min(dat$x.2), to = max(dat$x.2), length = 100)
x.grid <- expand.grid(x.2, x.1)

# Get class predictions over grid
pred <- predict(kernfit, newdata = x.grid)

# Plot the results
cols <- brewer.pal(3, "Set1")
plot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05))

classes <- matrix(pred, nrow = 100, ncol = 100)
contour(x = x.2, y = x.1, z = classes, levels = 1:3, labels = "", add = TRUE)

points(dat[, 2:1], pch = 19, col = cols[predict(kernfit)])
```

#### Aplications

The `Khan` data set contains data on 83 tissue samples with 2308 gene expression measurements on each sample. These were split into 63 training observations and 20 testing observations, and there are four distinct classes in the set. It would be impossible to visualize such data, so we choose the simplest classifier (linear) to construct our model. We will use the svm command from `e1071` to conduct our analysis.

```{r}
# fit model
dat <- data.frame(x = Khan$xtrain, y=as.factor(Khan$ytrain))
(out <- svm(y~., data = dat, kernel = "linear", cost=10))
## 
## Call:
## svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  10 
##       gamma:  0.0004332756 
## 
## Number of Support Vectors:  58
```

First of all, we can check how well our model did at classifying the training observations. This is usually high, but again, doesn’t validate the model. If the model doesn’t do a very good job of classifying the training set, it could be a red flag. In our case, all 63 training observations were correctly classified.

```{r}
# check model performance on training set
table(out$fitted, dat$y)
##    
##      1  2  3  4
##   1  8  0  0  0
##   2  0 23  0  0
##   3  0  0 12  0
##   4  0  0  0 20
```

To perform validation, we can check how the model performs on the testing set:

```{r}
# validate model performance
dat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te <- predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
##        
## pred.te 1 2 3 4
##       1 3 0 0 0
##       2 0 6 2 0
##       3 0 0 4 0
##       4 0 0 0 5
```

The model correctly identifies 18 of the 20 testing observations. SVMs and the boundaries they impose are more difficult to interpret at higher dimensions, but these results seem to suggest that our model is a good classifier for the gene data.

## Stacked model

In the previous chapter, you have learned how to train individual learners, which in the context of this chapter will be referred to as base learners. _Stacking_ (sometimes called “stacked generalization”) involves training a new learning algorithm to combine the predictions of several base learners. First, the base learners are trained using the available training data, then a combiner or meta algorithm, called the super learner, is trained to make a final prediction based on the predictions of the base learners. Such stacked ensembles tend to outperform any of the individual base learners (e.g., a single RF or GBM) and have been shown to represent an asymptotically optimal system for learning (Laan, Polley, and Hubbard 2003).

### Prerequisites
This chapter leverages the following packages, with the emphasis on __h2o__.

```{r}
# Helper packages
library(rsample) # train-test splits
library(recipes) # feature engineering tasks

# Modeling pacakges
library(h2o)
```

To illustrate key concepts we continue with the Ames housing example from previous chapters:

```{r}
# Load and split the Ames housing data
ames <- AmesHousing::make_ames()
set.seed(123)  # for reproducibility
split <- initial_split(ames, strata = "Sale_Price")
ames_train <- training(split)
ames_test <- testing(split)

# Make sure we have consistent categorical levels
blueprint <- recipe(Sale_Price ~ ., data = ames_train) %>%
  step_other(all_nominal(), threshold = 0.005)

# Create training & test sets for h2o
h2o.init()
train_h2o <- prep(blueprint, training = ames_train, retain = TRUE) %>%
  juice() %>%
  as.h2o()
test_h2o <- prep(blueprint, training = ames_train) %>%
  bake(new_data = ames_test) %>%
  as.h2o()

# Get response and feature names
Y <- "Sale_Price"
X <- setdiff(names(ames_train), Y)
```

### The idea
eo Breiman, known for his work on classification and regression trees and random forests, formalized stacking in his 1996 paper on Stacked Regressions (Breiman 1996b). Although the idea originated in (Wolpert 1992) under the name “Stacked Generalizations”, the modern form of stacking that uses internal k-fold CV was Breiman’s contribution.

However, it wasn’t until 2007 that the theoretical background for stacking was developed, and also when the algorithm took on the cooler name, __Super Learner__ (Van der Laan, Polley, and Hubbard 2007). Moreover, the authors illustrated that super learners will learn an optimal combination of the base learner predictions and will typically perform as well as or better than any of the individual models that make up the stacked ensemble. Until this time, the mathematical reasons for why stacking worked were unknown and stacking was considered a black art.

#### Common ensemble methods

Ensemble machine learning methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms. 

The idea of combining multiple models rather than selecting the single best is well-known and has been around for a long time. In fact, many of the popular modern machine learning algorithms (including ones in previous chapters) are actually ensemble methods.

For example, bagging (Chapter 10) and random forests (Chapter 11) are ensemble approaches that average the predictions from many decision trees to reduce prediction variance and are robust to outliers and noisy data; ultimately leading to greater predictive accuracy. Boosted decision trees (Chapter 12) are another ensemble approach that slowly learns unique patterns in the data by sequentially combining individual, shallow trees.

Stacking, on the other hand, is designed to __ensemble a diverse group of strong learners__.

#### Super learner algorithm

The super learner algorithm consists of three phases:

1. Set up the ensemble

- Specify a list of $L$ base learners (with a specific set of model parameters)
- Specify a meta learning algorithm. This can be any one of the algorithms discussed in the previous chapters but most often is some form of regularized regression.

2. Train the ensemble

- Train each of the $L$ base learners on the training set.
- Perform k-fold CV on each of the base learners and collect the cross-validated predictions from each (the same k-folds must be used for each base learner). These predicted values represent $p_1, ..., p_l$ in Eq 15.1
- The $N$ cross-validated predicted values from each of the $L$ algorithm can be combined to form a new $N*L$ feature matrix (represented by $Z$ in Eq. (15.1)). This matrix, along with the original response vector ($y$) are called the "level-one" data. ($N$ = number of rows in the training set)

$$
n[p_1]...[p_L][y] -> n[Z][y] ...(15.1)
$$

- Train the meta learning algorithm on the level-ne data ($y=f(Z)$). The “ensemble model” consists of the  
$L$ base learning models and the meta learning model, which can then be used to generate predictions on new data.

3. PRedict on new data
- To generate ensemble predictions, first generate predictions from the base learners
- Feed those predictions into the meta learner to generate the ensemble prediction

Stacking never does worse than selecting the single best base learner on the training data (but not necessarily the validation or test data). The biggest gains are usually produced when stacking base learners that have high variability, and uncorrelated, predicted values. The more similar the predicted values are between the base learners, the less advantage there is to combining them.

#### Available packages

There are a few package implementations for model stacking in the R ecosystem. SuperLearner (Polley et al. 2019) provides the original Super Learner and includes a clean interface to 30+ algorithms. Package subsemble (LeDell et al. 2014) also provides stacking via the super learner algorithm discussed above; however, it also offers improved parallelization over the SuperLearner package and implements the subsemble algorithm (Sapp, Laan, and Canny 2014).42 Unfortunately, subsemble is currently only available via GitHub and is primarily maintained for backward compatibility rather than forward development. A third package, caretEnsemble (Deane-Mayer and Knowles 2016), also provides an approach for stacking, but it implements a bootsrapped (rather than cross-validated) version of stacking. The bootstrapped version will train faster since bootsrapping (with a train/test set) requires a fraction of the work of k-fold CV; however, the the ensemble performance often suffers as a result of this shortcut.

This chapter focuses on the use of h2o for model stacking. h2o provides an efficient implementation of stacking and allows you to stack existing base learners, stack a grid search, and also implements an automated machine learning search with stacked results. All three approaches will be discussed.

### Stacking existing models

The first approach to stacking is to train individual base learner models separately and then stack them together. For example, say we fould the optimal hyperparameters that provided the best predictive accuracy for the following algorithms:

1. Regularized regression base learner.
2.Random forest base learner.
3. GBM base learner.
4. XGBoost base learner.

We can train each of these models individually (see the code chunk below). However, to stack them later we need to do a few specific things:

1. All models must be trained on the same training set.
2. All models must be trained with the same number of CV folds.
2. All models must use the same fold assignment to ensure the same observations are used (we can do this by using fold_assignment = "Modulo").
4. The cross-validated predictions from all of the models must be preserved by setting keep_cross_validation_predictions = TRUE. This is the data which is used to train the meta learner algorithm in the ensemble.

```{r}
# Train & cross-validate a GLM model
best_glm <- h2o.glm(
  x = X, y = Y, training_frame = train_h2o, alpha = 0.1,
  remove_collinear_columns = TRUE, nfolds = 10, fold_assignment = "Modulo",
  keep_cross_validation_predictions = TRUE, seed = 123
)

# Train & cross-validate a RF model
best_rf <- h2o.randomForest(
  x = X, y = Y, training_frame = train_h2o, ntrees = 1000, mtries = 20,
  max_depth = 30, min_rows = 1, sample_rate = 0.8, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# Train & cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X, y = Y, training_frame = train_h2o, ntrees = 5000, learn_rate = 0.01,
  max_depth = 7, min_rows = 5, sample_rate = 0.8, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "RMSE",
  stopping_tolerance = 0
)

# Train & cross-validate an XGBoost model
best_xgb <- h2o.xgboost(
  x = X, y = Y, training_frame = train_h2o, ntrees = 5000, learn_rate = 0.05,
  max_depth = 3, min_rows = 3, sample_rate = 0.8, categorical_encoding = "Enum",
  nfolds = 10, fold_assignment = "Modulo", 
  keep_cross_validation_predictions = TRUE, seed = 123, stopping_rounds = 50,
  stopping_metric = "RMSE", stopping_tolerance = 0
)
```

We can now use `h2o.stackedEnsemble()` to stack these models. Note how we apply a random forest model as the metalearning algorithm. However, you could also apply regularized regression, GBM or a neural network as the metalearner (see `?h2o.stackedEnsemble` for details).

```{r}
# Train a stacked tree ensemble
ensemble_tree <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "my_tree_ensemble",
  base_models = list(best_glm, best_rf, best_gbm, best_xgb),
  metalearner_algorithm = "drf"
)
```

Since our ensemble is built on the CV results of the base learners, but has no cross-validation results of its own, we’ll use the test data to compare our results. If we assess the performance of our base learners on the test data we see that the stochastic GBM base learner has the lowest RMSE of 20859.92. The stacked model achieves a small 1% performance gain with an RMSE of 20664.56.

```{r}
# Get results from base learners
get_rmse <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$RMSE
}
list(best_glm, best_rf, best_gbm, best_xgb) %>%
  purrr::map_dbl(get_rmse)
## [1] 30024.67 23075.24 20859.92 21391.20

# Stacked results
h2o.performance(ensemble_tree, newdata = test_h2o)@metrics$RMSE
## [1] 20664.56
```

We previously stated that the biggest gains are usually produced when we are stacking base learners that have high variability, and uncorrelated, predicted values. If we assess the correlation of the CV predictions we can see strong correlation across the base learners, especially with three tree-based learners. Consequentley, stacking provides less advantage in this situation since the base learners have highly correlated predictions; however, a 1% performance improvement can still be considerable improvement depending on the business context.

```{r}

data.frame(
   GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name)),
  RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name)),
  GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name)),
  XGB_pred = as.vector(h2o.getFrame(best_xgb@model$cross_validation_holdout_predictions_frame_id$name))
) %>% cor()
##           GLM_pred   RF_pred  GBM_pred  XGB_pred
## GLM_pred 1.0000000 0.9390229 0.9291982 0.9345048
## RF_pred  0.9390229 1.0000000 0.9920349 0.9821944
## GBM_pred 0.9291982 0.9920349 1.0000000 0.9854160
## XGB_pred 0.9345048 0.9821944 0.9854160 1.0000000
```

### Stacking a grid search

An alternative ensemble approach focuses on stacking multiple models generated from the same base learner. In each of the previous chapters, you learned how to perform grid searches to automate the tuning process. Often we simply select the best performing model in the grid search but we can also apply the concept of stacking to this process.

Many times, certain tuning parameters allow us to find unique patterns within the data. By stacking the results of a grid search, we can capitalize on the benefits of each of the models in our grid search to create a meta model. For example, the following performs a random grid search across a winde range of GBM hyperparameter settings. We set the search to stop after 25 models have run.

```{r}
# Define GBM hyperparameter grid
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(0.99, 1),
  sample_rate = c(0.5, 0.75, 1),
  col_sample_rate = c(0.8, 0.9, 1)
)

# Define random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
)

# Build random grid search 
random_grid <- h2o.grid(
  algorithm = "gbm", grid_id = "gbm_grid", x = X, y = Y,
  training_frame = train_h2o, hyper_params = hyper_grid,
  search_criteria = search_criteria, ntrees = 5000, stopping_metric = "RMSE",     
  stopping_rounds = 10, stopping_tolerance = 0, nfolds = 10, 
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123
)
```

If we look at the grid search models we see that the cross-validated RMSE ranges from 20756–57826
```{r}
# Sort results by RMSE
h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "rmse"
)
## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  learn_rate_annealing 
##   -  max_depth 
##   -  min_rows 
##   -  sample_rate 
## Number of models: 25 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing rmse
##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate         model_ids               rmse
## 1             0.9       0.01                  1.0         3      1.0         1.0 gbm_grid_model_20  20756.16775065606
## 2             0.9       0.01                  1.0         5      1.0        0.75  gbm_grid_model_2 21188.696088824694
## 3             0.9        0.1                  1.0         3      1.0        0.75  gbm_grid_model_5 21203.753908665003
## 4             0.8       0.01                  1.0         5      5.0         1.0 gbm_grid_model_16 21704.257699437963
## 5             1.0        0.1                 0.99         3      1.0         1.0 gbm_grid_model_17 21710.275753497197
## 
## ---
##    col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate         model_ids               rmse
## 20             1.0       0.01                  1.0         1     10.0        0.75 gbm_grid_model_11 26164.879525289896
## 21             0.8       0.01                 0.99         3      1.0        0.75 gbm_grid_model_15  44805.63843296435
## 22             1.0       0.01                 0.99         3     10.0         1.0 gbm_grid_model_18 44854.611500840605
## 23             0.8       0.01                 0.99         1     10.0         1.0 gbm_grid_model_21 57797.874642563846
## 24             0.9       0.01                 0.99         1     10.0        0.75 gbm_grid_model_10  57809.60302408739
## 25             0.8       0.01                 0.99         1      5.0        0.75  gbm_grid_model_4  
```

If we apply the best performing model to our test set, we achieve an RMSE of 21599.8.

```{r}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
## H2ORegressionMetrics: gbm
## 
## MSE:  466551295
## RMSE:  21599.8
## MAE:  13697.78
## RMSLE:  0.1090604
## Mean Residual Deviance :  466551295
```

Rather than use the single best model, we can combine all the models in our grid search using a super learner. In this example, our super learner does not provide any performance gains because the hyperparameter settings of the leading models have low variance which esults in predictions that are highly correlated. However, in cases where you see high variability across hyperparameter settings for your leading models, stacking the grid search or even the leaders in the grid search can provide significant performance gains.

Stacking a grid search provides the greatest benefit when leading models from the base learner have high variance in their hyperparameter settings.

```{r}
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids, metalearner_algorithm = "gbm"
)

# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
## H2ORegressionMetrics: stackedensemble
## 
## MSE:  469579433
## RMSE:  21669.78
## MAE:  13499.93
## RMSLE:  0.1061244
## Mean Residual Deviance :  469579433
```


### Automated machine learning

Our final topic to discuss involves performing an automated search across multiple base learners and then stack the resulting models  (this is sometimes referred to as _automated machine learning_ or AutoML). This is very much like the grid searches that we have been performing for base learners and discussed in Chapters 4-14; however, rather than search across a variety of parameters for a _single base learner_, we want to perform a search across a variety of hyperparameter settings for many _different base learners_.

There are several competitors that provide licensed software that help automate the end-to-end machine learning process to include feature engineering, model validation procedures, model selection, hyperparameter optimization, and more. Open source applications are more limited and tend to focus on automating the model building, hyperparameter configurations, and comparison of model performance.

Although AutoML has made it easy for non-experts to experiment with machine learning, there is still a significant amount of knowledge and background in data science that is required to produce high-performing machine learning models. AutoML is more about freeing up your time (which is quite valuable). The machine learning process is often long, iterative, and repetitive and AutoML can also be a helpful tool for the advanced user, by simplifying the process of performing a large number of modeling-related tasks that would typically require hours/days writing many lines of code. This can free up the user’s time to focus on other tasks in the data science pipeline such as data-preprocessing, feature engineering, model interpretability, and model deployment.

__h2o__ provides an open source implementation of AutoML with the `h2o.automl()` function. The current version of `h2o.automl()` trains and cross-validates a random forest, an extremely-randomized forest, a random grid of GBMs, a random grid of DNNs, and then trains a stacked ensemble using all of the models; see `?h2o::h2o.automl` for details.

The following performs an automated search for two hours, which ended up assessing 80 models. `h2o.automl()` will automatically use the same folds for stacking so you do not need to specify `fold_assignment = "Modulo"`. This allows for consistent model comparison across the same CV sets. We see that most of the leading models are GBM variants and achieve an RMSE in the 22000–23000 range. As you probably noticed, this was not as good as some of our best models we found using our own GBM grid searches (reference Chapter 12). However, we could start this AutoML procedure and then spend our two hours performing other tasks while h2o automatically assesses these 80 models. The AutoML procedure then provides us direction for further analysis. In this case, we could start by further assessing the hyperparameter settings in the top five GBM models to see if there were common attributes that could point us to additional grid searches worth exploring.

```{r}
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
  x = X, y = Y, training_frame = train_h2o, nfolds = 5, 
  max_runtime_secs = 60 * 120, max_models = 50,
  keep_cross_validation_predictions = TRUE, sort_metric = "RMSE", seed = 123,
  stopping_rounds = 50, stopping_metric = "RMSE", stopping_tolerance = 0
)

# Assess the leader board; the following truncates the results to show the top 
# and bottom 15 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>% 
  as.data.frame() %>%
  dplyr::select(model_id, rmse) %>%
  dplyr::slice(1:25)
##                                               model_id   rmse
## 1                     XGBoost_1_AutoML_20190220_084553   22229.97
## 2            GBM_grid_1_AutoML_20190220_084553_model_1   22437.26
## 3            GBM_grid_1_AutoML_20190220_084553_model_3   22777.57
## 4                         GBM_2_AutoML_20190220_084553   22785.60
## 5                         GBM_3_AutoML_20190220_084553   23133.59
## 6                         GBM_4_AutoML_20190220_084553   23185.45
## 7                     XGBoost_2_AutoML_20190220_084553   23199.68
## 8                     XGBoost_1_AutoML_20190220_075753   23231.28
## 9                         GBM_1_AutoML_20190220_084553   23326.57
## 10           GBM_grid_1_AutoML_20190220_075753_model_2   23330.42
## 11                    XGBoost_3_AutoML_20190220_084553   23475.23
## 12       XGBoost_grid_1_AutoML_20190220_084553_model_3   23550.04
## 13      XGBoost_grid_1_AutoML_20190220_075753_model_15   23640.95
## 14       XGBoost_grid_1_AutoML_20190220_084553_model_8   23646.66
## 15       XGBoost_grid_1_AutoML_20190220_084553_model_6   23682.37
## ...                                                ...        ...
## 65           GBM_grid_1_AutoML_20190220_084553_model_5   33971.32
## 66           GBM_grid_1_AutoML_20190220_075753_model_8   34489.39
## 67  DeepLearning_grid_1_AutoML_20190220_084553_model_3   36591.73
## 68           GBM_grid_1_AutoML_20190220_075753_model_6   36667.56
## 69      XGBoost_grid_1_AutoML_20190220_084553_model_13   40416.32
## 70           GBM_grid_1_AutoML_20190220_075753_model_9   47744.43
## 71    StackedEnsemble_AllModels_AutoML_20190220_084553   49856.66
## 72    StackedEnsemble_AllModels_AutoML_20190220_075753   59127.09
## 73 StackedEnsemble_BestOfFamily_AutoML_20190220_084553   76714.90
## 74 StackedEnsemble_BestOfFamily_AutoML_20190220_075753   76748.40
## 75           GBM_grid_1_AutoML_20190220_075753_model_5   78465.26
## 76           GBM_grid_1_AutoML_20190220_075753_model_3   78535.34
## 77           GLM_grid_1_AutoML_20190220_075753_model_1   80284.34
## 78           GLM_grid_1_AutoML_20190220_084553_model_1   80284.34
## 79       XGBoost_grid_1_AutoML_20190220_075753_model_4   92559.44
## 80      XGBoost_grid_1_AutoML_20190220_075753_model_10  125384.88
```

### References

- Breiman, Leo. 1996b. “Stacked Regressions.” Machine Learning 24 (1). Springer: 49–64.
- Deane-Mayer, Zachary A., and Jared E. Knowles. 2016. CaretEnsemble: Ensembles of Caret Models. https://CRAN.R-project.org/package=caretEnsemble.
- Laan, Mark J. van der, Eric C. Polley, and Alan E. Hubbard. 2003. “Super Learner.” Statistical Applications in Genetics and Molecular Biology 6 (1).
- LeDell, Erin, Stephanie Sapp, Mark van der Laan, and Maintainer Erin LeDell. 2014. Package “Subsemble”. https://CRAN.R-project.org/package=subsemble.
- Polley, Eric, Erin LeDell, Chris Kennedy, and Mark van der Laan. 2019. SuperLearner: Super Learner Prediction. https://CRAN.R-project.org/package=SuperLearner.
- Sapp, Stephanie, Mark J van der Laan, and John Canny. 2014. “Subsemble: An Ensemble Method for Combining Subset-Specific Algorithm Fits.” Journal of Applied Statistics 41 (6). Taylor & Francis: 1247–59.
- Van der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” Statistical Applications in Genetics and Molecular Biology 6 (1). De Gruyter.
- Wolpert, David H. 1992. “Stacked Generalization.” Neural Networks 5: 241–59.

## Interpretable ML

In the previous chapters you learned how to train several different forms of advanced ML models. Often, these models are considered “black boxes” due to their complex inner-workings. However, because of their complexity, they are typically more accurate for predicting nonlinear, faint, or rare phenomena. Unfortunately, more accuracy often comes at the expense of interpretability, and interpretability is crucial for business adoption, model documentation, regulatory oversight, and human acceptance and trust. Luckily, several advancements have been made to aid in interpreting ML models over the years and this chapter demonstrates how you can use them to extract important insights. Interpreting ML models is an emerging field that has become known as interpretable machine learning (IML).

### Prerequisites 

There are many packages that provide robust machine learning interpretation capabilities. Unfortunately, there is not one single package that is optimal for all IML applications; rather, when performing IML you will likely use a combination of packages. 

The following packages are used in this chapter.

```{r}
# Helper packages
library(dplyr)      # for data wrangling
library(ggplot2)    # for awesome graphics

# Modeling packages
library(h2o)       # for interfacing with H2O
library(recipes)   # for ML recipes
library(rsample)   # for data splitting
library(xgboost)   # for fitting GBMs

# Model interpretability packages
library(pdp)       # for partial dependence plots (and ICE curves)
library(vip)       # for variable importance plots
library(iml)       # for general IML-related functions
library(DALEX)     # for general IML-related functions
library(lime)      # for local interpretable model-agnostic explanations
```

To illustrate various concepts we’ll continue working with the h2o version of the Ames housing example from Section 15.1. We’ll also use the stacked ensemble model (`ensemble_tree`) created in Section 15.3.

### The idea

It is not enough to identify a machine learning model that optimizes predictive perfromance; understanding and trusting model results is a hallmark of good science and necessary for our model to be adopted.

 As we apply and embed ever-more complex predictive modeling and machine learning algorithms, both we (the analysts) and the business stakeholders need methods to interpret and understand the modeling results so we can have trust in its application for business decisions (Doshi-Velez and Kim 2017).

Advancements in interpretability now allow us to extract key insights and actionable information from the most advanced ML models. These advancements allow us to answer questions such as:

- What are the most important customer attributes driving behavior?
- How are these attributes related to the behavior output?
- Do multiple attributes interact to drive different behavior among customers?
- Why do we expect a customer to make a particular decision?
- Are the decisions we are making based on predicted results fair and reliable?

Approaches to model interpretability to answer the exemplar questions above can be broadly categorized as providing global or local explanations. It is important to understand the entire model that you have trained ona global scale, and also to zoome in on local regions of your data or your predictions and derive explanations. Being able to answer such questions and provide both levels of explanation is key to any ML project becoming accepted, adopted, embedded, and properly utilized.

#### Global interpretation

Global interpretability is about understanding how the model makes predictions, based on a holistic view of its features and how they influence the underlying model structure. It answers questions regarding which features are relatively influential, how these features influence the response variable, and what kinds of potential interactions are occurring. Global model interpretability helps to understand the relationship between the response variable and the individual features (or subsets thereof). Arguably, comprehensive global model interpretability is very hard to achieve in practice. Any model that exceeds a handful of features will be hard to fully grasp as we will not be able to comprehend the whole model structure at once.

While global model interpretability is usually out of reach, there is a better chance to understand at least some models on a modular level. This typically revolves around gaining understanding of which features are the most influential (via _feature importance_) and then focusing on how the most influential variables drive the model output (via _feature effects_). Although you may not be able to fully grasp a model with a hundred features, typically only a dozen or so of these variables are really influential in driving the model’s performance. And it is possible to have a firm grasp of how a dozen variables are influencing a model.

#### Local interpretation 

Global intepretability methods help us to understand the inputs and their overall relationship with the response variable, but they can be highly deceptive in some cases (e.g., when strong interactions are occurring). Although a given feature may influence the predictive accuracy of our model as a whole, it does not mean that that feature has the largest influence on a predicted value for a given observation (e.g., a customer, house, or employee) or even a group of observations. Local interpretations help us understand what features are influencing the predicted response for a given observation (or small group of observations). These techniques help us to not only answer what we expect a customer to do, but also why our model is making a specific prediction for a given observation.

There are three primary approaches to local interpretation:

- Local interpretable model-agnostic explanations (LIME)
- Shapley values
- Localized step-wise procedures

These techniques have the same objective; to explain which variables are most influential in predicting the target for a set of observations. To illustrate, we’ll focus on two observations. The first is the observation that our ensemble produced the highest predicted Sale_Price for (i.e., observation 1825 which has a predicted `Sale_Price` of $663,136$), and the second is the observation with the lowest predicted Sale_Price (i.e., observation 139 which has a predicted Sale_Price of $47,245.45$). Our goal with local interpretation is to explain what features are driving these two predictions

```{r}
# Compute predictions
predictions <- predict(ensemble_tree, train_h2o) %>% as.vector()

# Print the highest and lowest predicted sales price
paste("Observation", which.max(predictions), 
      "has a predicted sale price of", scales::dollar(max(predictions))) 
## [1] "Observation 1825 has a predicted sale price of $663,136"
paste("Observation", which.min(predictions), 
      "has a predicted sale price of", scales::dollar(min(predictions)))  
## [1] "Observation 139 has a predicted sale price of $47,245.45"

# Grab feature values for observations with min/max predicted sales price
high_ob <- as.data.frame(train_h2o)[which.max(predictions), ] %>% select(-Sale_Price)
low_ob  <- as.data.frame(train_h2o)[which.min(predictions), ] %>% select(-Sale_Price)
```

#### Model-specifc vs model-agnostic

It is also important to understand that there are model-specific and model-agnostic approaches for interpreting your model. Many of the approaches you’ve seen in the previous chapters for understanding feature importance are model-specific. For example, in linear models we can use the absolute value of the  
t
 –statistic as a measure of feature importance (though this becomes complicated when your linear model involves interaction terms and transformations). Random forests, on the other hand, can record the prediction accuracy on the OOB portion of the data, then the same is done after permuting each predictor variable, and the difference between the two accuracies are then averaged over all trees, and normalized by the standard error. These model-specific interpretation tools are limited to their respective model classes. There can be advantages to using model-specific approaches as they are more closely tied to the model performance and they may be able to more accurately incorporate the correlation structure between the predictors (Kuhn and Johnson 2013).

However, there are also some disadvantages. For example, many ML algorithms (e.g., stacked ensembles) have no natural way of measuring feature importance:

```{r}
vip(ensemble_tree, method = "model")
## Error in vi_model.default(ensemble_tree, method = "model") : 
##  model-specific variable importance scores are currently not available for objects of class "h2o.stackedEnsemble.summary"
```


Furthermore, comparing model-specific feature importance across model classes is difficult since you are comparing different measurements (e.g., the magnitude of  
t
 -statistics in linear models vs. degradation of prediction accuracy in random forests). In model-agnostic approaches, the model is treated as a “black box”. The separation of interpretability from the specific model allows us to easily compare feature importance across different models.
 
Ultimately, there is no one best approach for model interpretability. Rather, only by applying multiple approaches (to include comparing model specific and model agnostic results) can we really gain full trust in the interpretations we extract.

An important item to note is that when using model agnostic procedures, additional code preparation is often required. For example, the iml (Molnar 2019), __DALEX__ (Biecek 2019), and __LIME__ (Pedersen and Benesty 2018) packages use purely model agnostic procedures. Consequently, we need to create a model agnostic object that contains three components:

1. A data frame with just the features (must be of class "data.frame", cannot be an "H2OFrame" or other object).
2. A vector with the actual responses (must be numeric—0/1 for binary classification problems).
3. A custom function that will take the features from 1), apply the ML algorithm, and return the predicted values as a vector.

The following code extracts these items for the __h2o__ example:

```{r}
# 1) create a data frame with just the features
features <- as.data.frame(train_h2o) %>% select(-Sale_Price)

# 2) Create a vector with the actual responses
response <- as.data.frame(train_h2o) %>% pull(Sale_Price)

# 3) Create custom predict function that returns the predicted values as a vector
pred <- function(object, newdata)  {
  results <- as.vector(h2o.predict(object, as.h2o(newdata)))
  return(results)
}

# Example of prediction output
pred(ensemble_tree, features) %>% head()
## [1] 207144.3 108958.2 164248.4 241984.2 190000.7 202795.8
```

Once we have these three components we can create our model agnostic obejcts for the __iml__ and __DALEX__ pacakages, which will just pass these downstream components (along with the ML model) to other functions.

```{r}
# iml model agnostic object
components_iml <- Predictor$new(
  model = ensemble_tree, 
  data = features, 
  y = response, 
  predict.fun = pred
)

# DALEX model agnostic object
components_dalex <- DALEX::explain(
  model = ensemble_tree,
  data = features,
  y = response,
  predict_function = pred
)
```

### Permutation-based feature importance

In previous chapters, we illustrated a few model-specific approaches for measuring feature importance (e.g., linear models, we used the absolute value of the t-statistic). For SVMs, on the other hand, we had to rely on a model-agnostic approach which was based on the permutation feature importance measurement introduced for random forests by Breiman (2001) (see Section 11.6) and expanded on by Fisher, Rudin, and Dominici (2018).

#### Concept








## Resampling methods

Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the oriiginal training sample.

### tl;dr
This tutorial services asa an itorudction to sampling methods and covers:

1. [Replication requirements](#RS_RR): What you’ll need to reproduce the analysis in this tutorial.
2. [Why resampling](#RS_Overview): Understand why resampling is important.
3. [Leave-one-out cross-validation](#RS_LOCV): Provide greater reliability of an estimate test error.
4. [k-fold cross validation](#RS_KFCV): A faster alternative to leave-one-out cross validation.
5. [Bootstrapping](#RS_BSPNG): Quantify uncertainty around a particular statistic.
6. [Additional resources](#RS_Resources): Additional resources to help you learn more.

### Replication requirements {#RS_RR}

This tutorial primarily leverages the `Auto` data provided by the `ISLR` package. This is a  data set that contains gas mileage, horsepower, and other information for 392 vehicles. We’ll also use tidyverse for some basic data manipulation and visualization. Most importantly, we’ll use the `boot` package to illustrate resampling methods.

```{r}
# Packages
library(tidyverse)  # data manipulation and visualization
library(boot)       # resampling and bootstrapping

# Load data 
(auto <- as_tibble(ISLR::Auto))
```


### Resampling basics {#RS_Overview}

Thus far, in our tutorial we have been using the `validation` or `hold_out` approach to estimate the predicted error of our predictive models. This involves randomly dividing the available set of observations into two parts, a _training set_ and a _testing set_ (aka _validation set_). Our statistical model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically assessed using MSE in the case of a quantitative response) provides an estimate of the test error rate. 

The validation set approach is conceptually simple and is easy to implement. But it has two potential drawbacks:

First, the estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set. I will illustrate on our auto data set. Here we see that there is a relationship between mpg and horsepower and it doesn’t seem linear but we’re not sure which polynomial degree creates the best fit.

```{r}
ggplot(auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, linetype = 2) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE, linetype = 3) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 4), se = FALSE, linetype = 4)  
```

Let's go ahead and do the traditional validation set approach to split our data into a training and testing set. Then we will fit 10 different models ranging from a linear model to a 10th degree polynomial model. 

The results show us there is a steep decline in our test error (MSE) rate when we go from a linear model to a quadratic model; however, the MSE flatlines beyond that point suggesting that adding more polynomial degrees likely does not improve the model performance.

```{r}
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(auto), replace=T,
                 prob = c(0.6, 0.4))
train <- auto[sample,]
test <- auto[!sample, ]

# loop for first ten polynomial
mse.df <- tibble(degree=1:10, mse=NA)

for (i in 1:10){
  lm.fit <- lm(mpg ~ poly(horsepower, i), data = train)
  mse.df[i,2] <- mean((test$mpg - predict(lm.fit, test))^2)
}

ggplot(mse.df, aes(degree, mse))+
  geom_line()+
  geom_point()+
  ylim(c(10,30))

```

However, our MSE is dependent on our training and test samples. If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the test MSE each time. I illustrate below, which displays ten different validation set MSE curves from the auto data set, produced using ten different random splits of the observations into training and validation sets. All ten curves indicate that the model with quadratic term has a dramatically smaller validation set MSE than the model with only a linear term. 

Furthermore, all ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model. But it is worth noting that each of the ten curves result in a __different test MSE estimate__ for each of the ten regression models considered. And there is no consensus among the curves as to which model results in the smallest validation set MSE.

```{r}
mse.df.2 <- tibble(
  sample = vector("integer", 100),
  degree = vector("integer", 100),
  mse = vector("double",100)
)

counter <- i

for (i in 1:10){
  set.seed(i)
  sample <- sample(c(TRUE, FALSE), nrow(auto), replace=TRUE, prob=c(0.6,0.4))
  train <- auto[sample, ]
  test <- auto[!sample,]
  
  # modeling
  for (j in 1:10){
    lm.fit <- lm(mpg ~ poly(horsepower, j), data = train)
    
    # add degree & mse values
    mse.df.2[counter,2] <- j
    mse.df.2[counter, 3] <- mean((test$mpg - predict(lm.fit, test))^2)
    
    # add sample identifier
    mse.df.2[counter,1] <- i
    counter <- counter +1
  }
  next
}

mse.df.2 %>% 
  ggplot(aes(degree, mse, color=factor(sample)))+
  geom_line(show.legend = FALSE)+
  geom_point(show.legend = FALSE)+
  ylim(c(10,30))

```

### Leave-one-out cross-validation {#RS_LOCV}

Leav-one-out cross-validation (LOOCV) is closely related to the validation set approach as it involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size (i.e.,  60% training, 40% validation), a single observation ($x_1, y_1$) is used for the validation set, and the remaining $n-1$ observations ($(x_2,y_2)$...$(x_n, y_n)$) make up the training set.

The statistical learning method is fit on the $n-1$ training observations, and a prediction $\hat{y_1}$ is made for the excluded observation. since the validation observation $(x_1,y_1)$ was not used in the fitting process, the estimate error $MSE_1=(y_1-\hat{y_1})^2$ provides and approximately unbiased estimate for the test error. But even though $MSE_1$ is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based on a single observation.

However, we can repeat the procedure by selecting a difference row $(x_2,y_2)$ for the validation data, training the statistical learning procedure on the other $n-1$ observations and computing rror $MSE_2=(y_2-\hat{y_2})^2$. We can repeate this approach n times, where each time we holdout a different, single observation to validate on. This produces a total of n squared errors, $MSE_1,...,MSE_n$. The LOOCV estimate for the test MSE is the average of these $n$ test error estimates.

$$
CV_(n) = \frac{1}{n}\Sigma_{i=1}^n MSE_i...(1)
$$

To peform this procedure in R,  we first need to understand an important nuance. In the logistic regression tutorial, we used the `glm` function to perform logistic regression by passing in the `family = "binomial"` argument. But if we use glm to fit a model without passing in the family argument, then it performs linear regression, just like the `lm` function. So, for instance:

```{r eval=FALSE}
glm.fit <- glm(mpg ~ horsepower, data = auto)
coef(glm.fit)
## (Intercept)  horsepower 
##  39.9358610  -0.157844
```

is the same as

```{r eval=FALSE}
lm.fit <- lm(mpg ~ horsepower, data = auto)
coef(lm.fit)
## (Intercept)  horsepower 
##  39.9358610  -0.1578447
```

Why is this important? Because we can perform LOOCV for any generalized linear model using `glm` and the `cv.glm` function from the [`boot`](http://cran.r-project.org/web/packages/boot/index.html) package. boot provides extensive facilities for bootstrapping and related resampling methods. You can bootstrap a single statistic (e.g. a median), a vector (e.g., regression weights), or as you’ll see in this tutorial perform cross-validation. To perform LOOCV for a given generalized linear model we simply:

1. fit our model across the entire data set with `glm`
2. feed the entire data set and our fitted model into `cv.glm`

```{r}
# step 1: fit model
glm.fit <- glm(mpg ~ horsepower, 
               data = auto)

# step 2 perform LOOCV across entire data set
loocv.err <- cv.glm(auto, glm.fit)
loocv.err %>% 
  str()
## List of 4
##  $ call : language cv.glm(data = auto, glmfit = glm.fit)
##  $ K    : num 392
##  $ delta: num [1:2] 24.2 24.2
##  $ seed : int [1:626] 403 392 -1703707781 1994959178 434562476 -1277611857 -1105401243 1020654108 526650482 -1538305299 ...
```

`cv.glm` provide a list with 4 outputs:

1. `call`: the oroginal function call
2. `K`: the number of _folds_ used. In our case, it is 392 because the LOOCV looped through and pulled out each observation at least once to use a test observation.
3. `delta`: the CV estimate of prediction error. The first number, which is the primary number we care about, is the output from Eq.1 listed above.
4. `seed`: the values of the random seed used for the function call

The result we primarily care about is the corss-validation estimate of test error (Eq.1). our cross-validation estimate for the test error is approximately 24.23. This estimate is far less biased estimate of the test error compared to our single test MSE produced by a training - testing validation approach.

```{r}
loocv.err$delta[1]
```

We can repeat this procedure to estiamte an unbiased MSE across multiple model fits. For example, to assess multiple polynomial fits (as we did above) to identify the one that represents the best fit we can integrate this procedure into a function. 

Here we develop a function that computes the LOOCV MSE based on specified polynomial degree. We then feed this function (via `map_dbl`) values 1-5 to compute the first through fifth polynomials.

```{r}
# create function that computes LOOCV MSE on specified polynomial degree
loocv_error <- function(x){
  glm.fit <- glm(mpg ~ poly(horsepower, x), data = auto)
  cv.glm(auto, glm.fit)$delta[1]
}

# compute LOOCV MSE for polynomial degrees 1-5
library(purrr)
1:5 %>% 
  map_dbl(loocv_error) %>% 
  tibble()

```

Our results illustrate a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials. Thus, our unbiased MSEs suggest that using a 2nd polynomial (quadratic fit) is likely the optimal model balancing interpretation and low test errors.

This LOOCV approach can be used with any kind of predictive modeling. For example, we could use it with logistic regression or linear disciminant analysis. Unfortunately, this can be very time consuming approach if $n$ is large, you are trying to loop through many models (1-10 polynomials), and if each individual model is slow to fit. For example, if we wanted to perform this approach on the `ggplot2::diamonds` data set for a linear regression model, which contains 53,940 observations, the comutation time is nearly 30 minutes.

```{r eval=FALSE}
# DO NOT RUN THIS CODE - YOU WILL BE WAITING A LONG TIME!!
# system.time({
#     diamonds.fit <- glm(price ~ carat + cut + color + clarity, data = diamonds)
#     cv.glm(diamonds, diamonds.fit)
# })
#      user    system    elapsed 
#  1739.041   285.496   2035.062 
```


### K-Fold Cross Validation {#RS_KFCV}

An alternative to LOOCV is the `k`-fold cross validation approach. This resamling method involves randomly dividing the data into `k` groups (aka `folds`) of approximately equal size. 

The first fold is treated as a validation set, and the statistical method is fit on the remaining data. The mean squared error, $MSE_1$ is then computed on the observations in the held-out fold. This procedure is repreated `k` times; each time, a different group of observations is treated as the validation set. 

This process results in `k` estimates of the test error, $MSE_1, MSE_2, ..., MSE_k$. Thus, the `k` fold CV is computed by averaging these values:

$$
CV_(k) = \frac{1}{k} \Sigma_{i=1}^k MSE_i
$$

It is not hard to see that LOOCV is a special case of _k_-fold approach which _k_ is set to equal _n_. However, using the _k_-fold approach, one typically use $k=5$ or $k=10$. This can substantially reduce the computation burden of LOOCV.

Furthermore, there has been sufficient empirical evidence that demonstrates using 5-10 folds yild surprisingly accurate test error rate estimates (see chapter 5 of ISLR for model details).

We can implement the _k_-fold approach just as we did with the LOOCV approach. The only difference is incorporating the `K=10` argument that we include in the `cv.glm` function.

Below illustrates our _k_-fold MSE values for the different polynomial models on our auto data. When compared to the LOOCV outputs we see that the results are nearly identical.

```{r}
# create functions that computes k-fold MSE based on specified polynomial degree

kfcv_error <- function(x){
  glm.fit <- glm(mpg ~ poly(horsepower, x), data = auto)
  cv.glm(auto, glm.fit, K=10)$delta[1]
}

# compute k-fold MSE for polynomial degree 1-5
1:5 %>% map_dbl(kfcv_error)

# compare to LOOCV MSE values
1:5 %>% map_dbl(loocv_error)
```

We can also illustrate the computational advantage of the k-fold approach. As we saw, using LOOCV on the diamonds data set took nearly 30 minutes whereas using the k-fold approach only takes about 4 seconds.

```{r}
system.time({
  diamonds.fit <- glm(price ~ carat + cut + color + clarity, data = diamonds)
  cv.glm(diamonds, diamonds.fit, K = 10)
})
##    user  system elapsed 
##   3.760   0.564   4.347
```

We can apply this same approach to classification problems as well. For example, in the previous tutorial we compared the performance of a logistic regression, linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA) on some stock market data using the traditional training vs. testing (60%/40%) data splitting approach. We could’ve performed the same assessment using cross validation. In the classification setting, the LOOCV error rate takes the form

$$
CV_(n) = \frac{1}{n}\Sigma_{i=1}^n Err_i
$$

where $Err_i = I(y_i = \hat{y_i})$. The k-fold CV error rate and validation set error rates are defined analogously.

Consequently, for the logistic regression, we use `cv.glm` to perform a $k-$fold cross validation. The end result is an estimated CV error of .5 (Note: since the response variable is binary we incorporate a new cost function to compute the estimated error in Eq.3).

```{r}
stock <- ISLR::Smarket

# The cost function here correlates to that in Equ.3 
glm.fit <- glm(Direction ~ Lag1 + Lag2, family = binomial, data = stock)

# The cost function here correlates to that in Eq.3
cost <- function(r, pi = 0) {mean(abs(r - pi) > 0.5)}

# compute the k-fold estimated error with our cost function
cv.glm
cv.glm(stock, glm.fit, cost, K = 10)$delta[1]
```

To performm cross validation with our LDA and QDA models we use a slightly different approach. Both the `lda` and `qda` functions have built-in cross validation arguments. Thus, setting `CV = TRUE` within these functions will result in a LOOCV execution and the class and posterior probabilities are a product of this cross validation.

```{r}
library(MASS)

# fit discirimant analysis modles with cv=TRUE for LOOCV
lda.fit <- lda(Direction ~ Lag1+Lag2, CV=TRUE, data=stock)
qda.fit <- qda(Direction ~ Lag1+Lag2, CV=TRUE, data=stock)

# compute estimated test error based on CV
mean(lda.fit$class != stock$Direction)
## [1] 0.4816
mean(qda.fit$class != stock$Direction)
## [1] 0.4872

```

Thus, the results are similar to what we saw in the previous tutorial, none of these models do an exceptional (or even decent) job! However, we see that the LOOCV estimated error for the QDA model (.487) is fairly higher than what we saw in the train-test approach(.40). This suggests our previous QDA model with the train-test validation approach may have been a bit optimistically biased.

### Bootstrap {#RS_BSPNG}

_Bootstrapping_ is widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical leanring method.

As a simple example, bootstrapping can be used to estimate the standard errors of the coefficients from a linear regression fit. In the case of linear regression, this is not particularly useful, since we saw in the linear regression tutorial, that R provides such standard errors automatically. Howefver, the power of the boostrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software..

In essence bootstrapping repeatedly draws independent samples from our data set to create bootstrap data sets. This sample is performed with replacement, which means that the same observation can be sampled more than once. The figure below from the ISLR1 book depicts the bootsrap approach on a small data set (`n = 3`).

```{r}
knitr::include_graphics("image/bootstrap.png")
```

Each bootstrap data set ($Z^{*1}, Z^{*2}, Z^{*3}$) contains n observations, sampled with replacement from the original data set. Each bootstrap is used to compute the estimated statistic we are interested in ($\hat{\alpha}^*$). We can then use all the bootstrapped data sets to compute the standard error of $\hat{\alpha}^1,\hat{\alpha}^2, ..., \hat{\alpha}^B$ desired statistic as

$$
SE_B(\hat{\alpha}) = \sqrt{\frac{1}{B-1}\Sigma_{r=1}^B(\hat{\alpha}^{*r}-
\frac{1}{B}\Sigma_{r7=1}^B \hat{\alpha}^{*r'})}...(4)
$$

Thus, $SE_B(\hat{\alpha})$ serves as an estimate of the standard error of $\hat{\alpha}$ estimated from the original data set. Let's look at how we can implement this in R on a couple of simple examples:

#### Example 1: Estimating the accuracy of a single statistic

Performing a bootstrap analysis in R entails two steps:

1. Create a function that computes the statistic of interest
2. Use the `boot` function from the `boot` package to perform the bootstrapping

In this example, we will use the `ISLR::Portfolio` data set. This data set contains the return for two investment assets (`X` and `Y`)  Here, our goal is going to be minimizing the risk of investing a fixed sum of money in each asset. Mathematically, we can achieve this by minimizing the variance of our investment using the statistic

$$
\hat{\alpha} = \frac{\hat{\sigma_Y^2}-\hat{\sigma}_{XY}}
{\hat{\sigma_X}^2+\hat{\sigma_Y}^2-2\hat{\sigma_{XY}}} 
...(5)
$$


Thus, we need to create a function that will compute this test statistic:

```{r}
statistic <- function(data, index) {
  x <- data$X[index]
  y <- data$Y[index]
  (var(y) - cov(x, y)) / (var(x) + var(y) - 2* cov(x, y))
}
```

Now we compute $\hat{\alpha}$ for a specified subset of our portfolio data:

```{r}
portfolio <- ISLR::Portfolio

# compute our statistic for all 100 observations
statistic(portfolio, 1:100)
```

Next, we can use `sample` to randomly select 100 observations from the range 1 to 100, with replacement. This is equivalent to constructing a new bootstrap data set and recomputing $\hat{\alpha}$ based on the new data set.

```{r}
statistic(portfolio, sample(100,100, replace=TRUE))
```

If you re-ran this function several times, you will see that you are getting a different output each time. What we want to do is run this many times, record our output each time, and then compute a valid standard error of all the outputs. to do this, we can `boot` and supply it our original data, the function that computes the test statistic, and the number of bootstrap replicates(`R`).

```{r}
set.seed(123)
boot(portfolio, statistic, R = 1000)
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = portfolio, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.5758321 0.002396754  0.08752118
```

The final output shows that using the oroginal data, $\alpha = 0.5758$, and it also provides the bootstrap estimate of our standard error $SE(\hat{\alpha})=0.0875$. Once we generate the bootstrap estimates we can also view the confidence intervanls with `boot.ci` and plot our results.

```{r}
set.seed(123)
result <- boot(portfolio, statistic, R = 1000)

boot.ci(result, type = "basic")
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 1000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = result, type = "basic")
## 
## Intervals : 
## Level      Basic         
## 95%   ( 0.3958,  0.7376 )  
## Calculations and Intervals on Original Scale
plot(result)
```

#### Example 2: Estimating the accuracy of a linear regression model

We can use this same concept to assess the variability of the coefficient estimates and predictions from a statistical learning method such as linear regression. For instance, here we will assess the variability of the estimates for $\beta_0$ and $\beta_1$, the intercept and slope terms for the linear regression model that uses `horsepower` to predict `mpg` in our `auto` data set.

First, we create the function to compute the statistic of interest. We can apply this to our entire data set to get the baseline coefficients.

```{r}
statistic <- function(data, index){
  lm.fit <- lm(mpg ~ horsepower, data = data, subset = index)
  coef(lm.fit)
}
```

Now we can inject this into the `boot` function to compute the bootstrapped standard error estimate:

```{r}
set.seed(123)
boot(auto, statistic, 1000)
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = auto, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original        bias    std. error
## t1* 39.9358610  0.0295956008 0.863541674
## t2* -0.1578447 -0.0002940364 0.007598619
```

This indicates that the boostrap estimate for $SE(\beta_0)$ is 0.86, and that the bootstrap estimate for $SE(\beta_1)$ is 0.0076. If we compute these to the standard error provided by the `summary` function we see a difference. 

```{r}
summary(lm(mpg ~ horsepower, data = auto))
```

This difference suggests the standard errors provided by `summary` may be biased. That is, certain assumptions may be violated which is causing the standard errors in the non-bootstrap approach to be different than those in the bootstrap approach.

If you remember from earlier in the tutorial we found that a quadratic fit appeared to be the most approapriate for the relationship between `mpg` and `horsepower`. Lets adjust our code to capture this fit and see if we notice a difference with our outputs.

```{r}
quad.statistic <- function(data, index){
  lm.fit <- lm(mpg ~ poly(horsepower, 2), data = data, subset = index)
  coef(lm.fit)
}

set.seed(1)
boot(auto, quad.statistic, 1000)

## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = auto, statistic = quad.statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original      bias    std. error
## t1*   23.44592 0.003943212   0.2255528
## t2* -120.13774 0.117312678   3.7008952
## t3*   44.08953 0.047449584   4.3294215
```

Now if we compare the standard errors between the bootstrap approach and the non-bootstrap approach we see the standard errors align more closely. This better correspondence between the bootstrap estimates and the standard estimates suggests a better model fit. 

Thus, bootstrapping provides an additional method for assessing the adequacy of our model's fit.

```{r}
summary(lm(mpg ~ poly(horsepower, 2), data = auto))
## 
## Call:
## lm(formula = mpg ~ poly(horsepower, 2), data = auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.7135  -2.5943  -0.0859   2.2868  15.8961 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(>|t|)    
## (Intercept)            23.4459     0.2209  106.13   <2e-16 ***
## poly(horsepower, 2)1 -120.1377     4.3739  -27.47   <2e-16 ***
## poly(horsepower, 2)2   44.0895     4.3739   10.08   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.374 on 389 degrees of freedom
## Multiple R-squared:  0.6876,	Adjusted R-squared:  0.686 
## F-statistic:   428 on 2 and 389 DF,  p-value: < 2.2e-16
```

### Additional Resources
This will get you started with resampling methods; however, understand that there are many approaches for resampling and even more options within R to implement these approaches. The following resources will help you learn more:

- An Introduction to Statistical Learning
- Introduction to Statistics Through Resampling Methods and R
- Applied Predictive Modeling
- Elements of Statistical Learning

