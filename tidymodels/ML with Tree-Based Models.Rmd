---
title: "ML with Tree-Based Models"
author: "Koji Mizumura"
date: "2020-04-30 - `r Sys.Date()`"
output: 
  rmdformats::readthedown:
    number_sections: yes
    fig_height: 10
    fig_width: 14
    highlight: kate
    toc_depth: 3
#    css: style.css
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  # fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```

# Classification trees
## Overview

Tree-based models 
- Interpretability + Ease-of-use + Accuracy
- Make decisions + Numeric predictions

## Build a clasification tree

Let's get started and build our first classification tree. _A classification tree_ is a decision tree that performs a classification (vs regression) task.

You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the [German Credit Dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). The response variable, called "default", indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes).

You will use the rpart package to fit the decision tree and the rpart.plot package to visualize the tree.

```{r}
library(rpart)
library(rpart.plot)

# Look at the data
credit <- read.csv("credit.csv")
creditub <- credit
str(creditsub)

# Create the model
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")

# Display the results
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)

```

## Overview of the modeling process

```{r}
# Total number of rows in the restaurant data frame
n <- nrow(creditsub)

# Number of rows for the training set
n_train <- round(0.80*n)

# set a random seed for reproducibility
set.seed(123)

# Create a vector of indices which is an 80% random sample 
train_indices <- sample(1:n, n_train)

# Subset the data frame to training indices only
restaurant_train <- creditsub[train_indices, ]

# Exclude the training indices to create the test set
restaurant_test <- creditsub[-train_indices, ]
```

To train a classification tree in R, you will must specify the formula, the data and the method.

```{r}
# train the model to predict the binary response 
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")

```

## Train/test split

For this exercise, you will randomly split the German credit dataset into two pieces: a training set (80%) called `credit_train`

```{r}
# Total number of rows in the credit data frame
n <- nrow(credit)

# Number of rows for the training set (80% of the dataset)
n_train <- round(.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]  
```

## Train a classification tree model

In this exercise, you will train a model on the newly created training set and print the model object to get a sense of the results.

```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default~., 
                      data = credit_train, 
                      method = "class")

# Look at the model output                      
print(credit_model)
```

## Evaluate model performance 

$$
Accuracy = /frac {n of correct predictions} {n of total data points}
$$

```{r}
library(caret)
# calculate the confusion matrix for the test set
class_pred <- predict(object = credit_model,
                      newdata = credit_test,
                      type = "class")

caret::confusionMatrix(data = class_pred,
                       reference = credit_test$default)
```

## Compute confusion matrix

As discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once.

The c`onfusionMatrix()` function from the caret package prints both the confusion matrix and a number of other useful classification metrics such as "Accuracy" (fraction of correctly classified instances).

The caret package has been loaded for you.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class")  
                            
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = credit_test$default)  
```

## Splitting criterion in trees 

A classificatino tree uses a split condition to predict class labels based on one or more 
input variables. The classification process starts from the root node of the tree and each node, the process will check whether the input value should recursively continue to the right or left sub-branch according to the split condition.

The process stops when meeting any leaf or terminal nodes. The idea behind classification trees is to split the data into subsets where each subset belongs to only one class. This is accomplished by dividing the input space into pure regions, that is - regions with samples from only one class. 

With real data, completely pure regions may not be possible, so the decision tree will do the best it can to create regions that are as pure as possible. 

Boundaries separating these regions are called decsion boundaries, and the decision tree model makes classification decisions based on these decision boundaries. The goal is to partition data at a node into subsets that are as pure as possible.

Theefore, we need a way to measure the purity of a split, in order to compare different ways to partition a set of data. It works out better mathematically if we measure the impurity rather than the purity. Thus, the impurity measure of a node specifies how mixed the resulting subsets are. 

Since we want the resulting subsets to have homogeneous class labels, not mixed class labels, we want the split that minimizes the impurity measure. 

- Gini index: higher value equals less pure
- Misclassification rate

## Compare models with a different splitting criterion

Train two models that use a different splitting criterion and use the validation set to choose a "best" model from this group. To do this you'll use the parms argument of the rpart() function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter split to control the splitting criterion.

```{r}
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "gini"))

# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1, 
             newdata = credit_test,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
             newdata = credit_test,
             type = "class")

# Compare classification error
ce(actual = credit_test$default, 
   predicted = pred1)
ce(actual = credit_test$default, 
   predicted = pred2)  
```

## Introduction to regression trees

## Split the data

Split the data
These examples will use a subset of the Student Performance Dataset from UCI ML Dataset Repository.

The goal of this exercise is to predict a student's final Mathematics grade based on the following variables: sex, age, address, studytime (weekly study time), schoolsup (extra educational support), famsup (family educational support), paid (extra paid classes within the course subject) and absences.

The response is final_grade (numeric: from 0 to 20, output target).

After initial exploration, split the data into training, validation, and test sets. In this chapter, we will introduce the idea of a validation set, which can be used to select a "best" model from a set of competing models.

In Chapter 1, we demonstrated a simple way to split the data into two pieces using the sample() function. In this exercise, we will take a slightly different approach to splitting the data that allows us to split the data into more than two parts (here, we want three: train, validation, test). We still use the sample() function, but instead of sampling the indices themselves, we will assign each row to either the training, validation or test sets according to a probability distribution.

The dataset grade is already in your workspace.

```{r}
# Look at the data
str(grade)

# Set seed and create assignment
set.seed(1)
assignment <- sample(1:3, size = nrow(grade), prob = c(.7, .15, .15), replace = TRUE)

# Create a train, validation and tests from the original data frame 
grade_train <- grade[assignment == 1, ]    # subset grade to training indices only
grade_valid <- grade[assignment == 2, ]  # subset grade to validation indices only
grade_test <- grade[assignment == 3, ]   # subset grade to test indices only
```

## Train a regression tree model 

In this exercise, we will use the `grade_train` dataset to fit a regression tree using `rpart()` and visualize it using `rpart.plot()`. A regression tree plot looks identical to a classification tree plot, with the exception that there will be numeric values in the leaf nodes instead of predicted classes.

This is very similar to what we did previously in Chapter 1. When fitting a classification tree, we use method = "class", however, when fitting a regression tree, we need to set method = "anova". By default, the `rpart()` function will make an intelligent guess as to what the method value should be based on the data type of your response column, but it's recommened that you explictly set the method for reproducibility reasons (since the auto-guesser may change in the future).

The `grade_train` training set is loaded into the workspace.

```{r}
# Train the model
grade_model <- rpart(formula = final_grade ~ ., 
                     data = grade_train, 
                     method = "anova")

# Look at the model output                      
print(grade_model)

# Plot the tree model
rpart.plot(x = grade_model, yesno = 2, type = 0, extra = 0)
```

## Performance metrics for regression

1) Mean Absolute Error (MAE)
$$
MAE = \frac{1}{n} \Sigma|actual - predicted|
$$

2) Root Mean Square Error (RMSE)
$$
RMSE = \sqrt{\frac{1}{n}\Sigma(actual - predicted)^2}
$$

Both MAE and RMSE express average model prediction erro in units of the variable interest. Both metrics are indifferent to the direction of errors and lower values are better. The key difference between the two is that RMSE punishes large errors more harshly than MAE. 

This means the RMSE should be more useful when large errors are particularly undesirable. 

```{r eval=FALSE}
pred <- predict(object = model,
                newdata = testset)

library(Metrics)
# Compute the RMSE
rmse(
  actual = test$response, 
  predicted = pred
)
```

## Evaluate a regression tree model 

Predict the final grade for all students in the test set. The grade is on a 0-20 scale. Evaluate the model based on test set RMSE (Root Mean Squared Error). RMSE tells us approximately how far away our predictions are from the true values.

```{r}
# Generate predictions on a test set
pred <- predict(object = grade_model,   # model object 
                newdata = grade_test)  # test dataset

# Compute the RMSE
rmse(actual = grade_test$final_grade, 
     predicted = pred)
```

## What are the hyperparameters for a decision tree? 

There are sevral knobs that we can turn that affect how the tree is grown, and in many cases, tuning these knobs - or model hyperparameters will result in a performing model. 

The rpart function has a special control parameter, called `control`. 

```{r}
?rpart.control
```

- minsplit: minimum number of data points required to attempt a split (default 20)
- cp: complexity parameter (default 0.1)
- maxdepth: depth of a decision tree, which limits the maximum number of nodes between a leaf node and the root node (default 30)

Complexity parameter serves as a penalty term to control tree size, and is always monotonic with the number of splits. The smaller value of CP, the more complex will be the tree (the greater the number of spilits)

The rpart function computes the 10-fold cross-validated error of the model over various values for CP and stores the results in a table inside the model.

Across the different values of CP using the `plotcp()` function, we can see right away what the optimal value for CP is. To retrieve the optimal value, we simply find the row 
for which xerror is minimized and grab the corresponding CP value. 

Once you have the optimal value, you can tune (or trim) the model using the `prune()` function. The `prune()` function returns the optimized model. 

```{r}
plotcp(grade_model)

print(model$cptable)

# Prune the model to optimized cp value
model_opt <- prune(tree = model,
                   cp = cp_opt)
```

## Tuning the model 
Tune the model using the `prune()` function by finding the best CP value (CP stands for "Complexity Parameter"). 

```{r}
# Plot the "CP Table"
plotcp(grade_model)

# Print the "CP Table"
print(grade_model$cptable)

# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(grade_model$cptable[, "xerror"])
cp_opt <- grade_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
grade_model_opt <- prune(tree = grade_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)
```

## Grid search for model selection

Training a default model is good, but training a sequence of models with various hyprparameter settings with the goal of finding the best one is a typical task in any ML pipeline. 

This process is called "model selection" or "hyperparameter selection" because the end goal is to select the best model from the set to use in some ML application.  One of the most common technuques for performing model selection is called grid search. 

### Grid sarch

- What is a model hyperparameter?
- What is a grid? 
- What is the goal of a grid search? 
- How is the best model chosen? 

Model hyperparameters are the knobs that you tweak to get slightly different models. For example, max depth is the maximum depth allowed in a decision tree. 

The grid refers to the set of hyperparameter combinations that you will iterate over durign the grid search. 

The goal of the grid search is to evaluate a large number of parameter settings, by training models on each combination of hyperparameter values, to find the combination that produces the best model.

The first step is to choose an appropriate performance metric for your task - examples are classification error, AUC or root mean squared error. The performance of each model in the grid is computed by cross-validation or evaluation on a hold-out validation set. 

The model with the best performacne, as evaluated by this metric is selected as the winner. For example, if the hyperparameter of a decision tree is the minimum number of observations that must exist in a node in order for a split to be attempted, then the grid could be 5, 10, 20 30 etc. 

Some guesswork is necessary to specify the minimum and maximum values. So sometimes, people run a small grid, see if the optimum lies at either endpoint, and then expand the grid in that direction. 

Manual grid search
1) Define a list of possible values for minsplit and maxdepth
2) A data frame is created, containing all combinations using the expand.grid

```{r eval=FALE}
# Establish a list of possible options

splits <- seq(1,30,5)
depths <- seq(5,40,10)

# create a data frame containing all combinations
hyper_grid <- 
  expand.grid(
    minsplit = split,
    maxdepth = depths
  )

```

## Generate a grid of hyperparameter values

Use `expand.grid()` to generate a grid of maxdepth and minsplit values.

```{r}
# Establish a list of possible values for minsplit and maxdepth
minsplit <- seq(1, 4, 1)
maxdepth <- seq(1, 6, 1)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(minsplit = minsplit, maxdepth = maxdepth)

# Check out the grid
head(hyper_grid)

# Print the number of grid combinations
nrow(hyper_grid)
```

## Generate a grid of models

In this exercise, we will write a simple loop to train a "grid" of models and store the models in a list called `grade_models`. R users who are familiar with the `apply` functions in R could think about how this loop could be easily converted into a function applied to a list as an extra-credit thought experiment.

```{r}
# Number of potential models in the grid
num_models <- nrow(hyper_grid)

# Create an empty list to store models
grade_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    # Get minsplit, maxdepth values at row i
    minsplit <- hyper_grid$minsplit[i]
    maxdepth <- hyper_grid$maxdepth[i]

    # Train a model and store in the list
    grade_models[[i]] <- rpart(formula = final_grade ~ ., 
                               data = grade_train, 
                               method = "anova",
                               minsplit = minsplit,
                               maxdepth = maxdepth)
}
```

## Evaluate the grid

Earlier in the chapter we split the dataset into three parts: training, validation and test.

A dataset that is not used in training is sometimes referred to as a "holdout" set. A holdout set is used to estimate model performance and although both validation and test sets are considered to be holdout data, there is a key difference:

- Just like a test set, a validation set is used to evaluate the performance of a model. The difference is that a validation set is specifically used to compare the performance of a group of models with the goal of choosing a "best model" from the group. All the models in a group are evaluated on the same validation set and the model with the best performance is considered to be the winner.
- Once you have the best model, a final estimate of performance is computed on the test set.
- A test set should only ever be used to estimate model performance and should not be used in model selection. Typically if you use a test set more than once, you are probably doing something wrong.

```{r}
# Number of potential models in the grid
num_models <- length(grade_models)

# Create an empty vector to store RMSE values
rmse_values <- c()

# Write a loop over the models to compute validation RMSE
for (i in 1:num_models) {

    # Retrieve the i^th model from the list
    model <- grade_models[[i]]
    
    # Generate predictions on grade_valid 
    pred <- predict(object = model,
                    newdata = grade_valid)
    
    # Compute validation RMSE and add to the 
    rmse_values[i] <- rmse(actual = grade_valid$final_grade, 
                           predicted = pred)
}

# Identify the model with smallest validation set RMSE
best_model <- grade_models[[which.min(rmse_values)]]

# Print the model paramters of the best model
best_model$control

# Compute test set RMSE on best_model
pred <- predict(object = best_model,
                newdata = grade_test)
rmse(actual = grade_test$final_grade, 
     predicted = pred)
```

# Bagged Trees
## Introduction to bagged trees 

One of the main drawbacks of a decision tree is their high variance. Often a small change in the data can result in va very different series of splits, which can also make model intepretation somewhat precautious. 

Bagging, and in particular, bagged trees, averages many trees to reduce this variance. Combining several moels into one is what is called an ensemble model and averaging is one of the easiest ways to create an ensemble from a collection of models. 

In addition to reducing variance, it can also help avoid overfitting. 

Bagging is an ensemble method and the term "bagging" is shorthand for bootstrap aggregation. Bagging uses bootstrap sampling and agrregates the individual models by averaging. 

Bootstrap means sampling rows at random from the training dataset, with replacement. When we draw samples with replacement, that means it is possible that you will draw a single training example more than once. 

This results in a modified version of the training set where some rows are represented multiple times and some rows are absent. This let's you generate new data that is similar to the data you started with. 

By doing this, we can fit many different, but similar models. 

- Step 1: You draw B samples with replacement from the original training set where B is a number less than or equal to the N, number of total samples in the training set.

```{r}
knitr::include_graphics("Bootstrapping_step1.png")
```

- Step 2: Train a decision tree on the newly created bootstrapped sample. 

```{r}
knitr::include_graphics("Bootstrapping_step2.png")
```

Repeate step 1 through 2 for as many times as you like - that could be 20 times, 100 times or 1000. Typically, the more trees, the better the model. 

Suppose we use 1000 trees for the model construction. In this case, each model has different featurs, and to generate a prediction using a bagged tree model, you need to generate predictions from each of the 1000 trees and then simply average the prediction together to get a final prediction.

```{r}
knitr::include_graphics("Bootstrapping_step3.png")
```

The bagged, or ensemble prediction is the average prediction across the bootstrapped trees. Bagging can dramatically reduce the variance of unstable models such as trees, leading to improved prediction.

This means averaging reduces variance and leaves bias unchanged. 

```{r eval=FALSE}
library(ipred)
bagging(formula = response ~., data = dat)
```

## Train a bagged tree model

Let's start by training a bagged tree model. You'll be using the `bagging()` function from the ipred package. The number of bagged trees can be specified using the `nbagg` parameter, but here we will use the default (25).

If we want to estimate the model's accuracy using the "out-of-bag" (OOB) samples, we can set the the `coob` parameter to `TRUE`. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the `bagging()` function).

```{r}
# Bagging is a randomized model, so let's set a seed (123) for reproducibility
set.seed(123)

# Train a bagged model
credit_model <- bagging(formula = default ~ ., 
                        data = credit_train,
                        coob = TRUE)

# Print the model
print(credit_model)
```

## Evaluating the bagged tree performance 

We have to pass the model object, the test dataset and what type you want your prediction to be. In this example, we set the argument type to be class since we want the function to returns a vector of class predictions. 

If you want to take a peak in the predictions, you can print the class prediction and you will see that indeed classification labels were returned. After making predictions, it is time to evaluate the model performance. We can use the `confusionMatrix()` from the `caret` package. 

It's always good to take a look at the output using the `print()` function.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,    
                            newdata = credit_test,  
                            type = "class")  # return classification labels

# Print the predicted classes
print(class_prediction)
credit_test
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = credit_test$default)  
```

## PRedict on a test set and compute AUC

In binary classification problems, we can predict numeric values instead of class labels. In fact, class labels are created only after you use the model to predict a raw, numeric, predicted value for a test point.

The predicted label is generated by applying a threshold to the predicted value, such that all tests points with predicted value greater than that threshold get a predicted label of "1" and, points below that threshold get a predicted label of "0".

In this exercise, generate predicted values (rather than class labels) on the test set and evaluate performance based on [AUC (Area Under the ROC Curve)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve). The AUC is a common metric for evaluating the discriminatory ability of a binary classification model.

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_model,
                newdata = credit_test,
                type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])      

library(tidymodels)

# yardstick::roc_curve()
bind_cols(
  actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"]
) %>%
  mutate(actual = as.factor(actual)) %>% 
  roc_curve(truth = actual, predicted)

# yardstick::roc_auc()
bind_cols(
  actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"]
) %>%
  mutate(actual = as.factor(actual)) %>% 
  roc_auc(truth = actual, predicted)

```

## Using caret for cross-validating models

Although using a single training and test set is a quick way to get an estimate of your model's performance, it is susceptible to variations in the data. Variability in your data can stem both from the size of your training set - smaller datasets have more variability - and also from natural variability in the true population that you are sampling from. 

### K-fold cross validation 

1) Partition the rows of the dataset into K subset of equal sizes. 

For example, dataset size = 200 rows, k = 10 (number of corss validation folds)
In each iteration, you pick one of the k subsets as your test set and the remaining k minus 1 subsets are aggregated and used as the training set. 

2) Train the model on the training set and evaluate the performance
Then you train your machine learning algorthim on that training set, and evaluate the model's performance on the test set. You can use any metric to evaluate the performance, so as an example, let's say you compute the test set AUC. 

3) CV estimate of AUC
We average those 10 estimates together to get what is called the corss-validated estimate of AUC.
- 10 estimates of test set AUC
- the average is the cross-validated estimate of AUC

Since we end up training k models instead of one, its obvious that CV takes k times as long to evaluate your models this way. It depends on what's more valuable to your use-case: time or accuracy of your model performance estimates. 

caret:: `train()` and `trainControl()`

```{r eval=FALSE}
# Specify the training cofiguration
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary()
)

# For reproducibility set a seed
set.seed(123)
credit_model <- train(
  default ~ ., 
  data = credit_train,
  method = "treebag",
  metric = "ROC",
  trControl = ctrl
)
```

## Cross-validate a bagged tree model in caret

Use `caret::train()` with the `treebag` method to train a model and evaluate the model using cross-validated AUC. The __caret__ package allows the user to easily cross-validate any model across any relevant performance metric. In this case, we will use 5-fold cross validation and evaluate cross-validated AUC (Area Under the ROC Curve).

```{r}
# Specify the training configuration
ctrl <- trainControl(method = "cv",     # Cross-validation
                     number = 5,      # 5 folds
                     classProbs = TRUE,                  # For AUC
                     summaryFunction = twoClassSummary)  # For AUC

# Cross validate the credit model using "treebag" method; 
# Track AUC (Area under the ROC curve)
set.seed(1)  # for reproducibility
credit_caret_model <- train(default ~ .,
                            data = credit_train, 
                            method = "treebag",
                            metric = "ROC",
                            trControl = ctrl)

# Look at the model object
print(credit_caret_model)

# Inspect the contents of the model list 
names(credit_caret_model)

# Print the CV AUC
credit_caret_model$results[,"ROC"]
```

## Generate predictions from the caret model

Generate predictions on a test set for the `caret` model.

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_caret_model, 
                newdata = credit_test,
                type = "prob")

# Compute the AUC (`actual` must be a binary (or 1/0 numeric) vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
                    predicted = pred[,"yes"])
```

## Compare test set performance to CV performance

In this excercise, you will print test set AUC estimates that you computed in previous exercises. These two methods use the same code underneath, so the estimates should be very similar.

- The `credit_ipred_model_test_auc` object stores the test set AUC from the model trained using the `ipred::bagging()` function.
- The `credit_caret_model_test_auc` object stores the test set AUC from the model trained using the `caret::train()` function with `method = "treebag"`.

Lastly, we will print the 5-fold cross-validated estimate of AUC that is stored within the `credit_caret_model` object. This number will be a more accurate estimate of the true model performance since we have averaged the performance over five models instead of just one.

On small datasets like this one, the difference between test set model performance estimates and cross-validated model performance estimates will tend to be more pronounced. When using small data, it's recommended to use cross-validated estimates of performance because they are more stable.

```{r}
# Print ipred::bagging test set AUC estimate
print(credit_ipred_model_test_auc)

# Print caret "treebag" test set AUC estimate
print(credit_caret_model_test_auc)
                
# Compare to caret 5-fold cross-validated AUC
credit_caret_model$results[, "ROC"]
```

# Random forest

In this chapter, you will learn about the Random Forest algorithm, another tree-based ensemble method. Random Forest is a modified version of bagged trees with better performance. Here you'll learn how to train, tune and evaluate Random Forest models in R.

## Introduction to Random Forest

In the last chapter, we introduced bagged trees, which are a big improvement over a single decision tree. 

- Better pefromance
- Sample subset of the features 
- Improved version of bagging
- Reduced correlation between the sampled trees

The basic idea behind random forests is identical to bagging, - both are ensembles of trees trained on bootstrapped samples of the training data. 

However, in the random forest algorithm, there is slight tweak to the way the decision trees are built that leads to better performance. 

They key difference is that when we train the trees that make up the ensemble. We add a bit of extra randomness to the model -- hence the name, Random Forests.

At each split in the tree, rather than considering all features, or imput variables, for the split. We sample a subset of hese features and consider only these few variables as a candidates for the split. 

The technique of sampling variables from the input or 
feature space is also feature bagging, or the random subspace method.

We have fewer variables to choose from, which means there is less information available to the model - so how can that lead to better performance? Adding this extra randomness leads to a collection of trees that are decorrelated (or more different) from one another.

RF improve upon bagging by reducing the correlation between the sampled trees. The number of trees is specified through the `ntrees` argument and defaults to 500, which is usually a good place to start. 

We can always add more trees to improve the performance of the ensemble - more trees almost always means better performance in a Random Forest.

## Train a random forest model 

Here you will use the `randomForest()` function from the randomForest package to train a Random Forest classifier to predict loan default.

```{r}
# Train a Random Forest
library(randomForest)
set.seed(1)  # for reproducibility
credit_model <- randomForest(formula = default ~ ., 
                             data = credit_train)
                             
# Print the model output                             
print(credit_model)
```

## Understanding random forest model output

You will see what's called the "out-of-bag" or OOB estimate of the error rate for the model. This is the error rate computed across the samples that were not selected into the bootstrapped training sets.

Since this is the classification problem, we also see a confusion matrix based on the out-of-bag samples. Since each tree in the Random Forest is trained on a bootstrapped sample of the original training set, this means that some of the samples will be duplicated in the training set and some will be absent. 

```{r}
# Grab OOB error matrix & take a look
err <- credit_model$err.rate
head(err)
```

The i-th row reports the OOB error rate for all trees up to and including the i-th tree. The first column shows the error across all the classes and then there are additional columns for per-class OOB error. 

If you get the last row, you will have the final out-of-bag error. This is the same value that is printed in the model output. When plotting the OOB error rates, the OOB error rates asa function of the number of trees in the forest. 

This plot helps you decide how many trees are necessary to include in the ensemble. As you can see, when using less than 50 trees the OOB error remains quite high, but drops and starts to flattens out between 300 and 400 trees. 

After a certain point, including more trees in your model will not get you any additional performance. There is nothing wrong with using "too many" trees, however, computing the predictions for each tree does take time, so you don't want to include more trees than you actually need.

```{r}
# look at final OOB error rate
oob_err <- err[nrow(err), "OOB"]
print(oob_err)

err %>%
  as.tibble() %>% 
  mutate(trees = row_number()) %>% 
  ggplot(aes(x = trees, y = OOB))+
  geom_line()
```

## Evaluate out-of-bag error

Here you will plot the OOB error as a function of the number of trees trained, and extract the final OOB error of the Random Forest model from the trained model object. 

```{r}
# Grab OOB error matrix & take a look
err <- credit_model$err.rate
head(err)

# Look at final OOB error rate (last row in err matrix)
oob_err <- err[nrow(err), "OOB"]
print(oob_err)

# Plot the model trained in the previous exercise
plot(credit_model)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))
```

## Evaluate model performance on a test set

Use the `caret::confusionMatrix()` function to compute test set accuracy and generate a confusion matrix. Compare the test set accuracy to the OOB accuracy.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,   # model object 
                            newdata = credit_test,  # test dataset
                            type = "class") # return classification labels
                            
# Calculate the confusion matrix for the test set
pacman::p_load(caret, RandomForest)
cm <- confusionMatrix(data = class_prediction,       # predicted classes,
                      reference = credit_test$default)  # actual classes
print(cm)

# Compare test set accuracy to OOB accuracy
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
```

## OOB error vs. test set error

AS a feature of the Random Forest algorithm, you are provided with a built-in validation set without any extra work on your part, and training data is not required to be sacrificed for validation.

In addition the Random Forest has a OOB erro computation built-in, so you don't need to write any extra code to evaluate your model performance. 

The RandomForest package does not keep track of which observations were part of the ot-of-bag sample in each tree, so there is no way to calculate these metrics after-the-fact.

If you are comparing the performance of your Random Forest to another type model such as a GLM or SVM, then you would want to score each of these models on the same validation set to compare performance. 

So although the out-of-bag error rate can be used to compare several random forests, you won't be able to perform a model comparison to any other type of model using the out-of-bag estimate. 

## Evaluate test set AUC

In Chapter 3, we learned about the AUC metric for evaluating binary classification models. In this exercise, you will compute test set AUC for the Random Forest model.

```{r}
# Generate predictions on the test set
pred <- predict(object = credit_model,
            newdata = credit_test,
            type = "prob")

# `pred` is a matrix
class(pred)
                
# Look at the pred format
head(pred)
                
# Compute the AUC (`actual` must be a binary 1/0 numeric vector)
auc(actual = ifelse(credit_test$default == "yes", 1, 0), 
    predicted = pred[,"yes"])                    
```

## Tuning a Random Forest model 

Like any ML algorithm, the key to getting a good performance is to tune the model hyperparameters. Random Forest is one of the easiest algorithms to tune because there are only a handful of hyperparameters that have a big impact on the performance of the model.

If you contrast tuning a random forest model to support vector or deep neural network, its a walk in the park. On the other hand, you will get performance by using the random forest, with little tuning and no expert knowledge required. 
Random forest hyperparameters are:

- `ntree`: number of trees 
- `mtry`: number of variables randomy sampled as candidates at each split
- `sampsize`: number of samples to train on
- `nodesize`: minimum size (number of samples) of the terminal nodes
- `maxnodes`: maximum number of terminal nodes 
 
`mtry` is an important parameter that we will discuss in further detail in the next slide. 

`nodesize` and `maxnodes` are both parameers that control the complexity of the tree. Whe `nodesize` is small, it allows deeper, more complex trees to be grown. `Maxnodes` is another way to limit tree growth and avoid overfitting. 

Keep in mind that each random forest implementation can use different names for these same parameters. These are just the names that the random forest R package uses. 

At each split in a tree, we consider some number of predictor variables - from this group, we choose the variable that splits the data in the most pure manner. `Mtry` is te number of predictor variables that we sample at each split. 

The RandomForest package has a built-in function for tuning the `mtry` parameter called `tuneRF()`, which tunes the model based on OOB error. Rather thsan iterating over a set list of mtry values, the tuneRF function will start with default value of `mtry` and increase the value by an amount specified in the StepFactor argument. 

The search will be stopped when the OOB error stops decreasing by a specified amount. Keep in mind that the specialized tuneRF() function is just one way to tune a Random Forest. A manual grid search will give us more control over the search space, allows us to evaluate the Random Forest using metrics other than OOB error and allow us to include other model hyperparamters in the grid search, such as nodesize and sampsize. 

```{r eval=FALSE}
# Execute the tuning process
set.seed(1)
res <- tuneRF(
  x = train_predictor_df,
  y = train_response_vector, 
  mtreeTry = 500
)
```


## Tuning a Random Forest via `mtry`

In this exercise, you will use the randomForest::tuneRF() to tune mtry (by training several models). This function is a specific utility to tune the mtry parameter based on OOB error, which is helpful when you want a quick & easy way to tune your model. A more generic way of tuning Random Forest parameters will be presented in the following exercise.

```{r}
# Execute the tuning process
set.seed(1)       
library(randomForest)
res <- tuneRF(x = subset(credit_train, select = -default),
              y = credit_train$default,
              ntreeTry = 500)
               
# Look at results
print(res)

# Find the mtry value that minimizes OOB Error
mtry_opt <- res[,"mtry"][which.min(res[,"OOBError"])]
print(mtry_opt)

# If you just want to return the best RF model (rather than results)
# you can set `doBest = TRUE` in `tuneRF()` to return the best RF model
# instead of a set performance matrix.
```

## Tuning a Random Forest via tree depth

In Chapter 2, we created a manual grid of hyperparameters using the `expand.grid()` function and wrote code that trained and evaluated the models of the grid in a loop. In this exercise, you will create a grid of `mtry`, `nodesize` and `sampsize` values. In this example, we will identify the "best model" based on OOB error. The best model is defined as the model from our grid which minimizes OOB error.

Keep in mind that there are other ways to select a best model from a grid, such as choosing the best model based on validation AUC. However, for this exercise, we will use the built-in OOB error calculations instead of using a separate validation set.

```{r}
# Establish a list of possible values for mtry, nodesize and sampsize
mtry <- seq(4, ncol(credit_train) * 0.8, 2)
nodesize <- seq(3, 8, 2)
sampsize <- nrow(credit_train) * c(0.7, 0.8)

# Create a data frame containing all combinations 
hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)

# Create an empty vector to store OOB error values
oob_err <- c()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:nrow(hyper_grid)) {

    # Train a Random Forest model
    model <- randomForest(formula = default ~ ., 
                          data = credit_train,
                          mtry = hyper_grid$mtry[i],
                          nodesize = hyper_grid$nodesize[i],
                          sampsize = hyper_grid$sampsize[i])
                          
    # Store OOB error for the model                      
    oob_err[i] <- model$err.rate[nrow(model$err.rate), "OOB"]
}

# Identify optimal set of hyperparmeters based on OOB error
opt_i <- which.min(oob_err)
print(hyper_grid[opt_i,])
```

# Boosted trees 
## Introduction to boosting 

## Bagged trees vs Boosted trees 


## Train a GBM model





