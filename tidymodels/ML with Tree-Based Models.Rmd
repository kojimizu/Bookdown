---
title: "ML with Tree-Based Models"
author: "Koji Mizumura"
date: "2020-04-30 - `r Sys.Date()`"
output: 
  rmdformats::readthedown:
    number_sections: yes
    fig_height: 10
    fig_width: 14
    highlight: kate
    toc_depth: 3
#    css: style.css
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  # fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```

# Classification trees
## Overview

Tree-based models 
- Interpretability + Ease-of-use + Accuracy
- Make decisions + Numeric predictions

## Build a clasification tree

Let's get started and build our first classification tree. _A classification tree_ is a decision tree that performs a classification (vs regression) task.

You will train a decision tree model to understand which loan applications are at higher risk of default using a subset of the [German Credit Dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). The response variable, called "default", indicates whether the loan went into a default or not, which means this is a binary classification problem (there are just two classes).

You will use the rpart package to fit the decision tree and the rpart.plot package to visualize the tree.

```{r}
library(rpart)
library(rpart.plot)

# Look at the data
credit <- read.csv("credit.csv")
creditub <- credit
str(creditsub)

# Create the model
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")

# Display the results
rpart.plot(x = credit_model, yesno = 2, type = 0, extra = 0)

```

## Overview of the modeling process

```{r}
# Total number of rows in the restaurant data frame
n <- nrow(creditsub)

# Number of rows for the training set
n_train <- round(0.80*n)

# set a random seed for reproducibility
set.seed(123)

# Create a vector of indices which is an 80% random sample 
train_indices <- sample(1:n, n_train)

# Subset the data frame to training indices only
restaurant_train <- creditsub[train_indices, ]

# Exclude the training indices to create the test set
restaurant_test <- creditsub[-train_indices, ]
```

To train a classification tree in R, you will must specify the formula, the data and the method.

```{r}
# train the model to predict the binary response 
credit_model <- rpart(formula = default ~ ., 
                      data = creditsub, 
                      method = "class")

```

## Train/test split

For this exercise, you will randomly split the German credit dataset into two pieces: a training set (80%) called `credit_train`

```{r}
# Total number of rows in the credit data frame
n <- nrow(credit)

# Number of rows for the training set (80% of the dataset)
n_train <- round(.8 * n) 

# Create a vector of indices which is an 80% random sample
set.seed(123)
train_indices <- sample(1:n, n_train)

# Subset the credit data frame to training indices only
credit_train <- credit[train_indices, ]  
  
# Exclude the training indices to create the test set
credit_test <- credit[-train_indices, ]  
```

## Train a classification tree model

In this exercise, you will train a model on the newly created training set and print the model object to get a sense of the results.

```{r}
# Train the model (to predict 'default')
credit_model <- rpart(formula = default~., 
                      data = credit_train, 
                      method = "class")

# Look at the model output                      
print(credit_model)
```

## Evaluate model performance 

$$
Accuracy = /frac {n of correct predictions} {n of total data points}
$$

```{r}
library(caret)
# calculate the confusion matrix for the test set
class_pred <- predict(object = credit_model,
                      newdata = credit_test,
                      type = "class")

caret::confusionMatrix(data = class_pred,
                       reference = credit_test$default)
```

## Compute confusion matrix

As discussed in the previous video, there are a number of different metrics by which you can measure the performance of a classification model. In this exercise, we will evaluate the performance of the model using test set classification error. A confusion matrix is a convenient way to examine the per-class error rates for all classes at once.

The c`onfusionMatrix()` function from the caret package prints both the confusion matrix and a number of other useful classification metrics such as "Accuracy" (fraction of correctly classified instances).

The caret package has been loaded for you.

```{r}
# Generate predicted classes using the model object
class_prediction <- predict(object = credit_model,  
                        newdata = credit_test,   
                        type = "class")  
                            
# Calculate the confusion matrix for the test set
confusionMatrix(data = class_prediction,       
                reference = credit_test$default)  
```

## Splitting criterion in trees 

A classificatino tree uses a split condition to predict class labels based on one or more 
input variables. The classification process starts from the root node of the tree and each node, the process will check whether the input value should recursively continue to the right or left sub-branch according to the split condition.

The process stops when meeting any leaf or terminal nodes. The idea behind classification trees is to split the data into subsets where each subset belongs to only one class. This is accomplished by dividing the input space into pure regions, that is - regions with samples from only one class. 

With real data, completely pure regions may not be possible, so the decision tree will do the best it can to create regions that are as pure as possible. 

Boundaries separating these regions are called decsion boundaries, and the decision tree model makes classification decisions based on these decision boundaries. The goal is to partition data at a node into subsets that are as pure as possible.

Theefore, we need a way to measure the purity of a split, in order to compare different ways to partition a set of data. It works out better mathematically if we measure the impurity rather than the purity. Thus, the impurity measure of a node specifies how mixed the resulting subsets are. 

Since we want the resulting subsets to have homogeneous class labels, not mixed class labels, we want the split that minimizes the impurity measure. 

- Gini index: higher value equals less pure
- Misclassification rate

## Compare models with a different splitting criterion

Train two models that use a different splitting criterion and use the validation set to choose a "best" model from this group. To do this you'll use the parms argument of the rpart() function. This argument takes a named list that contains values of different parameters you can use to change how the model is trained. Set the parameter split to control the splitting criterion.

```{r}
# Train a gini-based model
credit_model1 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "gini"))

# Train an information-based model
credit_model2 <- rpart(formula = default ~ ., 
                       data = credit_train, 
                       method = "class",
                       parms = list(split = "information"))

# Generate predictions on the validation set using the gini model
pred1 <- predict(object = credit_model1, 
             newdata = credit_test,
             type = "class")    

# Generate predictions on the validation set using the information model
pred2 <- predict(object = credit_model2, 
             newdata = credit_test,
             type = "class")

# Compare classification error
ce(actual = credit_test$default, 
   predicted = pred1)
ce(actual = credit_test$default, 
   predicted = pred2)  
```

## Introduction to regression trees

## Split the data

Split the data
These examples will use a subset of the Student Performance Dataset from UCI ML Dataset Repository.

The goal of this exercise is to predict a student's final Mathematics grade based on the following variables: sex, age, address, studytime (weekly study time), schoolsup (extra educational support), famsup (family educational support), paid (extra paid classes within the course subject) and absences.

The response is final_grade (numeric: from 0 to 20, output target).

After initial exploration, split the data into training, validation, and test sets. In this chapter, we will introduce the idea of a validation set, which can be used to select a "best" model from a set of competing models.

In Chapter 1, we demonstrated a simple way to split the data into two pieces using the sample() function. In this exercise, we will take a slightly different approach to splitting the data that allows us to split the data into more than two parts (here, we want three: train, validation, test). We still use the sample() function, but instead of sampling the indices themselves, we will assign each row to either the training, validation or test sets according to a probability distribution.

The dataset grade is already in your workspace.

```{r}
# Look at the data
str(grade)

# Set seed and create assignment
set.seed(1)
assignment <- sample(1:3, size = nrow(grade), prob = c(.7, .15, .15), replace = TRUE)

# Create a train, validation and tests from the original data frame 
grade_train <- grade[assignment == 1, ]    # subset grade to training indices only
grade_valid <- grade[assignment == 2, ]  # subset grade to validation indices only
grade_test <- grade[assignment == 3, ]   # subset grade to test indices only
```

## Train a regression tree model 

In this exercise, we will use the `grade_train` dataset to fit a regression tree using `rpart()` and visualize it using `rpart.plot()`. A regression tree plot looks identical to a classification tree plot, with the exception that there will be numeric values in the leaf nodes instead of predicted classes.

This is very similar to what we did previously in Chapter 1. When fitting a classification tree, we use method = "class", however, when fitting a regression tree, we need to set method = "anova". By default, the `rpart()` function will make an intelligent guess as to what the method value should be based on the data type of your response column, but it's recommened that you explictly set the method for reproducibility reasons (since the auto-guesser may change in the future).

The `grade_train` training set is loaded into the workspace.

```{r}
# Train the model
grade_model <- rpart(formula = final_grade ~ ., 
                     data = grade_train, 
                     method = "anova")

# Look at the model output                      
print(grade_model)

# Plot the tree model
rpart.plot(x = grade_model, yesno = 2, type = 0, extra = 0)
```

## Performance metrics for regression

1) Mean Absolute Error (MAE)
$$
MAE = \frac{1}{n} \Sigma|actual - predicted|
$$

2) Root Mean Square Error (RMSE)
$$
RMSE = \sqrt{\frac{1}{n}\Sigma(actual - predicted)^2}
$$

Both MAE and RMSE express average model prediction erro in units of the variable interest. Both metrics are indifferent to the direction of errors and lower values are better. The key difference between the two is that RMSE punishes large errors more harshly than MAE. 

This means the RMSE should be more useful when large errors are particularly undesirable. 

```{r eval=FALSE}
pred <- predict(object = model,
                newdata = testset)

library(Metrics)
# Compute the RMSE
rmse(
  actual = test$response, 
  predicted = pred
)
```

## Evaluate a regression tree model 

Predict the final grade for all students in the test set. The grade is on a 0-20 scale. Evaluate the model based on test set RMSE (Root Mean Squared Error). RMSE tells us approximately how far away our predictions are from the true values.

```{r}
# Generate predictions on a test set
pred <- predict(object = grade_model,   # model object 
                newdata = grade_test)  # test dataset

# Compute the RMSE
rmse(actual = grade_test$final_grade, 
     predicted = pred)
```

## What are the hyperparameters for a decision tree? 

There are sevral knobs that we can turn that affect how the tree is grown, and in many cases, tuning these knobs - or model hyperparameters will result in a performing model. 

The rpart function has a special control parameter, called `control`. 

```{r}
?rpart.control
```

- minsplit: minimum number of data points required to attempt a split (default 20)
- cp: complexity parameter (default 0.1)
- maxdepth: depth of a decision tree, which limits the maximum number of nodes between a leaf node and the root node (default 30)

Complexity parameter serves as a penalty term to control tree size, and is always monotonic with the number of splits. The smaller value of CP, the more complex will be the tree (the greater the number of spilits)

The rpart function computes the 10-fold cross-validated error of the model over various values for CP and stores the results in a table inside the model.

Across the different values of CP using the `plotcp()` function, we can see right away what the optimal value for CP is. To retrieve the optimal value, we simply find the row 
for which xerror is minimized and grab the corresponding CP value. 

Once you have the optimal value, you can tune (or trim) the model using the `prune()` function. The `prune()` function returns the optimized model. 

```{r}
plotcp(grade_model)

print(model$cptable)

# Prune the model to optimized cp value
model_opt <- prune(tree = model,
                   cp = cp_opt)
```

## Tuning the model 
Tune the model using the `prune()` function by finding the best CP value (CP stands for "Complexity Parameter"). 

```{r}
# Plot the "CP Table"
plotcp(grade_model)

# Print the "CP Table"
print(grade_model$cptable)

# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(grade_model$cptable[, "xerror"])
cp_opt <- grade_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
grade_model_opt <- prune(tree = grade_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = grade_model_opt, yesno = 2, type = 0, extra = 0)
```

## Grid search for model selection

Training a default model is good, but training a sequence of models with various hyprparameter settings with the goal of finding the best one is a typical task in any ML pipeline. 

This process is called "model selection" or "hyperparameter selection" because the end goal is to select the best model from the set to use in some ML application.  One of the most common technuques for performing model selection is called grid search. 

### Grid sarch

- What is a model hyperparameter?
- What is a grid? 
- What is the goal of a grid search? 

Model hyperparameters are the knobs that you tweak to get slightly different models. For example, max depth is the maximum depth allowed in a decision tree. 

The grid refers to the set of hyperparameter combinations that you will iterate over durign the grid search. 




