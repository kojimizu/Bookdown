---
title: "Tidymodels.org"
author: "Koji Mizumura"
date: "2020-04-30 - `r Sys.Date()`"
output: 
  rmdformats::readthedown:
    number_sections: yes
    fig_height: 10
    fig_width: 14
    highlight: kate
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '4'
#    css: style.css
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  # fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)

# this is linked to Github Desktop
```

# Learn - create robust models
## [Regression models two ways](https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/) 

### Introduction

To use the code in this article, you will need to install: `AmesHousing`, `glmnet`, `randomForest`, `ranger`, and `tidymodels`.

```{r}
library(AmesHousing)
library(glmnet)
library(randomForest)
library(ranger)
library(tidymodels)
```


We can create regression models with the tidymodels package `parsnip` to predict ontinuous or numeric quantities. Here, let’s first fit a random forest model, which does not require all numeric input (see discussion here) and discuss how to use `fit()` and `fit_xy()`, as well as data descriptors.

Second, let’s fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip. 

### The Ames Housing Data

We’ll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:

```{r}
ames <- make_ames()

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price",
                            p = .75)

ames_train <- training(data_split)
ames_test <- testing(data_split)
```

The use of the test set here is only for illustration; normally in a data analysis, these data would be saved to the very end after many models have been evaluated. 

### Random Forest

We will start by fitting a random forest model to a small set of parameters. Let’s create a model with the predictors Longitude, `Latitude`, `Lot_Area`, `Neighborhood`, and `Year_Sold`. A simple random forest model can be specified via:

```{r}
rf_defaults <- rand_forest(mode = "regression")
rf_defaults
```

The model will be fit with the ranger package by default. Since we didn't add any extra arguments to `fit`, many of the arguments will be set to their defaults from the function `ranger::ranger()`. The help pages for the model function describe the default parameters and you can use the `translate()` function to check out such details.

The parsnip package provides two different interfaces to fit a model:

- the formula interface `fit()`, and
- the non-formula interface `fit_xy()`

Let's start with the non-formula interface:

```{r}
preds <- c("Longitude", "Latitude", "Lot_Area",
           "Neighborhood", "Year_Sold")

rf_xy_fit <- 
  rf_defaults %>% 
  set_engine("ranger") %>% 
  fit_xy(
    x = ames_train[, preds],
    y = log10(ames_train$Sale_Price)
  )

rf_xy_fit
```

The non-formula interface doesn't  do anything to the predictors before passing them to the underlying model function. This particular model does not require indicator variables (sometimes called "dummy variables") to be created to prior to fitting model.  Note that the output shows “Number of independent variables: 5”.

For regression models, we can use the basic `predict()` method, which returns a tibble with a column named `.pred`:

```{r}
test_results <- 
  ames_test %>% 
  select(Sale_Price) %>% 
  mutate(Sale_Price = log10(Sale_Price)) %>% 
  bind_cols(
    predict(rf_xy_fit, new_data = ames_test[,preds])
  )

test_results %>% 
  slice(1:5)

# summarize performance 
test_results %>% 
  metrics(truth = Sale_Price,
          estimate = .pred)
```

Note that:

- If the model required indicator variables, we would have to create them manually prior to using `fit()` (perhaps )
- We had to manually log the outcome prior to modeling

Now, for illustration, let's use the formula method using some new parameter values:

```{r}

rand_forest(mode = "regression",
            mtry = 3,
            trees = 1000) %>% 
  set_engine("ranger") %>% 
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  )
```

Suppose that we would like to use the `randomForest` package instead of ranger. To do so, the only part of the syntax that needs to change is the `set_engine()` argument:

```{r}
rand_forest(mode = "regression",
            mtry = 3,
            trees = 1000) %>% 
  set_engine("randomForest") %>% 
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

Look at the formula code that was printed out; one function uses the argument name `ntree` and the other uses `num.trees`. The parsnip models don’t require you to know the specific names of the main arguments.

Now suppose that we want to modify te value of `mtry` based on the number of predictors in the data. Usually, a good default value is `floor(sqrt(num_predictors))` but a pure bagging model requires an `mtry` value equal to the total number of parameters. There maybe cases where yo umay not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or variable filter) so this might be difficult to know exactly ahead of time when you write you code. 

When the model it being fit by parsnip, data descriptors are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using `rand_forest()`), the values of the arguments that you give it are immediately evaluated unless you delay them. To delay the evaluation of any argument, you can use `rlang::expr()` to make an expression.

Two relevant data description for our example model are: 

- `.pred()`: the number of predictor variables in the data set that are associated with the predictors __prior to dummy variable creation__
- `.cols()`: the number of predictor columns after dummy variables (or other encodings) are created. 

Since ranger won't create indicator values, `.preds()` would be appropriate for `mtry` for a bagging model. For example, let's use an expression with the `.preds()` descriptor to fit a bagging model:

```{r}
rand_forest(mode = "regression", mtry = .preds(), trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

### Regularized regression 

A linear model might work for this data set as well. We can use the `linear_reg()` parsnip model. There are two engines that can perform  regularization/penalization, the glmnet and sparklyr packages. Let’s use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used.

When regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won’t do that automatically so we will need to do this ourselves. We’ll use the recipes package for these steps.

```{r}
norm_recipe <- 
  recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %>%
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  step_log(Sale_Price, base = 10) %>% 
  # estimate the means and standard deviations
  prep(training = ames_train, retain = TRUE)

# Now let's fit the model using the processed version of the data

glmn_fit <- 
linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  fit(Sale_Price ~ ., data = juice(norm_recipe))
glmn_fit
```

If `penalty` were not specified, all of the `lambda` values would be computed.

To get the predictions for this specific value of lambda (aka penalty):

```{r}
# First, get the processed version of the test set predictors
test_normalized <- bake(norm_recipe, 
                        new_data = ames_test,
                        all_predictors())

test_results <- 
  test_results %>% 
  rename(`random forest` = .pred) %>% 
  bind_cols(
    predict(glmn_fit, new_data = test_normalized) %>% 
      rename(glmnet = .pred)
  )

test_results

test_results %>% metrics(truth = Sale_Price, estimate = glmnet) 
#> # A tibble: 3 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 rmse    standard      0.132 
#> 2 rsq     standard      0.410 
#> 3 mae     standard      0.0956

test_results %>% 
  gather(model, prediction, -Sale_Price) %>% 
  ggplot(aes(x = prediction, y = Sale_Price)) + 
  geom_abline(col = "green", lty = 2) + 
  geom_point(alpha = .4) + 
  facet_wrap(~model) + 
  coord_fixed()
```

This final plot compares the performance of the random forest and regularized regression models.

## [Classification models using a neural network](https://www.tidymodels.org/learn/models/parsnip-nnet/)

### Introduction

To use the code in this article, you will need to install the following packages: keras and tidymodels. You will also need the python keras library installed (see `?keras::install_keras()`).

```{r}
library(tidyverse)
library(tidymodels)
library(keras)
```


We can create classification models with the tidymodels package parsnip to predict categorical quantities or class labels. Here, let’s fit a single classification model using a neural network and evaluate using a validation set. While the tune package has functionality to also do this, the parsnip package is the center of attention in this article so that we can better understand its usage.

### Fitting a neural network

Let’s fit a model to a small, two predictor classification data set. The data are in the modeldata package (part of tidymodels) and have been split into training, validation, and test data sets. In this analysis, the test set is left untouched; this article tries to emulate a good data usage methodology where the test set would only be evaluated once at the end after a variety of models have been considered.

```{r}
library(keras)
data(bivariate)
nrow(bivariate_train)
#> [1] 1009
nrow(bivariate_val)
#> [1] 300
```

A plot of the data shows two right-skewed predictors:

```{r}
ggplot(bivariate_train, aes(x = A, y = B, col = Class)) + 
  geom_point(alpha = .2)
```

Let’s use a single hidden layer neural network to predict the outcome. To do this, we transform the predictor columns to be more symmetric (via the s`tep_BoxCox()` function) and on a common scale (using `step_normalize()`). We can use recipes to do so:

```{r}
biv_rec <- 
  recipe(Class ~ ., data = bivariate_train) %>%
  step_BoxCox(all_predictors())%>%
  step_normalize(all_predictors()) %>%
  prep(training = bivariate_train, retain = TRUE)

# We will juice() to get the processed training set back

# For validation:
val_normalized <- bake(biv_rec, new_data = bivariate_val, all_predictors())
# For testing when we arrive at a final model: 
test_normalized <- bake(biv_rec, new_data = bivariate_test, all_predictors())
```

We can use the keras package to fit a model with 5 hidden units and a 10% dropout rate, to regularize the model:

```{r}
set.seed(57974)
nnet_fit <-
  mlp(epochs = 100, hidden_units = 5, dropout = 0.1) %>%
  set_mode("classification") %>% 
  # Also set engine-specific `verbose` argument to prevent logging the results: 
  set_engine("keras", verbose = 0) %>%
  fit(Class ~ ., data = juice(biv_rec))

nnet_fit
```

### Model performance

In parsnip, the `predict()` function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data:

```{r}
val_results <- 
  bivariate_val %>%
  bind_cols(
    predict(nnet_fit, new_data = val_normalized),
    predict(nnet_fit, new_data = val_normalized, type = "prob")
  )
val_results %>% slice(1:5)
#> # A tibble: 5 x 6
#>       A     B Class .pred_class .pred_One .pred_Two
#>   <dbl> <dbl> <fct> <fct>           <dbl>     <dbl>
#> 1 1061.  74.5 One   Two             0.473    0.527 
#> 2 1241.  83.4 One   Two             0.484    0.516 
#> 3  939.  71.9 One   One             0.636    0.364 
#> 4  813.  77.1 One   One             0.925    0.0746
#> 5 1706.  92.8 Two   Two             0.355    0.645

val_results %>% roc_auc(truth = Class, .pred_One)
#> # A tibble: 1 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 roc_auc binary         0.815

val_results %>% accuracy(truth = Class, .pred_class)
#> # A tibble: 1 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.737

val_results %>% conf_mat(truth = Class, .pred_class)
#>           Truth
#> Prediction One Two
#>        One 150  27
#>        Two  52  71
```

Let’s also create a grid to get a visual sense of the class boundary for the validation set.

```{r}
a_rng <- range(bivariate_train$A)
b_rng <- range(bivariate_train$B)
x_grid <-
  expand.grid(A = seq(a_rng[1], a_rng[2], length.out = 100),
              B = seq(b_rng[1], b_rng[2], length.out = 100))
x_grid_trans <- bake(biv_rec, x_grid)

# Make predictions using the transformed predictors but 
# attach them to the predictors in the original units: 
x_grid <- 
  x_grid %>% 
  bind_cols(predict(nnet_fit, x_grid_trans, type = "prob"))

ggplot(x_grid, aes(x = A, y = B)) + 
  geom_contour(aes(z = .pred_One), breaks = .5, col = "black") + 
  geom_point(data = bivariate_val, aes(col = Class), alpha = 0.3)

```

## Subsampling for class imbalances 

Improve model performance in imbalanced data sets through undersampling or oversampling.

### Introduction 

To use the code in this article, you will need to install the following packages: `discrim`, `klaR`, `readr`, `ROSE`, `themis`, and tidymodels.

```{r}
# pacman::p_install(c(discrim, klaR, ROSE, themis))

pacman::p_load(
  discrim,
  klaR,
  readr,
  ROSE,
  themis,
  tidymodels)
```


Subsampling a training set, either undersampling or oversampling the appropriate class or classes, can be a helpful approach to dealing with classification data where one or more classes occur very infrequently. In such a situation (without compensating for it), most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance.

This article describes subsampling for dealing with class imbalances. For better understanding, some knowledge of classification metrics like _sensitivity_, _specificity_, and _receiver operating characteristic_ curves is required. See Section 3.2.2 in Kuhn and Johnson (2019) for more information on these metrics.

### Simulated data

Consider a two-class problem where the first class has a very low rate of occurence. The data were simulated and can be imported into R usig the code below. 

```{r}
imbal_data <- 
  readr::read_csv("http://bit.ly/imbal_data") %>% mutate(Class = factor(Class))

imbal_data

dim(imbal_data)
table(imbal_data$Class)
```

If Class 1 is the event of interest, it is very likely that a classification model would be able to achieve very good sepcificity since almost all of the data are of the second class. __Sensitivity__, however, would likelt to be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class.

One result of class imbalance when there are two classes is that the default probability cuttoff 50% is inappropriate; a different cutoff that is more extreme might be able to achieve a good performance.

### Subsampling the data 

One way to achieve this issue is to _subsample_ the data. There are a number of ways to do this but the most simple one is to _sample down_ (_downsample_) the majority class data until it occurs with the same frequency as the minority class.

While it may seem counterintuitive, throwing out a large percentage of your data can be effective at producing a useful model that can recognize both the majority and minority classes. n some cases, this even means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are better _calibrated_, meaning that the distributions of the class probabilities are more well behaved. As a result, the default 50% cutoff is much more likely to produce better sensitivity and specificity values than they would otherwise.

Let’s explore subsampling using `themis::step_rose()` in a recipe for the simulated data. It uses the ROSE (random over sampling examples) method from Menardi, G. and Torelli, N. (2014). This is an example of an oversampling strategy, rather than undersampling.

In terms of workflow:

- It is extremely important that subsampling occurs inside of resampling. Otherwise, the resampling process can produce [poor estimates of model performance](poor estimates of model performance). 
- The subsampling process should only be applied to the analysis set. The assessment set should reflect the event rates seen “in the wild” and, for this reason, the `skip` argument to `step_downsample()` and other subsampling recipes steps has a default of TRUE.

Here is a simple recipe implementing oversampling:

```{r}
library(themis)
imbal_rec <-
  recipe(Class ~ ., data = imbal_data) %>% 
  step_rose(Class)
```

For a model, let's use a [quadratic discriminant analysis](https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis) (QDA) model. From the `discrim` package, this model can be specified using:

```{r}
library(discrim)
qda_mod <- 
  discrim_regularized(frac_common_cov = 0,
                      frac_identity = 0) %>% 
  set_engine("klaR")

qda_mod
```

To keep these objects bound together, they can be comibined in a [workflow](https://tidymodels.github.io/workflows/):

```{r}
qda_rose_wflw <- 
  workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(imbal_rec)

qda_rose_wflw
```

### Model performance

Stratified, repeated 10-fold cross-validation is used to resample the model

```{r}
set.seed(5732)
cv_folds <- vfold_cv(imbal_data,
                    strata = "Class",
                    repeats = 5)
```

To measure model performance, let's use two metrics: 

- The area under the ROC curve is an overall assessment of performance across all cotoffs. Values near one indicate very good results while values near 0.05 would imply that the model is very poor. 
- THe J index (aka Youden's J statistic) is `sensitivity + specificity - 1`. Values near one are once again best.

If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the J index would be lower for models with pathological distributions for the class probabilities. The yardstick package will be used to compute these metrics.

```{r}
cls_metrics <- metric_set(roc_auc,
                          j_index)
```

Now, we train the moels adn generate the results using `tune::fit_resamples()`:

```{r}
set.seed(2180)
qda_rose_res <- fit_resamples(
  qda_rose_wflw,
  resamples = cv_folds,
  metrics = cls_metrics
)

collect_metrics(qda_rose_res)
```

What do the results look like without using ROSE? We can create another workflow and fit the QDA model along the same resamples:

```{r}
qda_wflw <- 
  workflow() %>% 
  add_model(qda_mod) %>% 
  add_formula(Class ~ .)

set.seed(2180)

qda_only_res <- fit_resamples(qda_wflw, resamples = cv_folds, metrics = cls_metrics)

# collect / compare results
collect_metrics(qda_only_res)
#> # A tibble: 2 x 5
#>   .metric .estimator  mean     n std_err
#>   <chr>   <chr>      <dbl> <int>   <dbl>
#> 1 j_index binary     0.250    50 0.0288 
#> 2 roc_auc binary     0.953    50 0.00479
```

It looks like ROSE helped a lot, especially with the J-index. Class imbalance sampling methods tend to greatly improve metrics based on the hard class predictions (i.e., the categorical predictions) because the default cutoff tends to be a better balance of sensitivity and specificity.

Let’s plot the metrics for each resample to see how the individual results changed.

```{r}
no_sampling <- 
  qda_only_res %>% 
  collect_metrics(summarize = FALSE) %>% 
  dplyr::select(-.estimator) %>% 
  mutate(sampling = "no_sampling")

with_sampling <- 
  qda_rose_res %>% 
  collect_metrics(summarize = FALSE) %>% 
  dplyr::select(-.estimator) %>% 
  mutate(sampling = "rose")

bind_rows(no_sampling, with_sampling) %>% 
  mutate(label = paste(id2, id)) %>% 
  ggplot(aes(x = sampling, y = .estimate, group = label))+
  geom_line(alpha =.4)+
  facet_wrap(~.metric, scales ="free_y")
```

## [Modeling time series with tidy resamples](https://www.tidymodels.org/learn/models/time-series/)

Calculate performance estimate for time series forecasts using `resamples`.

### Introduction 

To use the code in this article, you will need to install the following packages: `forecast`, `sweep`, `tidymodels`, `tidyr`, `timetk` and `zoo`.

```{r}
pacman::p_load(
  forecast, 
  sweep,
  tidymodels,
  tidyr,
  timetk,
  zoo
)
```


[“Demo Week: Tidy Forecasting with sweep”](https://www.business-science.io/code-tools/2017/10/25/demo_week_sweep.html) is an excellent article that uses tidy methods with time series. This article uses their analysis with rsample to find performance estimates for future observations using [rolling forecast origin resampling](https://robjhyndman.com/hyndsight/crossvalidation/).

### Example data

The data for this article are sales of alcholic beverages originally from [the Federal Reserve Bank of St.Louis website](https://fred.stlouisfed.org/series/S4248SM144NCEN). 

```{r}
pacman::p_load(modeldata)
pacman::p_load(fabletools,
               tsibble,
               feasts,
               fable)
data("drinks")

drinks
glimpse(drinks)

drinks %>% 
  tsibble::as_tsibble() %>% 
  fabletools::autoplot() 
```

Each row represents one month of sales (in millions of US dollars).

### Time series resampling 

Suppose we need predictions for one year ahead and our model should use the most recent data from the last 20 years. To set up this resampling scheme:

```{r}

roll_rs <- rolling_origin(
  drinks,
  initial = 12*20,
  assess = 12,
  cumulative = FALSE
)

nrow(roll_rs)

roll_rs
```

Each `split` element contains the information about that resample:

```{r}
roll_rs$splits[[1]]
```

For plotting let's index each split by the first day of the assessment set:

```{r}
get_date <- function(x) {
  min(assessment(x)$date)
}

start_date <- map(roll_rs$splits, get_date)
roll_rs$start_date <- do.call("c", start_date)
head(roll_rs$start_date)
```

This resampling scheme has 58 splits of the data so that there will be 58 ARIMA models that are fit. To create the models, we use the `auto.arima()` function from the `forecast` package. The rsample functions `analysis()` and `assessment` returns a data frame, so another step converts the data to `ts` object called `mod_dat` using a function in the `timetk` package.

```{r}
fit_model <- function(x, ...){
  # suggested by Matt Dancho:
  x %>% 
    analysis() %>% 
    # since the first data changes over resamples,
    # adjust it based on the first date value in the data frame
    tk_ts(start = .$date[[1]] %>% as.yearmon(),
          frequency = 12,
          silent = TRUE) %>% 
    auto.arima()
}
```

Save each model in a new column:

```{r}
roll_rs <- roll_rs %>% 
  mutate(arima = map(splits, fit_model))

# for example
roll_rs$arima[[1]]
```

(There are some warnings produced by these regarding extra columns in the data that can be ignored.)


### Model performance 

Using the model fits, let's measure performance in two ways:

- Interpolation error will measure how well the model fis to the data that were used to create the model. This is most likely optimistic since no holdout method is used.
- Extrapolation or forecast error evaluates the performance of the model on the data from the following year (that were not used in the model fit)

In each case, the mean absolute percent error (MAPE) is the statistic used to characterize the model fits. The interpolation error can be computed from `Arima` object. To make things easy, let's use the sweep package's `sw_glance()` function.

```{r}
library(sweep)

roll_rs$interpolation <- map_dbl(
  roll_rs$arima,
  function(x)
    sw_glance(x)[["MAPE"]]
)

summary(roll_rs$interpolation) %>% 
  broom::tidy()
```

For the extrapolation error, the model and split objects are required. Using these:

```{r}
get_extrap <- 
  function(split, mod){
    n <- nrow(assessment(split))
    # get assessment data
    pred_dat <- assessment(split) %>% 
      mutate(
        pred = as.vector(forecast(mod, h = n)$mean),
        pct_error = ( S4248SM144NCEN - pred ) / S4248SM144NCEN * 100
      )
    mean(abs(pred_dat$pct_error))
  }

roll_rs$extrapolation <- 
  map2_dbl(roll_rs$splits, roll_rs$arima, get_extrap)

summary(roll_rs$extrapolation) %>% 
  broom::tidy()
```

What do  these error estimate look like over time?

```{r warning=FALSE}
roll_rs %>% 
  select(interpolation, extrapolation, start_date) %>% 
  pivot_longer(cols = matches("ation"), names_to = "error", values_to = "MAPE") %>% 
  ggplot(aes(x = start_date, y = MAPE, col = error))+
  geom_point()+
  geom_line()+
  hrbrthemes::theme_ipsum_es()

```

It is likely that the interpolation error is an underestimate to some degree, as mentioned above.

It is also worth noting that `rolling_origin()` can be used over calendar periods, rather than just over a fixed window size. This is especially useful for irregular series where a fixed window size might not make sense because of missing data points, or because of calendar features like different months having a different number of days.

The example below demonstrates this idea by splitting `drinks` into a nested set of 26 years, and rolling over years rather than months. Note that the end result accomplishes a different task than the original example; in this new case, each slice moves forward an entire year, rather than just one month.

```{r}
# The idea is to nest by the period to roll over,
# which in this case is the year.
roll_rs_annual <- drinks %>%
  mutate(year = as.POSIXlt(date)$year + 1900) %>%
  nest(data = c(date, S4248SM144NCEN)) %>%
  rolling_origin(
    initial = 20, 
    assess = 1, 
    cumulative = FALSE
  )

analysis(roll_rs_annual$splits[[1]])
#> # A tibble: 20 x 2
#>     year data             
#>    <dbl> <list>           
#>  1  1992 <tibble [12 × 2]>
#>  2  1993 <tibble [12 × 2]>
#>  3  1994 <tibble [12 × 2]>
#>  4  1995 <tibble [12 × 2]>
#>  5  1996 <tibble [12 × 2]>
#>  6  1997 <tibble [12 × 2]>
#>  7  1998 <tibble [12 × 2]>
#>  8  1999 <tibble [12 × 2]>
#>  9  2000 <tibble [12 × 2]>
#> 10  2001 <tibble [12 × 2]>
#> 11  2002 <tibble [12 × 2]>
#> 12  2003 <tibble [12 × 2]>
#> 13  2004 <tibble [12 × 2]>
#> 14  2005 <tibble [12 × 2]>
#> 15  2006 <tibble [12 × 2]>
#> 16  2007 <tibble [12 × 2]>
#> 17  2008 <tibble [12 × 2]>
#> 18  2009 <tibble [12 × 2]>
#> 19  2010 <tibble [12 × 2]>
#> 20  2011 <tibble [12 × 2]>

```

The workflow to access these calendar slices is to use `bind_rows()` to join each analysis set together. 

## [Multivariate analysis using partial least squares](https://www.tidymodels.org/learn/models/pls/)

Build and fit a predictive model with more than one outcome. 

### Introduction

To use the code in this article, you need to install the following packages: `modeldata`, `pls`, `tidymodels` and `tidyr`.

```{r}
pacman::p_load(
  modeldata, 
  pls,
  tidymodels,
  tidymodels
)
```


“Multivariate analysis” usually refers to multiple outcomes being modeled, analyzed, and/or predicted. There are multivariate versions of many common statistical tools. For example, suppose there was a data set with columns `y1` and `y2` representing two outcomes to be predicted. The `lm()` function would look something like:

```{r eval=FALSE}
lm(cbind(y1, y2) ~ ., data = dat)
```

This `cbind()` calls its pretty awkward and is a consequence of how the traditional formula infrastructure works. The recipes package is a lot easier to work with. This article demonstrates how to model multiple outcomes. 

The data that we’ll use has three outcomes. From `?modeldata::meats`:

> “These data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.

> “For each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.”

The goal is to predict the proportion of the three substances using the chemistry test. There can often be a high degree of between-variable correlations in predictors, and that is certainly the case here. 

To start, let's take the two data matrices (called `endpoints` and `absorp`) and bind them together in a data frame:

```{r}
pacman::p_load(modeldata)
data(meats)
```

```{r}
meats
```

The three outcomes have fairly high correlations also. 

### Preprocessing the data

If the outcome can be predicted using a linear model, partial least squares (PLS) is an ideal method. PLS models the data as a function of a set of unobserved latent variables that are derived in a manner similar to principal component analysis (PCA). 


PLS, unlike PCA, also incorporate the outcome data when creating the PLS components. Like PCA, it tries to maximize the variance of the predictors that are explained by the components but it also tries to simultaneously maximize the correlation between those components and the outcomes. In this way, PLS chases variation of the predictors and outcomes. 

Since we are working with variances and covariance, we need to standardize the data. The recipe will center and scale all of the variables. 

Many base R functions that deal with multivariate outcomes using a formula require the use of `cbind()` on the left-hand side of the formula to work with the traditional formula methods. In tidymodels, recipes do not; the outcomes can be symbolically “added” together on the left-hand side:

```{r}
norm_rec <- 
  recipe(water + fat + protein ~., data = meats) %>% 
  step_normalize(everything())
```

Before, we can finalize the PLS model, the number of PLS components to retain must be determined. This can be done using performance metrics such as the root mean squared error. However, we can also calculate the proportion of variance explained by the components for the predictors and each of the outcomes. This allows an informed choice to be made based on the level of evidence that the situation requires. 

Since the data set isn’t large, let’s use resampling to measure these proportions. With ten repeats of 10-fold cross-validation, we build the PLS model on 90% of the data and evaluate on the heldout 10%. For each of the 100 models, we extract and save the proportions.

The folds can be created using the rsample package, and the recipe can be estimated for each rsample using the `prepper()` function:

```{r}
set.seed(57343)
folds <- vfold_cv(meats, repeats = 10)

folds <- 
  folds %>% 
  mutate(recipes = map(splits, prepper, recipe = norm_rec))
```

### Partial least squares 

The complicated parts for moving foward are:

1. Formatting the predictors and outcomes into the format that the pls package requires, and
2. Estimating the proportions

For the first part, he standardized outcomes and predictors need to be formatted into two separate matrices. Since we used `retain = TRUE` when prepping the recipes, we can use the `juice()` function. To save the data as a matrix, the option `composition = "matrix"` will avoid saving the data as tibbles and use the required format.

Thxpects a simple formula to specify the model, but each side of the formula should represent a matrix. In other words, we need a data set with two columns where each column is a matrix. The secret to doing this is to “protect” the two matrices using `I()` when adding them to the data frame.e pls package

The calculation for the proportion of variance explained is straightforward for the predictors; the function `pls::explvar()` will compute that. For the outcomes, the process is more complicated. A ready-made function to compute these is not obvious but there is some code inside of the summary function to do the computation (see below).

The function `get_var_explained()` shown here will do all these computations and return a data fram with columns `components`, `source` (for predictors water, etc.), and the `proportion` of variance that is explained by the components.

```{r}
pacman::p_load(pls,
               tidyr)

library(pls)
library(tidyr)

get_var_explained <- function(recipe, ...) {
  
  # Extract the predictors and outcomes into their own matrices
  y_mat <- juice(recipe, composition = "matrix", all_outcomes())
  x_mat <- juice(recipe, composition = "matrix", all_predictors())
  
  # The pls package prefers the data in a data frame where the outcome
  # and predictors are in _matrices_. To make sure this is formatted
  # properly, use the `I()` function to inhibit `data.frame()` from making
  # all the individual columns. `pls_format` should have two columns.
  pls_format <- data.frame(
    endpoints = I(y_mat),
    measurements = I(x_mat)
  )
  # Fit the model
  mod <- plsr(endpoints ~ measurements, data = pls_format)
  
  # Get the proportion of the predictor variance that is explained
  # by the model for different number of components. 
  xve <- explvar(mod)/100 

  # To do the same for the outcome, it is more complex. This code 
  # was extracted from pls:::summary.mvr. 
  explained <- 
    drop(pls::R2(mod, estimate = "train", intercept = FALSE)$val) %>% 
    # transpose so that components are in rows
    t() %>% 
    as_tibble() %>%
    # Add the predictor proportions
    mutate(predictors = cumsum(xve) %>% as.vector(),
           components = seq_along(xve)) %>%
    # Put into a tidy format that is tall
    pivot_longer(
      cols = c(-components),
      names_to = "source",
      values_to = "proportion"
    )
}
```

We compute this data frame for each resample and save the results in the different columns.

```{r}
folds <- 
  folds %>% 
  mutate(var = map(recipes, get_var_explained),
         var = unname(var))
```

To extract and aggregate these data, simple row binding can be used to stack the data vertically. Most of the action happens in the first 15 components, so ket's filter the data and compute the average proportion.

```{r}
variance_data <- 
  bind_rows(folds[["var"]]) %>%
  filter(components <= 15) %>%
  group_by(components, source) %>%
  summarize(proportion = mean(proportion))

variance_data
```

The plot below shows that, if the protein measurement is important, you might require 10 or so components to achieve a good representation of that outcome. Note that the predictor variance is captured extremely well using a single component. This is due to high degree of correlation in those data.

```{r warning=FALSE}
ggplot(variance_data,
       aes(x = components, y = proportion, col = source))+
  geom_line()+
  geom_point()+
  hrbrthemes::theme_ipsum_es()
```

# Learn - Tune, compare, and work with your models 

## Model tuning via grid search 

Choose hyperparameters for a model by training on a grid of many possible parameter values.

### Introduction 

To use the code in this article, you will need to install the following packages: `kernlab`, `mlbench`, and `tidymodels`.

```{r}
pacman::p_load(
  kernlab,
  mlbench,
  tidymodels
)

data("Ionosphere")
```

This article demonstrates how to tune a model using grid search. Many models have hyperparameters that can't be learned directly from a single data when training the model. Instead, we can train many models in a grid of possible hyperparameter values and see which ones turn out best. 

### Example data 

To demonstrate model tuning, we will use the lonosphere data in the mlbench package:

From `?Ionosphere`: 

> This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. “good” radar returns are those showing evidence of some type of structure in the ionosphere. “bad” returns are those that do not; their signals pass through the ionosphere.

> Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal. See cited below for more details.

There are 43 predictors and a factor outcome. Tow of the predictors are factors (`V1` and `V2`) and the rest are numeric variables that have been scaled to range of -1 and 1. Note that the two factor predictors have sparse distributions:

```{r}
table(Ionosphere$V1)
table(Ionosphere$V2)
```

There is no point of putting `V2` into any model since it is a zero-variance predictor. `V1` is not but it could be if the resampling process ends us sampling all of the same value. Is this an issue? It might be since the standard R formula infrastructure fails when there is only a single observed value:

```{r}
glm(Class ~ ., data = Ionosphere, family = binomial)
#> Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels

# Surprisingly, this doesn't help: 

glm(Class ~ . - V2, data = Ionosphere, family = binomial)
#> Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels

```

At a minimum, let's get rid of the most problematic variable:

```{r}
Ionosphere <- Ionosphere %>% 
  select(-V2)
```

### Inputs for the search

To demonstrate, we will fit a radial basis function support vector machine to these data and tune the SVM cost parameter and the $\sigma$ parameter in the kernel function:

```{r}
svm_mod <- 
  svm_rbf(cost = tune(),
          rbf_sigma = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")
```

In this article, tuning will be demonstrated in two ways, using:

- a standard $ formula, and
- a recipe

Let's create the recipe here:

```{r}
iono_rec <- 
  recipe(Class ~., data = Ionosphere) %>% 
  # In case V1 is a single value sampled
  step_zv(all_predictors()) %>% 
  # convert it to a dummy variable 
  step_dummy(V1) %>% 
  # scale it the same as the others 
  step_range(matches("Vq_"))
```

The only other required item for tuning is a resampling strategy as defined by an rsample object. Let's demonstrate using basic bootstrapping:

```{r}
set.seed(4943)
iono_rs <- bootstraps(Ionosphere,
                      times = 30)
```

### Optional inputs 

An optional step for model tuning is to specify which metrics should be computed using the out-of-sample predictions. For classification, the default is to calculate the log-likelihood statistic and overall accuracy. Instead of the defaults, the area under the ROC curve will be used. To do this, a yardstick package function can be used to create a metric set:

```{r}
roc_vals <- metric_set(roc_auc)
```


If no grid or parameters are provided, a set of 10 hyperparameters are created using a space-filling design (via a Latin hypercube). A grid can be given in a data frame where the parameters are in columns and parameter combinations are in rows. Here, the default will be used.

Also, a control object can be passed that specifies different aspects of the search. Here, the verbose option is turned off.

```{r}
ctrl <- control_grid(verbose = FALSE)
```


### Executing with a formula

First, we can use the formula interface:

```{r}
set.seed(35)
formula_res <- 
  svm_mod %>% 
  tune_grid(
    Class ~ .,
    resamples = iono_rs,
    metrics = roc_vals,
    control = ctrl
  )

formula_res
```


THe `.metrics` column contains tibbles of the performance metrics for each tuning parameter combination:

```{r}
formula_res %>% 
  select(.metrics) %>% 
  slice(1) %>% 
  pull(1)
```

To get the final resampling estimates, the `collect_metrics()` function can be used on the grid object:

```{r}
estimates <- collect_metrics(formula_res)
estimates
```

The top combinations are 
```{r}
show_best(formula_res,
          metric = "roc_auc")
```

### Executing with a recipe

Next, we can use the same syntax but pass a recipe in as the pre-processor argument:

```{r}
set.seed(325)
recipe_res <-
  svm_mod %>% 
  tune_grid(
    iono_rec,
    resamples = iono_rs,
    metrics = roc_vals,
    control = ctrl
  )
recipe_res
#> # Bootstrap sampling 
#> # A tibble: 30 x 4
#>    splits            id          .metrics          .notes          
#>  * <list>            <chr>       <list>            <list>          
#>  1 <split [351/120]> Bootstrap01 <tibble [10 × 5]> <tibble [0 × 1]>
#>  2 <split [351/130]> Bootstrap02 <tibble [10 × 5]> <tibble [0 × 1]>
#>  3 <split [351/137]> Bootstrap03 <tibble [10 × 5]> <tibble [0 × 1]>
#>  4 <split [351/141]> Bootstrap04 <tibble [10 × 5]> <tibble [0 × 1]>
#>  5 <split [351/131]> Bootstrap05 <tibble [10 × 5]> <tibble [0 × 1]>
#>  6 <split [351/131]> Bootstrap06 <tibble [10 × 5]> <tibble [0 × 1]>
#>  7 <split [351/127]> Bootstrap07 <tibble [10 × 5]> <tibble [0 × 1]>
#>  8 <split [351/123]> Bootstrap08 <tibble [10 × 5]> <tibble [0 × 1]>
#>  9 <split [351/131]> Bootstrap09 <tibble [10 × 5]> <tibble [0 × 1]>
#> 10 <split [351/117]> Bootstrap10 <tibble [10 × 5]> <tibble [0 × 1]>
#> # … with 20 more rows

```

The best setting here is:

```{r eval=FALSE}
show_best(recipe_res, metric = "roc_auc")
#> # A tibble: 5 x 7
#>      cost rbf_sigma .metric .estimator  mean     n std_err
#>     <dbl>     <dbl> <chr>   <chr>      <dbl> <int>   <dbl>
#> 1 15.6    0.182     roc_auc binary     0.981    30 0.00215
#> 2  0.385  0.0276    roc_auc binary     0.978    30 0.00220
#> 3  0.143  0.00243   roc_auc binary     0.948    30 0.00349
#> 4  0.841  0.000691  roc_auc binary     0.921    30 0.00421
#> 5  0.0499 0.0000335 roc_auc binary     0.903    30 0.00463

```

## [Nested resampling](https://www.tidymodels.org/learn/work/nested-resampling/)

Estimate the best performance for a model using nested resampling

### Introduction

To use the code in this article, you will need to install the following packages; `furrr`, `kernlab`, `mlbench`, `scales` and `tidymodels`.

In this article, we discuss and alternative method for evaluating and tuning models, called [nested resampling](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22nested+resampling%22+inner+outer&btnG=). While it is more computationally taxing and challenging to implement than other resampling methods, it has the potential to produce better estimates of model performance. 

### Resampling models 

A typical scheme for splitting the data when developing a predictive model is to create an initial split of the data into a training and test set. If resampling is used, it is executed on the training set. A series of binary splits is created. In `rsample`, we use the term `analysis set` for the data that are used to fit the model and the term `assessement set` for the set used to compute performance:

```{r}
knitr::include_graphics("resampling.svg")
```

A common method for tuning models is grid search where a candidate set of tuning parameters is created. The full set of models for every combination of the tuning parameter grid and the resamples is fitted. Each time, the assessment data are used to measure performance and the average value is determined for each tuning parameter. 

The potential problem is that once we pick the tuning parameter associated with the best performance, this performance value is usually quoted as the performance of the model. There is serious potential for _optimization bias_ we use the sample data to tune the model and to assess performance. This would result un an optimistic estimate of performance. 

Nested resampling uses an additional layer of resampling that separates the tuning activities from the process used to estimate the efficacy of the model. An _outer_ resampling scheme is used and, for every split in the outer resample, another full set of resampling splits are created on the original analysis set. For example, if 10-fold cross-validation is used on the outside and 5-fold cross-validation on the inside, a total of 500 models will be fit. The parameter tuning will be conducted 10 times and the best parameters are determined from the average of the 5 assessment sets. This process occurs 10 times. 

Once the tuning results are complete, a model is fit to each of the other resaompling splits using the best parameter associated with that resample. The average of the outer method's assessments sets are unbiasred estimate of the model.

We will simulate some regression data to illustrate the methods. The mlbench package has a function `mlbench::mlbench.friedman1()` that can simulate a complex regression data structure from the original MARS publication. A training set size of 100 data points are generated as well as a large set that will be used to characterize how well the resampling procedure performed.

```{r}
library(mlbench)
sim_data <- function(n) {
  tmp <- mlbench.friedman1(n, sd = 1)
  tmp <- cbind(tmp$x, tmp$y)
  tmp <- as.data.frame(tmp)
  names(tmp)[ncol(tmp)] <- "y"
  tmp
}

set.seed(9815)
train_dat <- sim_data(100)
large_dat <- sim_data(10^5)

```

### Nested resampling 

To get started, the types of resampling methods need to be specified. This isn't a large data set, so 5 repeated of 10-fold cross validation will be used as ourter resampling method for generating the estimate of overall performance. To tune the model, it would be good to have precise estimates for each of the values of the tuning parameter so let's use 25 iterations of the bootstrap. 

This means that there will eventually be `5 * 10 * 25 = 1250` models that are fit to the data per tuning parameter. These models will be discarded once the performance of the model has been quantified.

To create the tibble with the resampling specifications:

```{r}
pacman::p_load(tidymodels)
results <- nested_cv(
  train_dat, 
  outside = vfold_cv(repeates = 5),
  inside  = bootstraps(times = 25))

results
```

The splitting information for each resampling is contained in the `split` objects. Focusing on the second of the first repeat: 

```{r}
results$splits[[2]]
```

`<90/10/100>` indicates the number of observations in the analysis set, and the original data. Each element of `inner_resamples` has its own tibble with the bootstrapping splits.

```{r}
results$inner_resamples[[5]]
```

There are self-contained, meaning that the bootstrap sample is aware that it is a sample of a specific 90% of the data:

```{r}
results$inner_resamples[[5]]$splits[[1]]
```
To start, we need to define how the model will be created and measured. Let’s use a radial basis support vector machine model via the function `kernlab::ksvm`. This model is generally considered to have two tuning parameters: the SVM cost value and the kernel parameter `sigma`. For illustration purposes here, only the cost value will be tuned and the function `kernlab::sigest` will be used to estimate `sigma` during each model fit. This is automatically done by ksvm.

After the model is fit to the analysis set, the root-mean squared error (RMSE) is computed on the assessment set. __One important note__: for this model, it is critical to center and scale the predictors before computing dot products. We don't do this operation here, because `mlbench.friedman` simulates all of the predictors to be standardized uniform random variables.

Our function to fit the model and compute the RMSE is:

```{r}
pacman::p_load(kernlab)

# `object` will be an `rsplit` object from our `results` tibble
# `cost` is the tuning parameter

svm_rmse <- function(object, cost =1){
  y_col <- ncol(object$data)
  mod <- 
    svm_rbf(mode = "regression", cost = cost) %>% 
    set_engine("kernlab") %>% 
    fit(y ~ ., data = analysis(object))
  
  holdout_pred <- 
    predict(mod, assessment(object) %>% dplyr::select(-y)) %>% 
    bind_cols(assessmnent(object) %>% dplyr::select(y))
  rmse(holdout_pred, truth = y, estimate = .pred)$estimate
}

# In some case, we want to parameterize the function over the tuning parameter:
rmse_wrapper <- function(cost, object) svm_rmse(object, cost)

```


For the nested resampling, a model needs to be fit for each tuning parameter and each bootstrap split. To do this, create a wrapper:

```{r}
# object will be an rsplit object for the bootstrap samples 

tune_over_cost <- function(object){
  tibble(cost = 2 ^ seq(-2, 8, by = 1)) %>% 
    mutate(RMSE = map_dbl(cost, rmse_wrapper, object = object))
}

```


Since this will be called across the set of outer cross-validation splits, another wrapper is required:

```{r}
# `object` is an `rsplit` object in `results$inner_resamples` 
summarize_tune_results <- function(object) {
  # Return row-bound tibble that has the 25 bootstrap results
  map_df(object$splits, tune_over_cost) %>%
    # For each value of the tuning parameter, compute the 
    # average RMSE which is the inner bootstrap estimate. 
    group_by(cost) %>%
    summarize(mean_RMSE = mean(RMSE, na.rm = TRUE),
              n = length(RMSE))
}
```

Now that those functions are defined, we can execute all the inner resampling loops:

```{r}
tuning_results <- map(results$inner_resamples,
                      summarize_tune_results)
```

Alternatively, since these computations can be run in parallel, we can use the furr package. Instead of using `map()`, the function `future_map()` parallelizes the iterations using the [future package](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html). The `multisession` plan uses the local cores to process the inner resampling loop. The end results are the same as the sequential computations.

```{r}
library(furrr)
plan(multisession)

tuning_results <- future_map(results$inner_resamples,
                             summarize_tune_results)
```

The object `tuning_results` is a list of data frames for each of the 50 outer resamples. 

Let's make a plot of the averaged results to see what the relationship between the RMSE and the tuning parameters for each of the inner bootstrapping operations:

```{r}
library(scales)

pooled_inner <- tuning_results %>% 
  bind_rows()

best_cost <- function(dat){
  dat[which.min(dat$mean_RMSE)]
}

p <- pooled_inner %>% 
  ggplot(aes(X = cost, y = mean_RMSE))+
  scale_x_continuous(trans = "log2")+
  xlab("SVM Cost")+
  ylab("Inner RMSE")

for (i in 1:length(tuning_results))
  p <- p + 
  geom_line(data = tuning_results[[i]], alpha =.2)+
  geom_point(data = best_cost(tuning_results[[i]], pch = 16, alpha =.7))

p <- p + geom_smooth(data = pooled_inner,
                     se = FALSE)

p
```

Each gray line is a separate bootstrap resampling curve created from a different 90% of the data. The blue line is a LOESS smooth of all the results pooled together. 

To determine the best parameter estimate for each of the outer resampling iterations.

```{r}
cost_vals <- 
  tuning_results %>% 
  map_df(best_cost) %>% 
  select(cost)

results <- 
  bind_cols(results, cost_vals) %>% 
  mutate(cost = factor(cost, levels = paste(2 ^ seq(-2, 8, by = 1))))

ggplot(results, aes(x = cost)) + 
  geom_bar() + 
  xlab("SVM Cost") + 
  scale_x_discrete(drop = FALSE)

```

Most of the resamples produced an optimal cost value of 2.0, but the distribution is right-skewed due to the flat trend in the resampling profile once the cost value becomes 10 or larger.

Now that we have these estimates, we can compute the outer resampling results for each of the 50 splits using the corresponding tuning parameter value:

```{r}
results <- 
  results %>% 
  mutate(RMSE = map2_dbl(splits, cost, svm_rmse))

summary(results$RMSE)
```

The estimated RMSE for the model tuning process is 2.69. 

What is the RMSE estimate for the non-nested procedure when only the outer resampling method is used? For each cost value in the tuning grid, 50 SVM models are fit and their RMSE values are averaged. The table of cost values and mean RMSE estimates is used to determine the best cost value. The associated RMSE is the biased estimate.

```{r}
not_nested <- 
  map(results$splits, tune_over_cost) %>%
  bind_rows

outer_summary <- not_nested %>% 
  group_by(cost) %>% 
  summarize(outer_RMSE = mean(RMSE), n = length(RMSE))

outer_summary
#> # A tibble: 11 x 3
#>      cost outer_RMSE     n
#>     <dbl>      <dbl> <int>
#>  1   0.25       3.54    50
#>  2   0.5        3.11    50
#>  3   1          2.77    50
#>  4   2          2.62    50
#>  5   4          2.65    50
#>  6   8          2.75    50
#>  7  16          2.82    50
#>  8  32          2.82    50
#>  9  64          2.83    50
#> 10 128          2.83    50
#> 11 256          2.82    50

ggplot(outer_summary, aes(x = cost, y = outer_RMSE)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous(trans = 'log2') +
  xlab("SVM Cost") + ylab("RMSE")

```

The non-nested procedure estimates the RMSE to be 2.62. Both estimates are fairly close.

The approximately true RMSE for an SVM model with a cost value of 2.0 can be approximated with the large sample that was simulated at the beginning.

```{r}
finalModel <- ksvm(y ~ ., data = train_dat, C=2)
large_pred <- predict(finalModel, large_dat[, -ncol(large_dat)])

sqrt(mean(large_dat$y - large_pred)^2, na.rm=TRUE)
# 2.71
```

The nested procedure produces a closer estimate to the approximate truth but the non-nested estimate is very similar. 


## [Iterative Bayesian optimization of a classification model ](https://www.tidymodels.org/learn/work/bayes-opt/) 

Identify the best hyperparameters for a model using Bayesian optimization of iterative search. 

### Introduction 

```{r}
pacman::p_load(kernlab,
               tidymodels,
               tidyr)
```

Many of the examples for model tuning focus on grid search. for that method, all the candidate tuning parameter combinations are defined prior to evaluation. Alternatively, iterative search can be used to analyze the existing tuning parameter results and then predict which tuning parameters to try next.

There are a variety of methods for iterative search and the focus in this article is on Bayesian optimization. for more information on this method, these resources might be helpful. 

- [Practical bayesian optimization of machine learning algorithms (2012)](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=Practical+Bayesian+Optimization+of+Machine+Learning+Algorithms&btnG=). J Snoek, H Larochelle, and RP Adams. Advances in neural information.
-  [A Tutorial on Bayesian Optimization for Machine Learning (2018).](https://www.cs.toronto.edu/~rgrosse/courses/csc411_f18/tutorials/tut8_adams_slides.pdf) R Adams.
-  [Gaussian Processes for Machine Learning (2006)](http://www.gaussianprocess.org/gpml/). C E Rasmussen and C Williams.
-  Other articles!

### Cell segmentation revised

To demonstrate this approach to tuning models, the cell segmentation data from the [Getting Started]() article on resampling:

```{r}
 # Load data
library(modeldata)
data(cells)

set.seed(2369)
tr_te_split <- initial_split(cells %>% select(-case), prop = 3/4)
cell_train <- training(tr_te_split)
cell_test  <- testing(tr_te_split)

set.seed(1697)
folds <- vfold_cv(cell_train, v = 10)
```

### The tuning scheme

Since the predictors are highly correlated, we can use a recipe to convert the original predictors to principal component scores. There is also slight class imbalance in these data; about 64% of the data are poorly segmented. To mitigate this, the data will be down-sampled at the end of the pre-processing so that the number of poorly and well segmented cells occur with equal frequency. We can use a recipe for all this pre-processing, but the number of principal components will need to be tuned so that we have enough (but not too many) representations of the data.

```{r}
cell_pre_proc <-
  recipe(class ~ ., data = cell_train) %>%
  step_YeoJohnson(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors(), num_comp = tune()) %>%
  step_downsample(class)
```

In this analysis, we will use a support vector machine to model the data. Let's use a radial basis function (RBF) kernel and tune its main parameter ($\sigma$). Additionally, the main SVM parameter, the cost value, also needs optimization.

```{r}
svm_mod <- 
  svm_rbf(mode = "classification", 
          cost = tune(),
          rbf_sigma = tune()) %>% 
  set_engine("kernlab")
```

The two objects (the recipe and model) will be combined into a single object via the `workflow()` function from the [workflows](https://tidymodels.github.io/workflows/) package; this object will be used in the optimization process.

```{r}
svm_wflow <- 
  workflow() %>% 
  add_model(svm_mod) %>% 
  add_recipe(cell_pre_proc)
```

From this object, we can derive information about what parameters are slated to be tuned. A parameter set is derived by:

```{r}
svm_set <- parameters(svm_wflow)
svm_set
```

The default range for the number of PCA component is rather small for this data set. A member of the parameter set can be modified using the `update()` function. Let's constrain the search to one to twenty components by updating the `num_comp` parameter. Additionally, the lower bound of this parameter is set zero which specifies that the original predictor set should also be evaluated (i.e., with no PCA step at all):

```{r}
svm_set <- 
  svm_set %>% 
  update(num_comp = num_comp(c(0L, 20L)))
```

### Sequential tuning

Bayesian optimization is a sequential method that uses a model to predict new candidate parameters for assessment. When scoring potential parameter value, the mean and variance of performance are predicted. The strategy used to define how these two statistical quantities are used is defined by an _acquisition function_.

For example, one approach for scoring new candiates is to use a confidential bound. Suppose accuracy is being optimized. For a metric that we want to maximize, a lower bound can be used. The multiplier on the standard error (denoted as $\Kappa$) is a value that can be used to make trade-offs between exploration and exploitation. 

-  Exploration means that the search will consider candidates in untested space.
-  Exploitation focuses in areas where the previous best results occurred.
 
The variance predicted by the Bayesian model is mostly spatial variation; the value will be large for candidate values that are not close to values that have already been evaluated. If the standard error multiplier is high, the search process will be more likely to avoid areas without candidate values in the vicinity. 

We will use another acquisition function, _expected improvement_, that determines which candidates are likely to be helpful relative to the current best results. This is the default acquisition function. More information on these functions can be found in the package vignette. 

```{r}
set.seed(12)
search_res <-
  svm_wflow %>% 
  tune_bayes(
    resamples = folds,
    # To use non-default parameter ranges
    param_info = svm_set,
    # Generate five at semi-random to start
    initial = 5,
    iter = 50,
    # How to measure performance?
    metrics = metric_set(roc_auc),
    control = control_bayes(no_improve = 30, verbose = TRUE)
  )
```

The resulting tibble is stacked set of rows of the rsample object with an additional column for the iteration number:

```{r}
search_res 
```

As with grid search, we can summarize the results over rsamples:

```{r}
estimates <- 
  collect_metrics(search_res) %>% 
  arrange(.iter)

estimates 
```

The best perfromance of the initial set of candidate values was `AUC = 0.865`. The best results were achieved at iteration 45 with a correspondingg value of 0.878. The five best results are:

```{r}
show_best(
  search_res,
  metric = "roc_auc")
```

A plot of the search iterations can be created via:

```{r}
autoplot(search_res, 
         type = "performance")
```


There are many parameter combinations have roughly equivalent results. How did the parameters change over iterations? Since two of the parameters are usually treated on the log scale, we can use `mutate()` to transform them, and then construct a plot using `ggplot2`. 

```{r}
library(tidyr)

collect_metrics(search_res) %>%
  select(-.metric,-.estimator,-mean,-n,-std_err) %>%
  mutate(cost = log10(cost), 
         rbf_sigma = log10(rbf_sigma)) %>%
  pivot_longer(cols = c(-.iter),
               names_to = "parameter",
               values_to = "value") %>%
  ggplot(aes(x = .iter, y = value)) +
  geom_point() +
  facet_wrap( ~ parameter, scales = "free_y")

```


# Learn - develop custom modeling tools 

## [Create your own recipe step function](https://www.tidymodels.org/learn/develop/recipes/)

Write a new recipe step for data preprocessing. 

### Introduction

To use the code in this article, you will need to install the following packages.

```{r}
pacman::p_load(tidymodels, 
               modeldata)
```
There are many existing recipe steps in packages like recipes, themis, textrecipes, and others. A full list of steps in CRAN packages can be found [here](https://www.tidymodels.org/find/recipes/). However, you might need to define your own preprocessing operations; this article describes how to do that. If you are lookin for good examples of existing steps, we suggest looking at the code for [centering](https://github.com/tidymodels/recipes/blob/master/R/center.R) or [PCA](https://github.com/tidymodels/recipes/blob/master/R/pca.R) to start.

For check operations (e.g., check_class()), the process is vetry similar. Notes on this available at the end of this article.

The general process to follow is to:

1. Define a step constructor function
2. Create the minimal S3 methods for `prep()`, `bake()` and `print()`
3. Optionally add some extra methods to work with other tidymodels packages, such as `tunable()`, and `tidy()`. 

As an example, we will create a step for converting data into percentiles.

### A new step definition

Let's create a step that replaces the value of a variable with its percentile from the training set. The example data we will use from the modeldata package:

```{r}
data("biomass")
str(biomass) 

biomass_tr<- biomass[biomass$dataset == "Training",]
biomass_te <- biomass[biomass$dataset == "Testing",]
```

To illustrate the transformation with the `carbon` variable, note that the training set distribution of this variable with a vertical line below for the first value of the test set.

```{r warning=FALSE}
ggplot(biomass_tr, aes(x = carbon)) + 
  geom_histogram(binwidth = 5, col = "blue", fill = "blue", alpha = .5) + 
  geom_vline(xintercept = biomass_te$carbon[1], lty = 2)+
  hrbrthemes::theme_ft_rc()
```

Based on the training set, 42.1% of the data are less than a value of 46.35.  There are some applications where it might be advantageous to represent the predictor values as percentiles rather than their original values.

Our new step will do this computation for any numeric variables of interest. We will call this new recipe step `step_percentile()`. The code below is designed for illustration and not speed or best practices. We’ve left out a lot of error trapping that we would want in a real implementation

### Create the function

To start, there is a user-facing function. Let's call that `step_percentile()`. This is just a simple wrapper around a constrictor function, which defines the rules for any step that defines a percentile transformation. We will call this constructor `step_percentile_new()`.

The function `step_percentile` takes the same arguments as your function and simply adds it to a new recipe. The `...` signifies the variable selectors that can be used^[Towards the end of 2020, recipes will make the change to move to the tidyverse’s new selection style that does not use `...` to capture the selectors. See the tidyverse principles page for a discussion.].

```{r}
step_percentile <- function(
    recipe,
    ...,
    role = NA,
    trained = FALSE,
    ref_dist = NULL,
    options = list(probs = (0:100)/100, names = TRUE),
    skip = FALSE,
    id = rand_id("percentile"))
{
  ## The variable selectors are not immediately evaluated by using
  ##  the `quos()` function in `rlang`. `ellipse_check()` captures 
  ##  the values and also checks to make sure that they are not empty.  
  terms <- ellipse_check(...) 

  add_step(
    recipe, 
    step_percentile_new(
      terms = terms, 
      trained = trained,
      role = role, 
      ref_dist = ref_dist,
      options = options,
      skip = skip,
      id = id
    )
  )
}
```

You should always keep the first four arguments (`recipe` though `trained`) the same as listed above. Some notes:

- the `role` argument is used when you either 1) create new variables and want their role to be pre-set or 2) replace the existing variables with new values. The latter is what we will be doing and using `role = NA` will leave the existing role intact.
- `trained` is set by the pacakge when the estimation step has been run. You should default your function definition's argument to `FALSE`. 
-skip is a logical. Whenever a recipe is prepped, each step is trained and then baked. However, there are some steps that should not be applied when a call to `bake()` is used. For example, if a step is applied to the variables with roles of “outcomes”, these data would not be available for new samples.
id is a character string that can be used to identify steps in package code. `rand_id()` will create an ID that has the prefix and a random character sequence.

In order to calculate the percentile, the training data for the relevant coluns need to be saved. This daa will be saved in the `ref_dist` object. pprox() would be used when you want to save a grid of pre-computed percentiles from the training set and use these to estimate the percentile for a new data point.

We will use the `stats::quantile()` to compute the grid. However, we might also want to have control over the granularity of this grid, so the options argument will be used to define how that calculations is done. We can use the ellipses (aka ...) so that any options passed to `step_percentile()` that are not one of its arguments will then be passed to stats::quantile(). We recommend making a separate list object with the options and use these inside the function.

It is also important to consider if there are any main arguments to the step. For example, for spline-related steps such as `step_ns()`, users typically want to adjust the argument for the degress of freedom in the spline (e.g., `splines::ns(x, df)`). Rather letting users add `df` to the `options` argument:

- allows the important arguments to be main arguments to the step function
- Follow the tidymodels [conventions for naming arugmnets](https://tidymodels.github.io/model-implementation-principles/standardized-argument-names.html). Whenever possible, avoid jargon and keep common argument names. 

There are benefits for following these principles (as shown below)

### Initialize  a new object

Now, the constructor function can be created

The function cascade is :

```
step_percentile() callls recipes::add_step()
 |---> recipes::add_step() calls step_percentile_new()
 |---> step_percentile_new() calls recipes::step()
```

`step()` is a general constructor for recipes that mainly makes sure that the resulting step object is a list with an appropriate S3 class structure. Using 

```{r}
step_percentile_new <- 
  function(terms, role, trained, ref_dist, options, skip, id) {
    step(
      subclass = "percentile", 
      terms = terms,
      role = role,
      trained = trained,
      ref_dist = ref_dist,
      options = options,
      skip = skip,
      id = id
    )
  }
```

This constructor function should have no default argument values. Defaults should be set in the user-facing step object. 

### Define the percedure

You will need to create a new `prep()` method for your step's class. To do this, three aruments that the method should have are:

```
function(x, training, info = NULL)
```

where 

- `x` will be the `step_percentile` object,
- `training` will be a tibble that has the training set data, and
- `info` will also be a tibble that has information on the current set of data available. This information is updated as each step is evaluated by its specific `prep()` method so it may not have the variables from the original data. The columns in this tibble are `variable` (the variable name), `type` (currently either “numeric” or “nominal”), `role` (defining the variable’s role), and `source` (either “original” or “derived” depending on where it originated).

You can define other options as well.

The first thing you might want to do in the `prep()` function is to translate the specification listed in the `terms` argument to column names in the current data.














