---
title: "Tidymodels.org"
author: "Koji Mizumura"
date: "2020-04-30 - `r Sys.Date()`"
output: 
  rmdformats::readthedown:
    number_sections: yes
    fig_height: 10
    fig_width: 14
    highlight: kate
    toc_depth: 3
  word_document:
    toc: yes
    toc_depth: '4'
#    css: style.css
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  # fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```

# Learn 
## [Regression models two ways](https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/) 

### Introduction

To use the code in this article, you will need to install: `AmesHousing`, `glmnet`, `randomForest`, `ranger`, and `tidymodels`.

```{r}
library(AmesHousing)
library(glmnet)
library(randomForest)
library(ranger)
library(tidymodels)
```


We can create regression models with the tidymodels package `parsnip` to predict ontinuous or numeric quantities. Here, let’s first fit a random forest model, which does not require all numeric input (see discussion here) and discuss how to use `fit()` and `fit_xy()`, as well as data descriptors.

Second, let’s fit a regularized linear regression model to demonstrate how to move between different types of models using parsnip. 

### The Ames Housing Data

We’ll use the Ames housing data set to demonstrate how to create regression models using parsnip. First, set up the data set and create a simple training/test set split:

```{r}
ames <- make_ames()

set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price",
                            p = .75)

ames_train <- training(data_split)
ames_test <- testing(data_split)
```

The use of the test set here is only for illustration; normally in a data analysis, these data would be saved to the very end after many models have been evaluated. 

### Random Forest

We will start by fitting a random forest model to a small set of parameters. Let’s create a model with the predictors Longitude, `Latitude`, `Lot_Area`, `Neighborhood`, and `Year_Sold`. A simple random forest model can be specified via:

```{r}
rf_defaults <- rand_forest(mode = "regression")
rf_defaults
```

The model will be fit with the ranger package by default. Since we didn't add any extra arguments to `fit`, many of the arguments will be set to their defaults from the function `ranger::ranger()`. The help pages for the model function describe the default parameters and you can use the `translate()` function to check out such details.

The parsnip package provides two different interfaces to fit a model:

- the formula interface `fit()`, and
- the non-formula interface `fit_xy()`

Let's start with the non-formula interface:

```{r}
preds <- c("Longitude", "Latitude", "Lot_Area",
           "Neighborhood", "Year_Sold")

rf_xy_fit <- 
  rf_defaults %>% 
  set_engine("ranger") %>% 
  fit_xy(
    x = ames_train[, preds],
    y = log10(ames_train$Sale_Price)
  )

rf_xy_fit
```

The non-formula interface doesn't  do anything to the predictors before passing them to the underlying model function. This particular model does not require indicator variables (sometimes called "dummy variables") to be created to prior to fitting model.  Note that the output shows “Number of independent variables: 5”.

For regression models, we can use the basic `predict()` method, which returns a tibble with a column named `.pred`:

```{r}
test_results <- 
  ames_test %>% 
  select(Sale_Price) %>% 
  mutate(Sale_Price = log10(Sale_Price)) %>% 
  bind_cols(
    predict(rf_xy_fit, new_data = ames_test[,preds])
  )

test_results %>% 
  slice(1:5)

# summarize performance 
test_results %>% 
  metrics(truth = Sale_Price,
          estimate = .pred)
```

Note that:

- If the model required indicator variables, we would have to create them manually prior to using `fit()` (perhaps )
- We had to manually log the outcome prior to modeling

Now, for illustration, let's use the formula method using some new parameter values:

```{r}

rand_forest(mode = "regression",
            mtry = 3,
            trees = 1000) %>% 
  set_engine("ranger") %>% 
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  )
```

Suppose that we would like to use the `randomForest` package instead of ranger. To do so, the only part of the syntax that needs to change is the `set_engine()` argument:

```{r}
rand_forest(mode = "regression",
            mtry = 3,
            trees = 1000) %>% 
  set_engine("randomForest") %>% 
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

Look at the formula code that was printed out; one function uses the argument name `ntree` and the other uses `num.trees`. The parsnip models don’t require you to know the specific names of the main arguments.

Now suppose that we want to modify te value of `mtry` based on the number of predictors in the data. Usually, a good default value is `floor(sqrt(num_predictors))` but a pure bagging model requires an `mtry` value equal to the total number of parameters. There maybe cases where yo umay not know how many predictors are going to be present when the model will be fit (perhaps due to the generation of indicator variables or variable filter) so this might be difficult to know exactly ahead of time when you write you code. 

When the model it being fit by parsnip, data descriptors are made available. These attempt to let you know what you will have available when the model is fit. When a model object is created (say using `rand_forest()`), the values of the arguments that you give it are immediately evaluated unless you delay them. To delay the evaluation of any argument, you can use `rlang::expr()` to make an expression.

Two relevant data description for our example model are: 

- `.pred()`: the number of predictor variables in the data set that are associated with the predictors __prior to dummy variable creation__
- `.cols()`: the number of predictor columns after dummy variables (or other encodings) are created. 

Since ranger won't create indicator values, `.preds()` would be appropriate for `mtry` for a bagging model. For example, let's use an expression with the `.preds()` descriptor to fit a bagging model:

```{r}
rand_forest(mode = "regression", mtry = .preds(), trees = 1000) %>%
  set_engine("ranger") %>%
  fit(
    log10(Sale_Price) ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold,
    data = ames_train
  )
```

### Regularized regression 

A linear model might work for this data set as well. We can use the `linear_reg()` parsnip model. There are two engines that can perform  regularization/penalization, the glmnet and sparklyr packages. Let’s use the former here. The glmnet package only implements a non-formula method, but parsnip will allow either one to be used.

When regularization is used, the predictors should first be centered and scaled before being passed to the model. The formula method won’t do that automatically so we will need to do this ourselves. We’ll use the recipes package for these steps.

```{r}
norm_recipe <- 
  recipe(
    Sale_Price ~ Longitude + Latitude + Lot_Area + Neighborhood + Year_Sold, 
    data = ames_train
  ) %>%
  step_other(Neighborhood) %>% 
  step_dummy(all_nominal()) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  step_log(Sale_Price, base = 10) %>% 
  # estimate the means and standard deviations
  prep(training = ames_train, retain = TRUE)

# Now let's fit the model using the processed version of the data

glmn_fit <- 
linear_reg(penalty = 0.001, mixture = 0.5) %>% 
  set_engine("glmnet") %>%
  fit(Sale_Price ~ ., data = juice(norm_recipe))
glmn_fit
```

If `penalty` were not specified, all of the `lambda` values would be computed.

To get the predictions for this specific value of lambda (aka penalty):

```{r}
# First, get the processed version of the test set predictors
test_normalized <- bake(norm_recipe, 
                        new_data = ames_test,
                        all_predictors())

test_results <- 
  test_results %>% 
  rename(`random forest` = .pred) %>% 
  bind_cols(
    predict(glmn_fit, new_data = test_normalized) %>% 
      rename(glmnet = .pred)
  )

test_results

test_results %>% metrics(truth = Sale_Price, estimate = glmnet) 
#> # A tibble: 3 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 rmse    standard      0.132 
#> 2 rsq     standard      0.410 
#> 3 mae     standard      0.0956

test_results %>% 
  gather(model, prediction, -Sale_Price) %>% 
  ggplot(aes(x = prediction, y = Sale_Price)) + 
  geom_abline(col = "green", lty = 2) + 
  geom_point(alpha = .4) + 
  facet_wrap(~model) + 
  coord_fixed()
```

This final plot compares the performance of the random forest and regularized regression models.

## [Classification models using a neural network](https://www.tidymodels.org/learn/models/parsnip-nnet/)

### Introduction

To use the code in this article, you will need to install the following packages: keras and tidymodels. You will also need the python keras library installed (see `?keras::install_keras()`).

```{r}
library(tidyverse)
library(tidymodels)
library(keras)
```


We can create classification models with the tidymodels package parsnip to predict categorical quantities or class labels. Here, let’s fit a single classification model using a neural network and evaluate using a validation set. While the tune package has functionality to also do this, the parsnip package is the center of attention in this article so that we can better understand its usage.

### Fitting a neural network

Let’s fit a model to a small, two predictor classification data set. The data are in the modeldata package (part of tidymodels) and have been split into training, validation, and test data sets. In this analysis, the test set is left untouched; this article tries to emulate a good data usage methodology where the test set would only be evaluated once at the end after a variety of models have been considered.

```{r}
library(keras)
data(bivariate)
nrow(bivariate_train)
#> [1] 1009
nrow(bivariate_val)
#> [1] 300
```

A plot of the data shows two right-skewed predictors:

```{r}
ggplot(bivariate_train, aes(x = A, y = B, col = Class)) + 
  geom_point(alpha = .2)
```

Let’s use a single hidden layer neural network to predict the outcome. To do this, we transform the predictor columns to be more symmetric (via the s`tep_BoxCox()` function) and on a common scale (using `step_normalize()`). We can use recipes to do so:

```{r}
biv_rec <- 
  recipe(Class ~ ., data = bivariate_train) %>%
  step_BoxCox(all_predictors())%>%
  step_normalize(all_predictors()) %>%
  prep(training = bivariate_train, retain = TRUE)

# We will juice() to get the processed training set back

# For validation:
val_normalized <- bake(biv_rec, new_data = bivariate_val, all_predictors())
# For testing when we arrive at a final model: 
test_normalized <- bake(biv_rec, new_data = bivariate_test, all_predictors())
```

We can use the keras package to fit a model with 5 hidden units and a 10% dropout rate, to regularize the model:

```{r}
set.seed(57974)
nnet_fit <-
  mlp(epochs = 100, hidden_units = 5, dropout = 0.1) %>%
  set_mode("classification") %>% 
  # Also set engine-specific `verbose` argument to prevent logging the results: 
  set_engine("keras", verbose = 0) %>%
  fit(Class ~ ., data = juice(biv_rec))

nnet_fit
```

### Model performance

In parsnip, the `predict()` function can be used to characterize performance on the validation set. Since parsnip always produces tibble outputs, these can just be column bound to the original data:

```{r}
val_results <- 
  bivariate_val %>%
  bind_cols(
    predict(nnet_fit, new_data = val_normalized),
    predict(nnet_fit, new_data = val_normalized, type = "prob")
  )
val_results %>% slice(1:5)
#> # A tibble: 5 x 6
#>       A     B Class .pred_class .pred_One .pred_Two
#>   <dbl> <dbl> <fct> <fct>           <dbl>     <dbl>
#> 1 1061.  74.5 One   Two             0.473    0.527 
#> 2 1241.  83.4 One   Two             0.484    0.516 
#> 3  939.  71.9 One   One             0.636    0.364 
#> 4  813.  77.1 One   One             0.925    0.0746
#> 5 1706.  92.8 Two   Two             0.355    0.645

val_results %>% roc_auc(truth = Class, .pred_One)
#> # A tibble: 1 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 roc_auc binary         0.815

val_results %>% accuracy(truth = Class, .pred_class)
#> # A tibble: 1 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.737

val_results %>% conf_mat(truth = Class, .pred_class)
#>           Truth
#> Prediction One Two
#>        One 150  27
#>        Two  52  71
```

Let’s also create a grid to get a visual sense of the class boundary for the validation set.

```{r}
a_rng <- range(bivariate_train$A)
b_rng <- range(bivariate_train$B)
x_grid <-
  expand.grid(A = seq(a_rng[1], a_rng[2], length.out = 100),
              B = seq(b_rng[1], b_rng[2], length.out = 100))
x_grid_trans <- bake(biv_rec, x_grid)

# Make predictions using the transformed predictors but 
# attach them to the predictors in the original units: 
x_grid <- 
  x_grid %>% 
  bind_cols(predict(nnet_fit, x_grid_trans, type = "prob"))

ggplot(x_grid, aes(x = A, y = B)) + 
  geom_contour(aes(z = .pred_One), breaks = .5, col = "black") + 
  geom_point(data = bivariate_val, aes(col = Class), alpha = 0.3)

```

## Subsampling for class imbalances 

Improve model performance in imbalanced data sets through undersampling or oversampling.

### Introduction 

To use the code in this article, you will need to install the following packages: `discrim`, `klaR`, `readr`, `ROSE`, `themis`, and tidymodels.

```{r}
# pacman::p_install(c(discrim, klaR, ROSE, themis))

pacman::p_load(
  discrim,
  klaR,
  readr,
  ROSE,
  themis,
  tidymodels)
```


Subsampling a training set, either undersampling or oversampling the appropriate class or classes, can be a helpful approach to dealing with classification data where one or more classes occur very infrequently. In such a situation (without compensating for it), most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance.

This article describes subsampling for dealing with class imbalances. For better understanding, some knowledge of classification metrics like _sensitivity_, _specificity_, and _receiver operating characteristic_ curves is required. See Section 3.2.2 in Kuhn and Johnson (2019) for more information on these metrics.

### Simulated data

Consider a two-class problem where the first class has a very low rate of occurence. The data were simulated and can be imported into R usig the code below. 













