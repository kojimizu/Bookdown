---
title: "Tidymodels.org"
author: "Koji Mizumura"
date: "2020-04-30 - `r Sys.Date()`"
output: 
  word_document:
    toc: yes
    toc_depth: '4'
  rmdformats::readthedown:
    number_sections: yes
    fig_height: 10
    fig_width: 14
    highlight: kate
    toc_depth: 3
#    css: style.css
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  # fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```


```{r setup4, include=FALSE}
```

# Build a model {# Build-a-model}
## Introduction

How do you create a statistical model using tidymodels? IN this article, we will walk you through the steps. We start with data for modeling, learn how to specify and train models with different engines using the [parsnip package](parsnip package), and understand why these functions are desgined this way.

To use code in this article, you will need to install the following packages; readr, rstatnarm, and tidymodels. 

```{r}
# tictoc::tic()
pacman::p_load(tidymodels, readr)
# tictoc::toc()
```

## The Sea Urchins data

Let's use the data from Constable(1993) to explore how three different feeding regimes affect the size of sea urchins over time. The initial size of the sea urchins at the beginning of the experiment probably affects how big they grow as they are fed.

To start, let's rad our urchins data into R, which we’ll do by providing `readr::read_csv()` with a url where our CSV data is located (“https://tidymodels.org/start/models/urchins.csv”):

```{r}
urchins <- 
  # Data were assembled for a tutorial
  # at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
  read_csv("https://tidymodels.org/start/models/urchins.csv") %>% 
  # chnge the names to be a little more verbose
  setNames(c("food_regime", "initial_volume", "width")) %>% 
  # factors are very helpful for modeling, so we convert one column
  mutate(
    food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))
#> Parsed with column specification:
#> cols(
#>   TREAT = col_character(),
#>   IV = col_double(),
#>   SUTW = col_double()
#> )
```

```{r}
urchins
```

The urchins data is a [tibble](https://tibble.tidyverse.org/index.html). 
If you are new to tibbles, they best place to start is the tibbles chapter in R for Data Science. For each of the 72 urchins, we know their: 

- erimental feeding regime group (food_regime: either Initial, Low, or High),
- size in milliliters at the start of the experiment (initial_volume), and
- suture width at the end of the experiment (width).

As a first step in modeling, it’s always a good idea to plot the data:

```{r}
ggplot(urchins,
  aes(x = initial_volume,
      y = width,
      group = food_regime,
      col = food_regime))+
  geom_point()+
  geom_smooth(se = FALSE, #method = lm
              )+
  # hrbrthemes::theme_ft_rc()
  scale_color_viridis_d(option = "plasma", end = .7)
```

We can see that urchins that were larger in volume at the start of the experiment tended to have wider sutures at the end, but the slopes of the lines look different so this effect may depend on the feeding regime condition.

## Build and fit a model

A standard two-way analysis of variance (ANOVA) model makes sense for this dataset because we have continuous predictor and a categorical predictor. Since the slopes appear to be different for at least two of the feeding regimes, let’s build a model that allows for two-way interactions. Specifying an R formula with our variables in this way:

```{r eval=FALSE}
width ~ initial_volume + food_regime
```

allows our regression model depending on initial volume to have separate slopes and intercepts for each food regime.

For this kind of model, ordinary least squares is a good initial approach. With tidymodels, we start by specifying the functional form of the model that we want using the parsnip package. Since there is a numeric outcome and the model should be linear with slopes and intercepts, the model type is “linear regression”. We can declare this with:

```{r}
linear_reg()
```

That is pretty underwhelming since, on its own, it doesn’t really do much. However, now that the type of model has been specified, a method for fitting or training the model can be stated using the engine. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method. For example, to use ordinary least squares, we can set the engine to be `lm`:

```{r}
linear_reg() %>% 
  set_engine("lm")
```

The documentation page for `linear_reg()` lists the possible engines. We’ll save this model object as `lm_mod`.

```{r}
lm_mod <- 
  linear_reg() %>%
  set_engine("lm")
```

From here, the model can be estimated or trained using the `fit()` function.

```{r}
lm_fit <- 
  lm_mod %>% 
  fit(width ~ initial_volume*food_regime, data = urchins)

lm_fit  
```

Perhaps our analysis requires a description of the model parameter estimates and their statistical properties. Although the `summary()` function for lm objects can provide that, it gives the results back in an unwieldy format. Many models have a `tidy()` method that provides the summary results in a more predictable and useful format (e.g. a data frame with standard column names):

```{r}
tidy(lm_fit)
```

## Use a model to predict

This fitted object `lm_fit` has the `lm` model output built-in, which you can access with `lm_fit$fit`, but there are some benefits to using the fitted parsnp model object when it comes to predicting. 

Suppose that, for a publication, it would be particularly interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml. To create such a graph, we start with some new example data that we will make predictions for, to show in our graph:

```{r}
new_points <- expand.grid(
  initial_volume = 20,
  food_regime = c("Initial", "Low", "High")
)
```

To get one predicted results, we can use the `predict()` funtio to find the mean values at 20ml.

It is also important to communicate the variability, so we also need to find the predicted confidence intervals. If we had used `lm() `to fit the model directly, a few minutes of reading the documentation page for `predict.lm()` would explain how to do this. However, if we decide to use a different model to estimate urchin size (spoiler: we will!), it is likely that a completely different syntax would be required.

Instead, with tidymodels, the types of predicted values are standardized so that we can use the same syntax to get these values.

First, let’s generate the mean body width values:

```{r}
mean_pred <- 
  predict(lm_fit, new_data = new_points)

mean_pred
```

When making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format:

```{r}
conf_int_pred <- predict(lm_fit, 
                         new_data = new_points, 
                         type = "conf_int")
conf_int_pred
#> # A tibble: 3 x 2
#>   .pred_lower .pred_upper
#>         <dbl>       <dbl>
#> 1      0.0555      0.0729
#> 2      0.0499      0.0678
#> 3      0.0870      0.105

# Now combine: 
plot_data <- 
  new_points %>% 
  bind_cols(mean_pred) %>% 
  bind_cols(conf_int_pred)

# library(hrbrthemes)
# library(thematic)
# thematic_on(
#   bg = "#222222", fg = "white", accent = "#0CE3AC",
#   font = font_spec("Oxanium", scale = 1.25)
# )

# and plot:
ggplot(plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, 
                    ymax = .pred_upper),
                width = .2) + 
  labs(y = "urchin size")

```

# Model with a different engine

Every one on your team is happy with that plot except that one person who jut read their first book on Bayesian analysis. They are interested in knowing if the results would be different if the model were estimated using a Bayesian approach. In such an analysis, a prior distribution needs to be declared for each model parameter that represents the possible values of the parameters (before being exposed to the observed data). After some discussion, the group agrees that the priors should be bell-shaped but, since no one has any idea what the range of values should be, to take a conservative approach and make the priors wide using a Cauchy distribution (which is the same as a t-distribution with a single degree of freedom).

The documentation on the rstanarm package shows us that the `stan_glm()` function can be used to estimate this model, and that the function arguments that need to be specified are called prior and prior_intercept. It turns out that `linear_reg()` has a stan engine. Since these prior distribution arguments are specific to the Stan software, they are passed as arguments to `parsnip::set_engine()`. After that, the same exact `fit()` call is used:

```{r}
# see the prior distribution
prior_dist <- rstanarm::student_t(df = 1)
set.seed(123)

# make the parsnip model
bayes_mod <- 
  linear_reg() %>% 
  set_engine("stan",
             prior_intercept = prior_dist,
             prior = prior_dist)

# train the model
bayes_fit <- 
  bayes_mod %>% 
  fit(width ~ initial_volume*food_regime, data = urchins)

print(bayes_fit, digits = 5)
```

This kind of Bayesian analysis (like many models) involves randomly generated numbers in its fitting procedure. We can use `set.seed()` to ensure that the same (pseudo-)random numbers are generated each time we run this code. The number 123 isn’t special or related to our data; it is just a “seed” used to choose random numbers.

To update the parameter table, the `tidy() `method is once again used:

```{r}
tidy(bayes_fit, intervals = TRUE)
```

A goal of the tidymodels packages is that the interfaces to common tasks are standardized (as seen in the `tidy()` results above). The same is true for getting predictions; we can use the same code even though the underlying packages use very different syntax:

```{r}
bayes_plot_data <- 
  new_points %>% 
  bind_cols(predict(bayes_fit, new_data = new_points)) %>% 
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

bayes_plot_data

ggplot(bayes_plot_data, 
       aes(x = food_regime))+
  geom_point(aes(y =.pred))+
  geom_errorbar(aes(ymin = .pred_lower,
                    ymax = .pred_upper),
                width = .2)+
  labs(y = "urchin size")+
  ggtitle("Bayesian model with t(1) prior distribution")
```

## Why does it work that way?

The extra step of defining the model using a function like linear_reg() might seem superfluous since a call to lm() is much more succinct. However, the problem with standard modeling functions is that they don’t separate what you want to do from the execution. For example, the process of executing a formula has to happen repeatedly across model calls even when the formula does not change; we can’t recycle those computations.

Also, using the tidymodels framework, we can do some interesting things by incrementally creating a model (instead of using single function call). Model tuning with tidymodels uses the specification of the model to declare what parts of the model should be tuned. That would be very difficult to do if linear_reg() immediately fit the model.

If you are familiar with the tidyverse, you may have noticed that our modeling code uses the magrittr pipe (%>%). With dplyr and other tidyverse packages, the pipe works well because all of the functions take the data as the first argument. For example:

```{r}
urchins %>% 
  group_by(food_regime) %>% 
  summarize(med_vol = median(initial_volume))
```

whereas the modeling code uses the pipe to pass around the moel object:

```{r}
bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)
```

This may seem jarring if you have used dplyr, but it is extremely similar to how ggplot2 operates:

```{r}
ggplot(urchins,
       aes(initial_volume, width)) +      # returns a ggplot object 
  geom_jitter() +                         # same
  geom_smooth(method = lm, se = FALSE)   # same                    
```

# Process your data with recipes

## Introduction
In our [build a model article](#Build-a-model), we learned how to specify and train models with different engines using the [parsnip package](https://tidymodels.github.io/parsnip/). In this article, we will explorer another tidymodels packages recipes, which is designed to help you preprocess your data before  traing your model. Recipes are built as a series of preprocess steps, such as:

- converting qualitative predictors to indicate variables(also known as dummy variables)
- transforming data to be on a different scale (e.g., taking the logarithm of a variable)
- transforming whole groups of predictors together,
- extracting key features from raw variables (e.g., getting the day of the week out of a date variable)

and so on. If you are familiar with R’s formula interface, a lot of this might sound familiar and like what a formula already does. Recipes can be used to do many of the same things, but they have a much wider range of possibilities. This article shows how to use recipes for modeling.

To use code in this article, you will need to install the following packages: `nycflights13`, `skimr`, and `tidymodels`.

```{r}
pacman::p_load(tidymodels,   # recipes package
               nycflights13, # flight data
               skimr)        # for variables summaries
```

## The New York City Fligt Data

Let’s use the `nycflights13` data to predict whether a plane arrives more than 30 minutes late. This data set contains information on 325,819 flights departing near New York City in 2013. Let’s start by loading the data and making a few changes to the variables:

```{r}
set.seed(123)

flight_data <- 
  flights %>% 
  mutate(
    # Convert the arrivial delay to a factor
    arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
    arr_delay = factor(arr_delay),
    # we will use the date (not date-time) in the recipes below
    date = as.Date(time_hour)
  ) %>% 
  # Include the weather data
  inner_join(weather, by = c("origin", "time_hour")) %>% 
  # ONly rtain the specific columns we will use 
  select(dep_time, flight, origin, dest, arr_time, distance,
         carrier, date, arr_delay, time_hour) %>% 
  # Exclude missing data
  na.omit() %>% 
  # For creating models, it is better to have qualitative columns
  # Encoded as factors (instead of character strings)
  mutate_if(is.character, as.factor)
```

We can see that about 16% of the flights in this data set arrived more than 30 minutes late.

```{r}
flight_data %>% 
  skimr::skim()

flight_data %>% 
  count(arr_delay) %>% 
  mutate(prop = n/sum(n))
```

Before we start building op our recipe, let's take a quick look at a few specific variables that will be important for both preprocessing and modeling.

First, notice that the variable we created called `arr_delay` is a factor variable; it is important that our outcome variable for training a logistic model is a factor.

```{r}
glimpse(flight_data)
```

Second, there are two variables that we don’t want to use as predictors in our model, but that we would like to retain as identification variables that can be used to troubleshoot poorly predicted data points. These are `flight`, a numeric value, and `time_hour`, a date-time value.

Third, there are 104 flights destinations contained in `dest` and 16 distinc `carrier`s.

```{r}
flight_data %>% 
  skimr::skim(dest, carrier) 
```

Because we’ll be using a simple logistic regression model, the variables dest and carrier will be converted to [dummy variables](https://bookdown.org/max/FES/creating-dummy-variables-for-unordered-categories.html). However, some of these values do not occur very frequently and this could complicate our analysis. We’ll discuss specific steps later in this article that we can add to our recipe to address this issue before modeling.

## Data splitting 

To get started, let’s split this single dataset into two: a training set and a testing set. We’ll keep most of the rows in the original dataset (subset chosen randomly) in the training set. The training data will be used to fit the model, and the testing set will be used to measure model performance.

To do this, we can use the `rsample` package to create an object that contains the information on how to split the data, and then two more rsample functions to create data frames for the training and testing sets:

```{r}
# Fix the random numbers by setting the seed 
# This enables the analysis to be reproducible when random numbers are used 
set.seed(555)
# Put 3/4 of the data into the training set 
data_split <- initial_split(flight_data, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data <- testing(data_split)
```

## Create recipe and role

To get started, let’s create a recipe for a simple logistic regression model. Before training the model, we can use a recipe to create a few new predictors and conduct some preprocessing required by the model.

Let’s initiate a new recipe:

```{r eval=FALSE}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) 
```

The `recipe()` function as we used it here has two arguments:

- A formula:Any variable on the left-hand side of the tilde (`~`) is considered the model outcome (here, `arr_delay`). On the right-hand side of the tilde are the predictors. Variables may be listed by name, or you can use the dot (`.`) to indicate all other variables as predictors.
- The data: A recipe is associated with the data set used to create the model. This will typically be the training set, so `data = train_data` here. Naming a data set doesn’t actually change the data itself; it is only used to catalog the names of the variables and their types, like factors, integers, dates, etc.

Now we can add roles to this recipe. We can use the `update_role()` function to let recipes know that `flight` and `time_hour` are variables with a custom role that we called "ID" (a role can have any character value). Whereas our formula included all variables in the training set other than arr_delay as predictors, this tells the recipe to keep these two variables but not use them as either outcomes or predictors.

```{r}
flights_rec <- 
  recipe(arr_delay ~., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID")
```

This step of adding roles to a recipe is optional; the purpose of using it here is that those two variables can be retained in the data but not included in the model. This can be convenient when, after the model is fit, we want to investigate some poorly predicted value. These ID columns will be available and can be used to try to understand what went wrong.

To get the current set of variables and roles, use the `summary()` function:

```{r}
summary(flights_rec)
```

## Create features

Now we can start adding steps onto our recipe using the pipe operator. Perhaps it is reasonable for the date of the flight to have an effect on the likelihood of a late arrival. A little bit of __feature engineering__ might go a long way to improving our model. How should the date be encoded into the model? The date column has an R date object so including that column “as is” will mean that the model will convert it to a numeric format equal to the number of days after a reference date:

```{r}
flight_data %>% 
  distinct(date) %>% 
  mutate(numeric_date = as.numeric(date))
```

It’s possible that the numeric date variable is a good option for modeling; perhaps the model would benefit from a linear trend between the log-odds of a late arrival and the numeric date variable. However, it might be better to add model terms derived from the date that have a better potential to be important to the model. For example, we could derive the following meaningful features from the single date variable:

- the day of the week
- the month, and
- whether or not he date corresponds to our recipe:

```{r}
flight_rec <- 
  recipe(arr_delay ~., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, features = c("dow", "month")) %>% 
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date)
```

What do each of these steps do? 

- With `step_date()`, we created two new factor columns with the appropriate day of the week and the month. 
- With `step_holiday()`, we created a binary variable indicating whether the current date is a holiday or not. The argument value of `timeDate::listHolidays("US)` uses the timeDate package to list the 17 standard US holidays.
- With `step_rm()`, we remove the original date variable since we no longer wait it in the model.

Next, we will turn our attention to the variable types of our predictors. Because we plan to train a logistic regression model, we know that predictors will ultimately need to be numeric, as opposed to factor variables. In other words, there may be  a difference in how we store our data (in factors inside a data frame), and how the underlying equations require them (a purely numeric matrix).

For factors like `dest` and `origin`, [standard practice](https://bookdown.org/max/FES/creating-dummy-variables-for-unordered-categories.html) is to convert them into dummy or indicator variables to make them numeric. These are binary values for each level of the factor. For example, our origin variable has values of `"EWR"`, `"JFK"`, and `"LGA"`. The standard dummy variable encoding, shown below, will create two numeric columns of the data that are 1 when the originating airport is `"JFK"` or "`LGA"` and zero otherwise, respectively.

But, unlike the standard model formula methods in R, a recipe __does not__ automatically create these dummy variables for you; you’ll need to tell your recipe to add this step. This is for two reasons. First, many models do not require numeric predictors, so dummy variables may not always be preferred. Second, recipes can also be used for purposes outside of modeling, where non-dummy versions of the variables may work better. For example, you may want to make a table or a plot with a variable as a single factor. For those reasons, you need to explicitly tell recipes to create dummy variables using `step_dummy()`:

```{r}
flights_rec <- 
  recipe(arr_delay ~., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>%
  step_date(date, features = c("dow", "month")) %>% 
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date) %>% 
  step_dummy(all_nominal(), -all_outcomes())
```

Here, we did something different than before: instead of applying a step to an individual variable, we used [selectors](https://tidymodels.github.io/recipes/reference/selections.html) to apply this recipe step to several variables at once.

- The first selector, `all_nominal()`, selects all variables that are either factors or characters.
- The second selector, -`all_outcomes()` removes any outcome variables from this recipe step.

With these two selectors together, our recipe step above translates to:

> Create dummy variables for all of the factor or character columns unless they are outcomes. 

At this stage in the recipe, this step selects the `origin`, `dest` and `carrier` variables. It also includes two new variables, `date_dow` and `date_month`, that were created by earlier `step_date()`.

More generally, the recipe selectors mean that you don't always have to apply step to individual variables one at a time. Since a recipe knows the _variable type_ and role of each column, they can also be selected (or dropped) using this information.

We need one final step to add to our recipe. Since carrier and dest have some infrequently occurring values, it is possible that dummy variables might be created for values that don’t exist in the training set. For example, there is one destination that is only in the test set:

```{r}
test_data %>% 
  distinct(dest) %>% 
  anti_join(train_data)
```

When the recipe is applied to the training set, a column is made for LEX but it will contain all zeros. This is a zero-variance predictor that has no information within the column. 

While some R functions will not produce an error for such predictors, it usually causes warning and other issue. `step_nz()` will remove columns from the data when the training set data has a single value, so it is added to the recipe after `step_dummy()`.

```{r}
flights_rec <- 
  recipe(arr_delay ~ ., data = train_data) %>% 
  update_role(flight, time_hour, new_role = "ID") %>% 
  step_date(date, features = c("dow", "month")) %>% 
  step_holiday(date, holidays = timeDate::listHolidays("US")) %>% 
  step_rm(date) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors())
```

Now we have created a specification of what should be done with the data. How do we use the recipe we made? 

## Fit a model with a recipe

Let's use logistic regression to model the flight data. As we saw in Build a Model, we start by building a model specification using the parsnip package. 

```{r}
lr_mod <- 
  logistic_reg() %>% 
  set_engine("glm")
```

We will want to use our recipe across several steps as we train and test our model. We will 

1. __Process the recipe using the training set__: This involves any estimation or calculations based on the training set. For our recipe, the training set will be used to determine which predictors should be converted to dummy variables and which predictors will have zero-variance in the training set, and should be slated for removal.

2. __Apply the recipe to the training set__: We create the final predictor set on the training set. 

3. __Apply the recipe to the test set__: We create the final predictor set on the test set. Nothing is recomputed and no information from the test set is used here; the dummy variable and zero-variance results from the training set are applied to the test set.

To simplify this process, we can use a model workflow, which pairs a model and recipe together. This is a straightforward approach because different recipes are often needed for different models, so when a model and recipe are bundled, it becomes easier to train and test workflows. We’ll use the [workflows package](https://tidymodels.github.io/workflows/) from tidymodels to bundle our parsnip model(`lr_mod`) with our recipe(`flights_rec`)

```{r}
library(workflows)

flights_wflow <- 
  workflows::workflow() %>% 
  workflows::add_model(lr_mod) %>% 
  workflows::add_recipe(flights_rec)

flights_wflow
```

Now, there is a single function that can be used to prepare the recipe and train the model from the resulting predictors:

```{r}
flights_fit <- 
  flights_wflow %>% 
  fit(data = train_data)
```

This object has the finalized recipe and fitted model objects inside. You may want to extract the model or recipe objects from the workflow. To do this, you can use the helper functions `pull_workflow_fit()` and `pull_workflow_recipe()`. For example, here we pull the fitted model object then use the broom::tidy() function to get a tidy tibble of model coefficients:

```{r}
flights_fit %>% 
  pull_workflow_fit() %>% 
  tidy()
```

## Use a trained workflow to predict

Our goal was to predict whether a plane arrives more than 30 minutes late. We have just:

1. Built the model (`lr_mod`),
2. Created a preprocessing recipe (`flights_rec`),
3. Bundled the model and recipe (`flights_wflow`), and
4. Trained our workflow using a single call to `fit()`.

The next step is to use the trained workflow (`flights_fit`) to predict with the unseen test data, which we will do with a single call to `predict()`. The `predict()` method applies the recipe to the new data, then passes them to the fitted model.

```{r}
predict(flights_fit, test_data)
```

Because our outcome variable here is a factor, the output from `predict()` returns the predicted class: late versus on_time. But, let’s say we want the predicted class probabilities for each flight instead. To return those, we can specify `type = "prob"` when we use `predict()`. We’ll also bind the output with some variables from the test data and save them together:

```{r}
flights_pred <- 
  predict(flights_fit, test_data, type = "prob") %>% 
  bind_cols(test_data %>% select(arr_delay, time_hour, flight)) 

# The data look like: 
flights_pred
```

Now that we have a tibble with our predicted class probabilities, how will we evaluate the performance of our workflow? We can see from these first few rows that our model predicted these 5 on time flights correctly because the values of `.pred_on_time` are `p > .50`. But we also know that we have 81,454 rows total to predict. We would like to calculate a metric that tells how well our model predicted late arrivals, compared to the true status of our outcome variable, `arr_delay`.

Let’s use the area under the [ROC curve](https://bookdown.org/max/FES/measuring-performance.html#class-metrics) as our metric, computed using `roc_curve()` and `roc_auc()` from the [yardstick package](https://tidymodels.github.io/yardstick/). 

To generate a ROC curve, we need the predicted class probabilities for late and on_time, which we just calculated in the code chunk above. We can create the ROC curve with these values, using `roc_curve()` and then piping to `the autoplot()` method:

```{r}
flights_pred %>% 
  roc_curve(truth = arr_delay, .pred_late) %>% 
  autoplot()
```

Similarly, `roc_auc()` estimates the area under the curve:

```{r}
flights_pred %>% 
  roc_auc(truth = arr_delay, .pred_late)
```

Not too bad! We leave it to the reader to test out this workflow without this recipe. You can use `workflows::add_formula(arr_delay ~ .`) instead of `add_recipe()` (remember to remove the identification variables first!), and see whether our recipe improved our model’s ability to predict late arrivals.

# Evaluate your model with resamples

# Introduction

So far, we have built a model and preprocessed data with a recipe. We also introduced [workflows](https://www.tidymodels.org/start/recipes/#fit-workflow) as a way to bundle a [parsnip model](https://tidymodels.github.io/parsnip/) and [recipe](https://tidymodels.github.io/recipes/) together. Once we have a model trained, we need a way yo measure how well that model predicts new dat. This tutorial explains how to characterize model performance based on resampling statistics.

To use code in this article, you need to install the following packages.

```{r}
library(tidymodels)
library(modeldata)
```

## The cell image data

Let’s use data from Hill, LaPan, Li, and Haney (2007), available in the modeldata package, to predict cell image segmentation quality with resampling. To start, we load this data into R:

```{r}
data(cells, package = "modeldata")
cells
```

We have data for 2019 cells, with 58 variables. The main outcome variable of interest for us here is called class, which you can see is a factor. But before we jump into predicting the class variable, we need to understand it better. Below is a brief primer on cell image segmentation.

## Predicting image segmentation quality

Some biologists conduct experiments on cells. In drug discovery, a particular type of cell can be treated with either a drug or control and then observed to see what the effect is (if any). A common approach for this kind of measurement is cell imaging. Different parts of the cells can be colored so that the locations of a cell can be determined.

For example, in top panel of this image of five cells, the green color is meant to define the boundary of the cell (coloring something called the cytoskeleton) while the blue color defines the nucleus of the cell.

Using these colors, the cells in an image can be segmented so that we know which pixels belong to which cell. If this is done well, the cell can be measured in different ways that are important to the biology. Sometimes the shape of the cell matters and different mathematical tools are used to summarize characteristics like the size or “oblongness” of the cell.

The bottom panel shows some segmentation results. Cells 1 and 5 are fairly well segmented. However, cells 2 to 4 are bunched up together because the segmentation was not very good. The consequence of bad segmentation is data contamination; when the biologist analyzes the shape or size of these cells, the data are inaccurate and could lead to the wrong conclusion.

A cell-based experiment might involve millions of cells so it is unfeasible to visually assess them all. Instead, a subsample can be created and these cells can be manually labeled by experts as either poorly segmented (`PS`) or well-segmented (`WS`). If we can predict these labels accurately, the larger data set can be improved by filtering out the cells most likely to be poorly segmented.

## Back to the cells data

The `cells` data has class labels for 2019 cells - each is labeled as either poorly sgmented (`PS`) or well-segmented (`WS`). Each also has a total of 56 predictors based on automated image analysis measurements. 

For example, `avg_inten_ch_1` is the mean intensity of the data contained in the nucleus, `area_ch_1` is the total size of the cell, and so on (some predictors are fairly arcane in nature).

```{r}
cells
```

The rates of the claases are somewhat imbalanced; there are more poorly segmented cells than well-segmented cells:

```{r}
cells %>% 
  count(class) %>% 
  mutate(prop = n/sum(n))

cells %>% 
  ggplot(aes(x = angle_ch_1, y = area_ch_1, col = class))+
  geom_point()
```

## Data splitting 

In our previous article,  we started by splitting our data. It is common when beginning a modeling project to separate the data set into two partitions:

- The _training set_ is used to estimate parameters, compare models and feature engineering techniques, tune models, etc.
- The _test set_ is held in reserve until the end of the project, at which point there should only be one or two models under serious consideration. It is used as an unbiased source for measuring final model performance.


There are different ways to these partitions of the data. The most common approach is to use a random sample. Suppose that one quarter of the data were reserved for the test set. Random sampling would randomly select 25% for the test set and use the remainder for the training set. We can use the rsample package for this purpose.

Since random sampling uses random numbers, it is important to set the random number seed. This ensures that the random numbers can be reproduced at a later time (if needed).

The function `rsample::initial_split()` takes the original data and saves the information on how to make the partitions. In the original analysis, the authors made their own training/test set and that information is contained in the column case. To demonstrate how to make a split, we’ll remove this column before we make our own split:

```{r}
set.seed(123)
cell_split <- initial_split(
  cells %>% select(-case),
  strata = class
)
```

Here we used the strata argument, which conducts a stratified split. This ensures that, despite the imbalance we noticed in our class variable, our training and test data sets will keep roughly the same proportions of poorly and well-segmented cells as in the original data. After the initial_split, the `training()` and `testing()` functions return the actual data sets.

```{r}
cell_train <- training(cell_split)
cell_test <- testing(cell_split)

nrow(cell_train)
nrow(cell_train) / nrow(cells)

# training set proportions by class
cell_train %>% 
  count(class) %>% 
  mutate(prop = n/sum(n))

# test set proportions by class
cell_test %>% 
  count(class) %>% 
  mutate(prop = n/sum(n))
```

The majority of the modeling work is then conducted on the training set data.
To note, there is no missing value on this data, thus no missing data both on Training and Test sets.

```{r}
cells %>% 
  Amelia::missmap()

dim(cells)
```


## Modeling 

[Random forest models](https://bradleyboehmke.github.io/HOML/random-forest.html) are ensembles of [decision trees](https://bradleyboehmke.github.io/HOML/DT.html). A large number of decision tree models are created for the ensemble based on slightly different versions of the training set. When creating the individual decision trees, the fitting process encourages them to be as diverse as possible. The collection of trees are combined into the random forest model and, when a new sample is predicted, the votes from each tree are used to calculate the final predicted value for the new sample. For categorical outcome variables like class in our cells data example, the majority vote across all the trees in the random forest determines the predicted class for the new sample.

One of the benefits of a random forest model is that it is very low maintenance; it requires very little preprocessing of the data and the default parameters tend to give reasonable results. For that reason, we won’t create a recipe for the `cells` data.

At the same time, the number of trees in the ensemble should be large (in the thousands) and this makes the model moderately expensive to compute.

To fit a random forest model on the training set, let’s use the parsnip package with the `ranger` engine. We first define the model that we want to create:

```{r}
# random forest model
rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

gbm_mod <- 
  parsnip::boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```

Starting with this parsnip model object, the `fit()` function can be used with a model formula. Since random forest models use random numbers, we again set the seed prior to computing:

```{r}
set.seed(234)
rf_fit <- 
  rf_mod %>% 
  fit(class ~., data = cell_train)

rf_fit
```

This new `rf_fit` object is our fitted model, trained on our training data set.
gbm_fit is created apart from the original article.

```{r}
gbm_fit <- 
  gbm_mod %>% 
    fit(class ~., data = cell_train)

gbm_fit
```

## Estimating performance

During a modeling project, we might create a variety of different models. To choose between them, e need to consider how well these models do, as measured by some performance statistics. In our example in this article, some options we could use are:

- the area under the Receiver Operating Characteristic (ROC) curve, and
- the overall classification accuracy

The ROC curve uses the class probability estimates to give us a sense of performance across the entire set of potential probability cutoffs. Overall accuracy uses the hard class predictions to measure performance. The hard class predictions tell us whether our model predicted `PS` or `WS` for each cell. But, behind those predictions, the model is actually estimating a probability. A simple 50% probability cutoff is used to categorize a cell as poorly segmented.

The [yardstick package](https://tidymodels.github.io/yardstick/) has functions for computing both of these measures called `roc_auc()` and `accuracy()`.

At first glance, it might seem like a good idea to use the training set data to compute these statistics. (This is actually a very bad idea.) Let’s see what happens if we try this. To evaluate performance based on the training set, we call the `predict()` method to get both types of predictions (i.e. probabilities and hard class predictions).

```{r}
rf_training_pred <- 
  predict(rf_fit, cell_train) %>% 
  bind_cols(predict(rf_fit, cell_train, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(cell_train %>% 
              select(class))
```

Using the yardstick functions, this model has spectacular results, so spectacular that you might be starting to get suspicious:

```{r}
rf_training_pred %>%                # training set predictions
  roc_auc(truth = class, .pred_PS)
#> # A tibble: 1 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 roc_auc binary          1.00
rf_training_pred %>%                # training set predictions
  accuracy(truth = class, .pred_class)
#> # A tibble: 1 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.993
```

Now that we have this model with exceptional performance, we proceed to the test set. Unfortunately, we discover that, although our results aren’t bad, they are certainly worse than what we initially thought based on predicting the training set:

```{r}
rf_testing_pred <- 
  predict(rf_fit, cell_test) %>% 
  bind_cols(predict(rf_fit, cell_test, type = "prob")) %>% 
  bind_cols(cell_test %>% 
              select(class))
```

```{r}
# Test set predictions
rf_testing_pred %>% 
  roc_auc(truth = class, .pred_PS)

# Test set predictions
rf_testing_pred %>% 
  accuracy(truth = class, .pred_class)
```

```{r}
# GBM implementation 
# please refer - https://tidymodels.github.io/parsnip/reference/boost_tree.html

gbm_training_pred <- 
  predict(gbm_fit, cell_train) %>% 
  bind_cols(predict(rf_fit, cell_train, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(cell_train %>% 
              select(class))

gbm_training_pred %>%                # training set predictions
  roc_auc(truth = class, .pred_PS)
#> # A tibble: 1 x 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 roc_auc binary          1.00
gbm_training_pred %>%                # training set predictions
  accuracy(truth = class, .pred_class)

gbm_training_pred
```

### What happended here? 

There are several reasons why training set statisics like the ones shown in this section can be unrealistically optimistic: 

- Models like random forests, neural networks or and other black-box methods can essentially memorize the training set. Re-predicting that same set should always result in nearly perfect results. 
- The training set does not have the capacity to be a good arbiter of performance. It is not an independent piece of information; predicting the training set can only reflect what the model already knows. 

To understand that second point better, think about an analogy from teaching. Suppose you give a class a test, then give them answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first set.

## Resampling to the rescue

Resampling methods, such as cross-validation and the bootstrap, are emprical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data are used for creating the model and a different subset is used to measure performance. Resampling is always used with the training set. This schematic from [Kuhn and Johnson (2019)](https://bookdown.org/max/FES/resampling.html) illustrates data usage for resampling methods:


```{r}
knitr::include_graphics("resampling.svg")
```

In the first level of this diagram, you see what happens when you use `rsample::initial_split()`, which splits the original data into training and test sets. THen, the training set is chosen for resampling, and the test set is held out. 

Let’s use 10-fold cross-validation (CV) in this example. This method randomly allocates the 1515 cells in the training set to 10 groups of roughly equal size, called “folds”. For the first iteration of resampling, the first fold of about 151 cells are held out for the purpose of measuring performance. This is similar to a test set but, to avoid confusion, we call these data the assessment set in the tidymodels framework.

The other 90% of the data (about 1363 cells) are used to fit the model. Again, this sounds similar to a training set, so in tidymodels we call this data the analysis set. This model, trained on the analysis set, is applied to the assessment set to generate predictions, and performance statistics are computed based on those predictions.

In this example, 10-fold CV moves iteratively through the folds and leaves a different 10% out each time for model assessment. At the end of this process, there are 10 sets of performance statistics that were created on 10 data sets that were not used in the modeling process. For the cell example, this means 10 accuracies and 10 areas under the ROC curve. While 10 models were created, these are not used further; we do not keep the models themselves trained on these folds because their only purpose is calculating performance metrics.

The final resampling estimates for the model are the __averages__ of the performance statistics replicates. For example, suppose for our data the results were:

From these resampling statistics, the final estimate of performance for this random forest model would be 0.903 for the area under the ROC curve and 0.833 for accuracy.

These resampling statistics are an effective method for measuring model performance without predicting the training set directly as a whole.

## Fit a model with resampling

To generate these results, the first step is to create a resampling object using `resample`. There are [several resampling methods]() implemented in rsample; cross-validation folds can be created using `vfold_cv()`:

```{r}
set.seed(345)
folds <- vfold_cv(cell_train, v = 10)
folds
```

The list column for `splits` contains the information on which rows belong in the analysis and assessment sets. There are functions that can be used to extract the individual resampled data called `analysis()` and `assessment()`.

However, the tune package contains high-level functions that can do the required computations to resample a model for the purpose of measuring performance. You have several options for building an object for resampling:

- Resample a model specification preprocessed with a formula or recipe, or 
- Resample a [`workflow()`](https://tidymodels.github.io/workflows/) that bundles together a model specification and formula/recipe

For this example, let's use a `workflow()` that bundles together the random forest model and a formula, since we are not using a recipe. Whichever of these options you use, the syntax to `fit_resamples()` is very similar to `fit()`:

```{r}
rf_wf <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(class ~ .)

set.seed(456)
rf_fit_rs <- 
  rf_wf %>% 
  tune::fit_resamples(folds)

gbm_wf <- 
  workflow() %>% 
  add_model(gbm_mod) %>% 
  add_formula(class ~ .)

gbm_fit_rs <- 
  gbm_wf %>% 
  tune::fit_resamples(folds)
```

```{r}
rf_fit_rs
```

The results are similar to the `folds` results with some extra columns. The column .metrics contains the performance statistics created from the 10 assessment sets. These can be manually unnested but the tune package contains a number of simple functions that can extract these data:

```{r}
# df <- tibble(x = c(1, 1, 1, 2, 2, 3), y = 1:6, z = 6:1)
# df
# df %>% 
#   nest(data = c(y, z)) %>% 
#   unnest(data)
# df %>% chop(c(y, z)) %>% 
#   unnest(y)

rf_fit_rs %>% 
  unnest(.metrics)

collect_metrics(rf_fit_rs)
collect_metrics(gbm_fit_rs)
```

Think about these values we now have for accuracy and AUC. These performance metrics are now realistic (i.e. lower) than our ill-advised first attempt at computing performance metrics in the section above. If we wanted to try different model types for this data set, we could more confidently compare performance metrics computed using resampling to choose between models. Also, remember that at the end of our project, we return to our test set to estimate final model performance. We have looked at this once already before we started using resampling, but let’s remind ourselves of the results:

```{r}
rf_testing_pred %>%  # test set predictions
  roc_auc(truth = class, .pred_PS)

rf_testing_pred %>%  # test set prediction
  accuracy(truth = class, .pred_class)

```

The performance metrics from the test set are much closer to the performance metrics computed using resampling than our first (“bad idea”) attempt. Resampling allows us to simulate how well our model will perform on new data, and the test set acts as the final, unbiased check for our model’s performance.

# Tune model parameters 

## Introduction

Some model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called __hyperparameters__. Some examples of hyperparameters include the number of predictors hat are sampled at splits in a tree-based model (we call this `mtry` in tidymodels) or the learning rate in a boosted tree model (we call this `learn_rate`). Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called __tuning__.

To use code in this article, you will need to install the following packages: mdoeldata, rpart, tidymodels, and vip.

```{r}
library(tidymodels)  # for the tune package, along with the rest of tidymodels

# Helper packages
library(modeldata)   # for the cells data
library(vip)         # for variable importance plots
```

## The cell image data, revisited

In our previous [evaluate your model with resampling article](https://www.tidymodels.org/start/resampling/), we introduced a dataset of images of cells that were labeled by experts as well-segmented (`WS`) or poorly segmented (`PS`). WE trained a random forest model to predict which images are segmented well vs poorly, so that a biologist could filter our poorly segmented cell images in their analysis. We used resamplig to estimate the performance of our model on this data. 

```{r}
data(cells, package = "modeldata")
cells
```

## Predicting image segmentation, but better

Random forest models are a tree-based ensemble method, and typically perform well with default hyperparameters. However, the accuracy of some other tree-based models, such as boosted tree models or decision tree models, can be sensitive to the values of hyperparameters. In this article, we will train a decision tree model. There are several hyperparameters for __decision tree__ models that can be tuned for better performance. Let’s explore:

- complexity parameter (which we call `cost_complexity` in tidymodels) for the tree, and
- the maximum `tree_depth`

Tuning these hyperparameters can improve model performance because decision tree models are prone to overfitting. This happens because single tree models tend to fit the training data too well — so well, in fact, that they over-learn patterns present in the training data that end up being detrimental when predicting new data.

Tuning the value of `cost_complexity` helps by pruning back our tree. It adds a cost, or penalty, to error rates of more complex trees; a cost closer to zero decreases the number tree nodes pruned and is more likely to result in an overfit tree. However, a high cost increases the number of tree nodes pruned and can result in the opposite problem—an underfit tree. Tuning tree_depth, on the other hand, helps by stopping our tree from growing after it reaches a certain depth. We want to tune these hyperparameters to find what those two values should be for our model to do the best job predicting image segmentation.

We will tune the model hyperparameters to avoid overfitting. Tuning the value of `cost_compexity` helps by pruning back our tree. It adds a cost, or penalty, to error rates of more complex trees; a cost closer to zero decreases the number tree nodes pruned and is more likely to result in an overfit tree. However, a high cost increases the number of tree nodes pruned and can result in the opposite problem—an underfit tree. Tuning tree_depth, on the other hand, helps by stopping our tree from growing after it reaches a certain depth. We want to tune these hyperparameters to find what those two values should be for our model to do the best job predicting image segmentation.

Before we start the tuning process, we split our data into training and testing sets, just like when we trained the model with one default set of hyperparameters. As before, we can use strata = class if we want our training and testing sets to be created using stratified sampling so that both have the same proportion of both kinds of segmentation.

```{r}
set.seed(123)
cell_split <- initial_split(cells %>% 
                              select(-case),
                            strata = class)
cell_train <- training(cell_split)
cell_test <- testing(cell_split)
```

## Tuning hyperparameters

Let's start with the parsnip package, using `decision_tree()` model with the `rpart` engine. To tune the decision tree hyperparameters `cost_complexity` and `tree_depth`, we create a model specification that identifies whichi hyperparameters we plan to tune.

```{r}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec

tune_gbm_mod <- 
  parsnip::boost_tree(
    mtry = tune(),
    trees = tune(),
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

tune_gbm_mod
```

Think of `tune()` here as a placeholder. After the tuning process, we will select a single numeric value for each of these hyperparameters. For now, we specify our parsnip model object and identify the hyperparameters we will `tune()`.

We can’t train this specification on a single data set (such as the entire training set) and learn what the hyperparameter values should be, but we can train many models using resampled data and see which models turn out best. We can create a regular grid of values to try using some convenience functions for each hyperparameter:

```{r}
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
)

tree_grid

# gbm_grid <- grid_regular(
#   mtry(),
#   trees(),
#   tree_depth(),
#   learn_rate()
# )
```

Here, you can see all 5 vaues of `cost_compexity` ranging up to 0.1. These values get repeated for each of the 5 values of `tree_depth`.

```{r}
tree_grid %>% 
  count(tree_depth)
```

Armed with our grid filled with 25 candidate decision tree models, let's create cross-validation folds for tuning:

```{r}
set.seed(234)
cell_folds <- vfold_cv(cell_train)

cell_folds
```

Tuning in tidymodels requires a resampled object created with the rsample package. 

## Model tuning with a grid

We are ready to tune. Let's use [tune_grid()](https://tidymodels.github.io/tune/reference/tune_grid.html) to fit models at all the different values we chose for each tuned parameter. THere are sveral options for building the object for tuning: 

- Tune a model specification along with a recipe or model, or
- Tune a `workflow()` that bundles together a model specification and a recipe or model preprocessor.

Here we use a `workflow()` with a straightfoward formula; if this model required more involved data preprocessing, we could use `add_recipe()` intead of `add_formula()`.

```{r}
set.seed(345)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(class ~ .)

tree_res <- 
  tree_wf %>% 
  tune_grid(
    resamples = cell_folds,
    grid = tree_grid
    )

tree_res
#> #  10-fold cross-validation 
#> # A tibble: 10 x 4
#>    splits             id     .metrics          .notes          
#>  * <list>             <chr>  <list>            <list>          
#>  1 <split [1.4K/152]> Fold01 <tibble [50 × 5]> <tibble [0 × 1]>
#>  2 <split [1.4K/152]> Fold02 <tibble [50 × 5]> <tibble [0 × 1]>
#>  3 <split [1.4K/152]> Fold03 <tibble [50 × 5]> <tibble [0 × 1]>
#>  4 <split [1.4K/152]> Fold04 <tibble [50 × 5]> <tibble [0 × 1]>
#>  5 <split [1.4K/152]> Fold05 <tibble [50 × 5]> <tibble [0 × 1]>
#>  6 <split [1.4K/151]> Fold06 <tibble [50 × 5]> <tibble [0 × 1]>
#>  7 <split [1.4K/151]> Fold07 <tibble [50 × 5]> <tibble [0 × 1]>
#>  8 <split [1.4K/151]> Fold08 <tibble [50 × 5]> <tibble [0 × 1]>
#>  9 <split [1.4K/151]> Fold09 <tibble [50 × 5]> <tibble [0 × 1]>
#> 10 <split [1.4K/151]> Fold10 <tibble [50 × 5]> <tibble [0 × 1]>
```


Once we have our tuning results, we can both explore them through visualization and then select the best result. THe function `collect_metrics()` gives us a tidy tibble with all the rsults. We had 25 candidate models and two metrics `accuracy` and `roc_auc` and we get a row for each `metric` and model. 

```{r}
tree_res %>% 
  collect_metrics()
```

We might get more out of plotting these results:

```{r}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

We can see that our stubbiest tree, with a depth of 1, is the worst model according to both metrics and across all candidate values of `cost_complexity`. Our deepest tree, with a depth of 15, did better. However, the best tree seems to be between these values with a tree depth of 4. The `show_best()` function shows us the top 5 candidate models by default:

```{r}
tree_res %>% 
  show_best("roc_auc")
```

We can also use the `select_best()` function to pull out the single set of hyperparameter values for our best decision tree model:

```{r}
best_tree <- 
  tree_res %>% 
  select_best("roc_auc")

best_tree
```
 
These are the values for `tree_depth` and `cost_complexity` that maximize AUC in this data set of cell images.

## Finalizing our model

We can update (or "finalize") our workflow object `tree_wf` with the values from `select_best()`. 

```{r}
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf
```

## Exploring results 

Let's fit this final model to the training data. What does the decision tree look like? 

```{r}
final_tree <- 
  final_wf %>% 
  fit(data = cell_train)

final_tree
```

This `final_tree` object has the finalized, fitted model object inside. You may want to extract the model object from the workflow. To do this, you can use the helper function `pull_workflow_fit()`.

For example, perhaps we woul also like to understand what variables are important in this final model. We can use the [vip](https://koalaverse.github.io/vip/) package to estimate variable importance. 

```{r eval=FALSE}
library(vip)

final_tree %>% 
  pull_workflow_fit() %>% 
  vip::vip()
```

These are the automated image analysis measurements that are the most important in driving segmentation quality predictions

## The last fit

Finally, let's return to our test data and estimte the model performance we expect to see with new data. We can use the function `last_fit()` with our finalized model; this function fits the finalized model on the full training data set and evaluates the finalized model on the testing data.

```{r}
final_fit <- 
  final_wf %>% 
  last_fit(cell_split)

final_fit %>% 
  collect_metrics()

final_fit %>% 
  collect_predictions() %>% 
  roc_curve(class, .pred_PS) %>% 
  autoplot()
```

The performance metric from the test set indicates that we did not overfit during our tuning procedure. 

We leave it to the reader to xplore whether you can tune a different decision tree hyperparameter. You can explore the [reference docs](https://www.tidymodels.org/find/parsnip/#models), or use the `args()` function to see which parsnip object arguments are available:

```{r}
args(decision_tree)
```

You could tune the other hyperparameter we didn’t use here, min_n, which sets the minimum n to split at any node. This is another early stopping method for decision trees that can help prevent overfitting. Use this [searchable table](https://www.tidymodels.org/find/parsnip/#model-args) to find the original argument for min_n in the rpart package (hint). See whether you can tune a different combination of hyperparameters and/or values to improve a tree’s ability to predict cell segmentation quality.

# [A predictive modeling case study](https://www.tidymodels.org/start/case-study/)

## Introduction
 
Each of the four previous _Get Started_ articles has focused on a single task related to modeling. Along the way, we also introduced core packages in the tidymodels ecosystem and some of the key functions you will need to start working with models. 

In this final case setudy, we will use all of the previous articles as a foundation to build a predictive model from beggining to end with data on hotel stays.

```{r}
library(tidymodels)  

# Helper packages
library(readr)       # for importing data
library(vip)         # for variable importance plots
```

## The hotel booking data

Let's use hotel bookings data from [Antonio, Almeida, and Nunes (2019)](https://doi.org/10.1016/j.dib.2018.11.126) to predict which hotel stays included children and/or babies, based on the other characteristics of the stays such as which hotel the guests stay at, how much they pay, etc. This was also a [#TidyTuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11) dataset with a [data dictionary](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-02-11#data-dictionary) you may want to look over to learn more about the variables. We’ll use a slightly edited version of the dataset for this case study.

To start, let’s read our hotel data into R, which we’ll do by providing `readr::read_csv()` with a url where our CSV data is located (“https://tidymodels.org/start/case-study/hotels.csv”):

```{r}
hotels <- 
  read_csv('https://tidymodels.org/start/case-study/hotels.csv') %>% 
  mutate_if(is.character, as.factor)

dim(hotels)
```

In the original paper, the authors caution that the distribution of many variables (such as number of adults/children, room type, meals bought, country of origin of the guests, and so forth) is different for hotel stays that were canceled versus not canceled. This makes sense because much of that information is gathered (or gathered again more accurately) when guests check in for their stay, so canceled bookings are likely to have more missing data than non-canceled bookings, and/or to have different characteristics when data is not missing. Given this, it is unlikely that we can reliably detect meaningful differences between guests who cancel their bookings and those who do not with this dataset. To build our models here, we have already filtered the data to include only the bookings that did not cancel, so we’ll be analyzing hotel stays only.

```{r}
glimpse(hotels)
```

We will build a model to predict which actual hotel stays included children and/or babies, and which did not. Our outcome variable `childre`n is a factor variable with two levels:

```{r}
hotels %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))
```
 
We can see that children were only in 8.1% of the reservations. This type of class imbalance can often wreak havoc on an analysis. While there are several methods for combating this issue using recipes (search for steps to `upsample` or `downsample`) or other more specialized packages like themis, the analyses shown below analyze the data as-is.

## Data splitting & resampling 

For a data splitting strategy, let’s reserve 25% of the stays to the test set. As in our Evaluate your model with resampling article, we know our outcome variable children is pretty imbalanced so we’ll use a stratified random sample:

```{r}
set.seed(123)
splits <- initial_split(hotels, strata = children)

hotel_other <- training(splits)
hotel_test <- testing(splits)

# training set proportions by children 
hotel_other %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))

# test set proportions by children 
hotel_test %>% 
  count(children) %>% 
  mutate(prop = n/sum(n))

```

In our articles so far, we’ve relied on 10-fold cross-validation as the primary resampling method using `rsample::vfold_cv()`. This has created 10 different resamples of the training set (which we further split into analysis and assessment sets), producing 10 different performance metrics that we then aggregated.

For this case study, rather than using multiple iterations of resampling, let’s create a single resample called a validation set. In tidymodels, a validation set is treated as a single iteration of resampling. This will be a split from the 37,500 stays that were not used for testing, which we called `hotel_other`. This split creates two new datasets:

- the set held out for the purpose of measuring performance, called the validation set and
- the remaining data used to fit the model, called the training set

We will use the `validation_split()` function to allocate 20% of the `hotel_other` stays to the validation set and 30,000 stays to the training set. This means that ur model performance metrics will be computed on a single set of 7,500 hotel stays. This is fairly large, so the amount of data should provide enough precision to be a reliable indicator for how well each model predicts the outcome with a single iteration of resampling.

```{r}
set.seed(234)
val_set <- validation_split(hotel_other, 
                            strata = children, 
                            prop = 0.80)
val_set
#> # Validation Set Split (0.8/0.2)  using stratification 
#> # A tibble: 1 x 2
#>   splits             id        
#>   <named list>       <chr>     
#> 1 <split [30K/7.5K]> validation

```

This function, like `initial_split()` has the same `strata` argument, which uses stratified sampling to create the resample. This means that we will have roughly the same proportions of hotel stays with and without children in our new validation and training sets, as compared to the original `hotel_other` proportions.

## A First model: penalized logistic regression

Since our outcome variable children is categorical, logistic regression would be a good first model to start. Let’s use a model that can perform feature selection during training. The [glmnet](https://cran.r-project.org/web/packages/glmnet/index.html) R package fits a generalized linear model via penalized maximum likelihood. This method of estimating the logistic regression slope parameters uses a penalty on the process so that less relevant predictors are driven towards a value of zero. One of the glmnet penalization methods, called the [lasso method](https://en.wikipedia.org/wiki/Lasso_(statistics)) can actually set the predictor slopes to zero if a large enough penalty is used.

### Build the model

To specify a penalized logistic regression model that uses a feature selection penalty, let' use the parsnip package with the [glmnet engine](https://www.tidymodels.org/find/parsnip/):

```{r}
lr_model <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

lr_model
```

We will set the penalty argument to `tune()` as a placeholder for now. This is a model hyperparameter that we will tune to find the best value for making predictions with our data. Setting mixture to a value of one means that the glmnet model will potentially remove irrelevant predictors and choose a simpler model.

## Create the receipe 

Let's create a recipe to define the preprocessing steps we need to prepare our hotel stays data for this moel. It might make sense to crete a set of date-based predictors that reflect important components related to the arrival date. We have already introduced a [number of useful receipe steps](https://www.tidymodels.org/start/recipes/#features) for creating features from dates:

- `step_date()` creates predictors for the year, month, and day of the week
- `step_holiday()` generates a set pf omdocatpr varoab;es fpr specific holidays. Although we don't know where theses two hotels are located, we do know that the countries for origin for most staays are based on Europe
- `step_rm()` removes vairables: here we will use it to remove theoriginal date variables ince we no longer wait in the model.

Additionally, all categorical predictors (e.g., `distribution_channel`, `hotel`) should be converted to dummy variables, and all numeric predictors need to be centered and scaled. 

- `step_dummu()`  converts characters or factors (i.e., nominal variables) into one or more numeric binary model terms for the levels of the original data.
- `step_zv()` removes indicator variables that only contain a single unique value (e.g. all zeros). This is important because, for penalized models, the predictors should be centered and scaled.
- `step_normalize()` centers and scales numeric variables.

Putting all these steps together into a recipe for penalized logistic regression model, we have

```{r}
holidays <- c("AllSouls", "AshWednesday", "ChristmasEve", "Easter", 
              "ChristmasDay", "GoodFriday", "NewYearsDay", "PalmSunday")

lr_recipe <- 
  recipe(children ~., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date, holidays = holidays) %>%
  step_rm(arrival_date) %>% 
  step_dummy(all_predictors()) %>% 
  step_normalize(all_predictors())
```

### Create the workflow

As we introduced in Preproess your data with recipe, let’s bundle the model and recipe into a single `workflow() `object to make management of the R objects easier:

```{r}
lr_workflow <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(lr_recipe)
```

### Creatt the grid for tuning

Before we fit this model, we need to set up a grid of `penality` values to tune. In our Tune model parameters article, we used `dials::grid_regular()` to create an expanded grid based on a combination of two hyperparameters. Since we have only one hyperparameter to tune here, we can set the grid up manually using a one-column tibble with 30 candidate values:

```{r}
lr_reg_grid <- 
  tibble(penalty = 10^seq(-4,-1, length.out = 30))

# lowest penalty values 
lr_reg_grid %>% 
  top_n(-5)

# highest penalty values 
lr_reg_grid %>% 
  top_n(5)
```

## Train and tune the moel

Let's use `tune::tune_grid()` to train these 30 penalized logistic regression models. We will alos save the validation set predictions (via the call to `control_grid()`) so that diagnostic information can be availble after the model fit. The area under the ROC curve will be used to quantify how well the model performs across a continuum of event thresholds (recall that the event rate—the proportion of stays including children— is very low for these data).

```{r}
lr_res <- 
  lr_workflow %>% 
  tune_grid(val_set,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

It might be easier to visualize the validation set metrics by plotting the area under the ROC curve against the range of penalty values. 

```{r}
lr_plot <- 
  lr_res %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean))+
  geom_point()+
  geom_line()+
  ylab("Area under the ROC curve")+
  scale_x_log10(labels = scales::label_number())

lr_plot
```

This plots shows us that model performance is generally better at the smaller penalty values. This suggests that the majority of the predictors are important to the model. We also see a steep drop in the area under the ROC curve towards the highest penalty values. This happens because a large enough penalty will remove all predictors from the model, and not surprisingly predictive accuracy plummets with no predictors in the model (recall that an ROC AUC value of 0.50 means that the model does no better than chance at predicting the correct class).

Our model performance seems to plateau at the smaller penalty values, so going by the `roc_auc` metric alone could lead us to multiple options for the “best” value for this hyperparameter:

```{r}
top_models <- 
  lr_res %>% 
  show_best("roc_auc", n = 15) %>% 
  arrange(penalty)
```

Every candidate model ni this tibble likely includes more predictor variables than the model in the row below it. If we used `select_best()`, it would return candidate model 8 with a penalty value of 0.00053, shown with the dotted line below.

However, we may want to choose a penalty value further along the x-axis, closer to where we start to see the decline in model performance. For example, candidate model 12 with a penalty value of 0.00137 has effectively the same performance as the numerically best model, but might eliminate more predictors. This penalty value is marked by the solid line above. In general, fewer irrelevant predictors is better. If performance is about the same, we’d prefer to choose a higher penalty value.

Let's select this value and visualize the validation set ROC curve:

```{r}
lr_best <- 
  lr_res %>% 
  collect_metrics() %>% 
  arrange(penalty) %>% 
  slice(12)
lr_best
```

```{r}
lr_auc <- 
  lr_res %>% 
  collect_predictions(parameters = lr_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Logistic Regression")

autoplot(lr_auc)
```

The level of performance generated by this logistic regression model is good, but not groundbreaking. Perhaps the linear nature of the prediction equation is too limiting for this dataset. As a next step, we might consider a highly non-linear model generated using a tree-based ensemble method. 

## A second model: tree-based ensemble

An effective and low-maintenance modeling technique is a random forest. This model was also used in our [Evaluate your model with resampling article](https://www.tidymodels.org/start/resampling/).Compared to logistic regression, a random forest model is more flexible. A random forest is an ensemble model typically made up of thousands of decision trees, where each individual tree sees a slightly different version of the training data and learns a sequence of splitting rules to predict new data. Each tree is non-linear, and aggregating across trees makes random forests also non-linear but more robust and stable compared to individual trees. Tree-based models like random forests require very little preprocessing and can effectively handle many types of predictors (sparse, skewed, continous, categorical etc.).

### Build the model and improve training time 

Although the default hyperparametes for random forests tend to give reasonable results, we will plan to tune two hyperparametes that we think could improve performance. 

Unfortunately, random forest models can be computationally expensive to train and to tune. The computations required for model tuning can usually be easily parallelized to improve training time. The tune package can do [parallel processing](https://tidymodels.github.io/tune/articles/extras/optimizations.html#parallel-processing) for you, and allows users to use multiple cores or separate machines to fit models. 

But, here we are using a single validation set, so parallelization isn’t an option using the tune package. For this specific case study, a good alternative is provided by the engine itself. The ranger package offers a built-in way to compute individual random forest models in parallel. To do this, we need to know the the number of cores we have to work with. We can use the parallel package to query the number of cores on your own computer to understand how much parallelization you can do:

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
all_cores
```

We have 8 cores to work with. We can passe this information to the ranger engine when we set up our parsnip `rand_forest()` model. To enable parallel processing, we can pass engine-specific arguments like `num.threads` to ranger when we set the engine:

```{r}
rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger", num.threads = cores) %>% 
  set_mode("classification")
```

This works well in this modeling context, but it bears repeating: if you use any other resampling methods, let tune do the parallel processing for you - we typically do not recommend relying on the modeling engine. 

In this model, we used `tune()` as a placeholder for the `mtry` and `min_n` argument values, because these are our two hyperparameters that we will tune. 

### Create the recipe and workflow

Unlike penalized logistic regression models, random forest models do not require dummy or normalized predictor variables. Nevertheless, we want to do some feature engineering again with our `arrival_date` variable. As before, the date predictor is engineered so that the random forest model does not need to work hard to tease these potential patterns from the data.

```{r}
rf_recipe <- 
  recipe(children ~., data = hotel_other) %>% 
  step_date(arrival_date) %>% 
  step_holiday(arrival_date) %>% 
  step_rm(arrival_date)
```

Adding this recipe to our parsnip model gives us a new workflow for predicting whether a hotel stay included children and/or babies as guests with a random forest.

```{r}
rf_workflow <- 
  workflow() %>% 
  add_model(rf_mod) %>% 
  add_recipe(rf_recipe)
```


### Train and tune the model 

When we set up our parsnip model, we chose two hyperaparameters for tuning:

```{r}
rf_mod
#> Random Forest Model Specification (classification)
#> 
#> Main Arguments:
#>   mtry = tune()
#>   trees = 1000
#>   min_n = tune()
#> 
#> Engine-Specific Arguments:
#>   num.threads = cores
#> 
#> Computational engine: ranger

# show what will be tuned
rf_mod %>%    
  parameters()  
#> Collection of 2 parameters for tuning
#> 
#>     id parameter type object class
#>   mtry           mtry    nparam[?]
#>  min_n          min_n    nparam[+]
#> 
#> Model parameters needing finalization:
#>    # Randomly Selected Predictors ('mtry')
#> 
#> See `?dials::finalize` or `?dials::update.parameters` for more information.

```

The `mtry` hyperparameter sets the number of predictor variables that each node in the decision tree sees and can learn about, so it can range from 1 to the total number of features present; when `mtry` = all possible features, the moel is the same as bagging decision trees. The `min_n` hyperparameter sets the minimum `n` to split at any node. 

We will use a space-filing design to tune, with 25 candidate models:

```{r}
set.seed(345)
rf_res <- 
  rf_workflow %>% 
  tune_grid(val_set, 
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
# I created pre-processing data to finalize unknown parameter: mtry
```

The message printed above "Creating pre-processing data to finalize unknown parameter: mtry" is related to the size of the data set. Since `mtry` depends on the number of predictors in the data set, `tune_grid()` determines the upper bound for `mtry` once it receives the data. 

Here are our top 5 random forest models, out of the 25 candidates: 

```{r}
rf_res %>% 
  show_best(metric = "roc_auc")
#> # A tibble: 5 x 7
#>    mtry min_n .metric .estimator  mean     n std_err
#>   <int> <int> <chr>   <chr>      <dbl> <int>   <dbl>
#> 1     8     7 roc_auc binary     0.933     1      NA
#> 2     6    18 roc_auc binary     0.933     1      NA
#> 3     9    12 roc_auc binary     0.932     1      NA
#> 4     3     3 roc_auc binary     0.932     1      NA
#> 5     5    35 roc_auc binary     0.932     1      NA

```

Right away, we see that these values for area under the ROC look more promising than our top model using penalized logistic regression, which yielded an ROC AUC of 0.881. 

Plotting the results of the tuning process highlights that both `mtry` (number of predictors at each node) and `min_n` (minimum number of data points required to keep splitting) should be fairly small to optmize performance. However, the range of the y-axis ndicates that the model is very robust to the choice of these parameter values — all but one of the ROC AUC values are greater than 0.90.

```{r}
autoplot(rf_res)
```

Let's select the best model according to the ROC AUC metric. Our final tuning parameter values are:

```{r}
rf_best <- 
  rf_res %>% 
  select_best(metric = "roc_auc")
rf_best
#> # A tibble: 1 x 2
#>    mtry min_n
#>   <int> <int>
#> 1     8     7

```

To calculate the data needed to plot the ROC curve, we use `collect_predictions()`. This is only possible after tuning with `control_grid(save_pred = TRUE)`. In the output, you can see the two columns that hold our class probabilities for predicting hotel stays including and not including children. 

```{r}
rf_res %>% 
  collect_predictions()
#> # A tibble: 187,475 x 7
#>   id         .pred_children .pred_none  .row  mtry min_n children
#>   <chr>               <dbl>      <dbl> <int> <int> <int> <fct>   
#> 1 validation       0.00158       0.998    11    12     7 none    
#> 2 validation       0.000167      1.00     13    12     7 none    
#> 3 validation       0.000286      1.00     31    12     7 none    
#> 4 validation       0.000168      1.00     32    12     7 none    
#> 5 validation       0.00075       0.999    36    12     7 none    
#> # … with 1.875e+05 more rows
```

To filter the predictions for only our best random forest model, we can use the `parameters` argument and pass it our tibble with the best hyperparameter values from tuning, which we called `rf_best`:

```{r}
rf_auc <- 
  rf_res %>% 
  collect_predictions(parameters = rf_best) %>% 
  roc_curve(children, .pred_children) %>% 
  mutate(model = "Random Forest")
```

Now, we can compare the validation set ROC curves for our top penalized logistic regression model and random forest model:

```{r}
bind_rows(rf_auc, lr_auc) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 1.5, alpha = 0.8) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  scale_color_viridis_d(option = "plasma", end = .6)
```

THe random forest is uniformly better across event probability thresholds.

## The last fit

Our goal was to predict which hotel stays included children and/or babies. The random forest model clearly performed better than the penalized logistic regression model, and would be our best bet for predicting hotel stays with and without children. After selecting our best model and hyperparameter values, our last step is to fit the final model on all the rows of data not originally held out for testing (both the training and the validation sets combined), and then evaluate the model performance one last time with the held-out test set.

We’ll start by building our parsnip model object again from scratch. We take our best hyperparameter values from our random forest model. When we set the engine, we add a new argument: `importance = "impurity"`. This will provide variable importance scores for this last model, which gives some insight into which predictors drive model performance.

```{r}
# the last model
last_rf_mod <- 
  rand_forest(mtry = 8, min_n = 7, trees = 1000) %>% 
  set_engine("ranger", num.threads = cores, importance = "impurity") %>% 
  set_mode("classification")

# the last workflow
last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

# the last fit
set.seed(345)
last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit
#> # Monte Carlo cross-validation (0.75/0.25) with 1 resamples  
#> # A tibble: 1 x 6
#>   splits         id          .metrics     .notes      .predictions     .workflow
#> * <list>         <chr>       <list>       <list>      <list>           <list>   
#> 1 <split [37.5K… train/test… <tibble [2 … <tibble [0… <tibble [12,500… <workflo…

```

This fitted workflow contains _everything_, including our final metrics based on the test set. So, how did this model do on the test set? Was the validation set a good estimate of future performance?

```{r}
last_rf_fit %>% 
  collect_metrics()
#> # A tibble: 2 x 3
#>   .metric  .estimator .estimate
#>   <chr>    <chr>          <dbl>
#> 1 accuracy binary         0.947
#> 2 roc_auc  binary         0.922
```

This ROC AUC value is pretty close to what we saw when we tuned the random forest model with the validation set, which is good news. That means that our estimate of how well our model would perform with new data was not too far off from how well our model actually performed with the unseen test data.

We can access those variable importance scores via the `.workflow` column. We first need to [pluck](https://purrr.tidyverse.org/reference/pluck.html) out the first element in the workflow column, then [pull out the fit](https://tidymodels.github.io/workflows/reference/workflow-extractors.html) from the workflow object. Finally, the vip package helps us visualize the variable importance scores for the top 20 features:

```{r}
last_rf_fit %>% 
  pluck(".workflow", 1) %>% 
  pull_workflow_fit() %>% 
  vip(num_features = 20)
```

The most important predictors in whether a hotel stay had children or not were the daily cost for the room, the type of room reserved, the type of room that was ultimately assigned, and the time between the creation of the reservation and the arrival date.

Let's generate our last ROC curve to visualize. Since the event we are predicting is the first level in the `children` factor ("children"), we provide `roc_curve()` with the relevant class probability `.pred_children`:

```{r}
last_rf_fit %>% 
  collect_predictions() %>% 
  roc_curve(children, .pred_children) %>% 
  autoplot()
```

Based on these results, the validation set and test set performance statistics are very close, so we would have pretty high confidence that our random forest model with the selected hyperparameters would perform well when predicting new data.

## Where to Next

If you’ve made it to the end of this series of Get Started articles, we hope you feel ready to learn more! You now know the core tidymodels packages and how they fit together. After you are comfortable with the basics we introduced in this series, you can learn how to go farther with tidymodels in your modeling and machine learning projects.

Here are some more ideas for where to go next:

- Study up on statistics and modeling with our comprehensive books.
- Dig deeper into the package documentation sites to find functions that meet your modeling needs. Use the searchable tables to explore what is possible.
- Keep up with the latest about tidymodels packages at the tidyverse blog.
- Find ways to ask for help and contribute to tidymodels to help others.

# [Learn](https://www.tidymodels.org/learn/)
## Perfrom statitical analysis 

After you know what you need to get started with tidymodels, you can learn more and go further. Find articles here to help you solve specific problems using the tidymodels framework. Articles are organized into four categories:

## [Correltion nd regression fundamental with tidy data principles](https://www.tidymodels.org/learn/statistics/tidy-analysis/)

Analyze the results of correlation tests and simple regression models for many data sets at ounce. 

### Introduction

To use the code in this article, you need to install the following packages: `tidymodels` and `tidyr`. 

While the tidymodels package broom is useful for summarizing the result of a single analysis in a consistent format, it is really designed for high-throughput applications, where you must combine results from multiple analyses. These could be subgroups of data, analyses using different models, bootstrap replicates, permutations, and so on. In particular, it plays well with the `nest()`/`unnest()` functions from tidyr and the `map()` function in purrr.

### Correlation analysis

LEt's demonsotrate this with a simple data set, the built-in `Orange`. We start by coercing `Orange` to a `tibble`. This give us a nicer print method that will be especially useful later on when we start working with list-columns. 

```{r}
library(tidymodels)
data("Orange")

Orange <- as_tibble(Orange)
Orange
```

This contains 35 observatios of three variables: `Tree`, `age` and `circumference`. `Tree` is a factor with five levels describing five trees. As might be expected, age and circumference are correlated:

```{r}
cor(Orange$age, Orange$circumference)

library(ggplot2)
ggplot(Orange, aes(age, circumference, color = Tree))+
  geom_point()+
  geom_line()
```

Suppose you want to test for correlations individually within each tree. You can do this with dplyr's `group_by()`. 

```{r}
Orange %>% 
  group_by(Tree) %>% 
  summarise(correlation = cor(age, circumference) )
```

(Note that the correlations are much higher than the aggregated one, and also we can now see the correlation is similar across trees).

Suppose that instead of simply estimating a correlation, we want to perform a hypothesis test with `cor.test()`:

```{r}
ct <- cor.test(Orange$age, Orange$circumference)
ct
```

This test output contains multiple values we may be interested in. Some are vectors of length 1, such as the p-value and the estimate, and some are longer, such as the confidence interval. We can get this into a nicely organized tibble using the `tidy()` function. 

```{r}
tidy(ct)
```

Often, we want to perform multiple tests or fit multiple models, each on a different part of the data. In this exercise, we recommend a `nest-map-unnest` workflow. For example, suppose we want to perform correlation test for each different tree. We start by `nest`ing our data based on the group of interest.

```{r}
library(tidyr)

nested <- 
  Orange %>% 
  nest(data = c(age, circumference))

nested
```

Then we perform a correlation test for each nested tibble using `purrr::map()`. 

```{r}
nested %>% 
  mutate(test = map(data, ~cor.test(.x$age, .x$circumference)))
```

This results in a list-column of S3 objects. We want to tidy each of the objects, which we can also do with `map()`. Finally, we want to unnest the tidied data frames so we can see the results in a flat tibble. Altogether, this looks like: 

```{r}
nested %>% 
  mutate(
    test = map(data, ~cor.test(.x$age, .x$circumference)),
    tidied = map(test, tidy)
  ) %>% 
  unnest(cols = tidied) %>% 
  select(-data, -test)
```

### Regression models 

This type of workflow becomes even more useful when applied to regressions. Untidy output for a regression looks like:

```{r}
lm_fit <- lm(age ~ circumference, data = Orange)
summary(lm_fit)
```

When we tidy these results, we get multiple rows of output for each model:

```{r}
tidy(lm_fit)
```

Now we can handle multiple regression at once using exactly the same workflows as before: 

```{r}
Orange %>% 
  nest(data = c(-Tree)) %>% 
  mutate(
    fit = map(data, ~lm(age ~ circumference, data = .x)),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>% 
  select(-data, -fit)
```

You can just easily use multiple predictors in the regressions, as shown here on the `mtcars` dataset. We nest the data into automatic vs. manual cars (the `am` column), then perform the regression within each nested tibble.

```{r}
data(mtcars)
mtcars <- as_tibble(mtcars)  # to play nicely with list-cols
mtcars

mtcars %>%
  nest(data = c(-am)) %>% 
  mutate(
    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),  # S3 list-col
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied) %>% 
  select(-data, -fit)
```

What if you want not just the `tidy()` output, but the `augment()` and `glance()` outputs as well, while still performing each regression only once? Since we’re using list-columns, we can just fit the model once and use multiple list-columns to store the tidied, glanced and augmented outputs.

```{r}
regressions <- 
  mtcars %>% 
  nest(data = c(-am)) %>% 
  mutate(
    fit = map(data, ~lm(wt ~ mpg + qsec + gear, data = .x)),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    augmented = map(fit, augment)
  )

regressions %>% 
  select(tidied) %>% 
  unnest(tidied)

regressions %>% 
  select(glanced) %>% 
  unnest(glanced)
```

By combining the estimates and p-values across all groups into the same tidy data frame (instead of a list of output model objects), a new class of analyses and visualizations becomes straightforward. This includes:

sorting by p-value or estimate to find the most significant terms across all tests,
p-value histograms, and
volcano plots comparing p-values to effect size estimates.
In each of these cases, we can easily filter, facet, or distinguish based on the term column. In short, this makes the tools of tidy data analysis available for the results of data analysis and models, not just the inputs.

## [K-means clustering with tidy data principles](https://www.tidymodels.org/learn/statistics/k-means/) 

### Introduction 

To use the code in this article, you will need to install `tidymodels` and `tidyr`.

K-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions: 

- `tidy()`
- `augment()`
- `glance()`

Let's start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster. 

```{r}
library(tidymodels)
library(tidyr)

set.seed(27)

centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)

labelled_points <- 
  centers %>% 
  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm)
  ) %>% 
  select(-num_points) %>% 
  unnest(cols = c(x1, x2))

ggplot(
  labelled_points, 
  aes(x1, x2, color = cluster))+
  geom_point(alpha = .3)
```

### How does K-means work

Rather than using equations, this short animation using the [artwork](https://github.com/allisonhorst/stats-illustrations) of Allison Horst explains the clustering process:

### Clustering in R

We will use the built-in `kmeans()` function, which accepts a data frame with all numeric columns as its primary argument. 

```{r}
points <- labelled_points %>% 
  select(-cluster)

kclust <- kmeans(points, centers = 3)
kclust

summary(kclust)
```

The output is a list of vectors, where each component has a different length. There is one of length 300, the same as out original data set. There are two elements of length 3 (`withinss` and `tot.withinss`) and `centers` is a matrix with 3 rows. And then there are the elements of length 1: `totss`, t`ot.withinss`, `betweenss`, and `iter`. (The value ifault indicates

These differing lengths have important meaning when we want to tidy our data set; they signify that each type of component communicates different kind of information. 

- `cluster` (300 values) contains information about each point
- `centers`, `withinss` and `size` (3 values) contain information about each cluster. 
- `totss`, `tot.withinss`, `betweenss`, and `iter` (1 value) contain information about the full clustering

Which of these do we want to extract? There is no right answer; each of them may be interesting to an analysis. Because they communicate entirely different information (not to mention there is no straightfoward way to combine them), they are extracted by separate function, `augment` adds the point classification to the original data set:

```{r}
augment(kclust, points)
```

The tidy() function summarizes on a per-cluster level:

```{r}
tidy(kclust)
```

And as it always does, the `glance()` function extracts a single row-summary:

```{r}
glance(kclust)
```

### Exploratory clustering 

While these summaries are useful, they would not have benn too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like `dplyr`.

Let's say we want to explore the effect of different choices of `k` from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of `k`, the create columns containing the tidied glanced and augmented data:

```{r}
kclust <- 
  tibble(k = 1:9) %>% 
  mutate(
    kclust    = map(k, ~ kmeans(points, .x)),
    tidied    = map(kclust, tidy),
    glanced   = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

kclust
```

We can turn these into three separate data sets each representing a different type of data: using `tidy()`, `augment()`, and `glance()`. Each of these goes into a separate data set as they represent different types of data.

```{r}
clusters <- 
  kclust %>% 
  unnest(cols = c(tidied))

assignments <- 
  kclust %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclust %>% 
  unnest(cols = c(glanced))

clusters
assignments
clusterings
```

Now we can plot the original points using the data from `augment()`, with each point colored according to the predicted cluster.

```{r}
p1 <- 
  ggplot(assignments, aes(x1, x2))+
  geom_point(aes(color = .cluster), alpha = .5)+
  facet_wrap(~k)

p1
```

Already we get a good sense of the proper number of clusters (3), and how the k-means clustering algorithm functions when `k` is too high or low. We can then add centers of the cluster using the data from `tidy()`:

```{r}
p2 <- p1+geom_point(data = clusters, size = 1)
p2
```

The data from glance() fills a different but equally important purpose; it lets us view trends of some summary statistics across values of k. Of particular interest is the total within sum of squares, saved in the tot.withinss column.

```{r}
ggplot(clusterings, 
       aes(k, tot.withinss))+
  geom_line()+
  geom_point()
```

This represents the variance within the clusters. It decreases as `k` increases, but notice a bend (or “elbow”) around k = 3. This bend indicates that additional clusters beyond the third have little value. (See [here](https://web.stanford.edu/~hastie/Papers/gap.pdf) for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output.

## Bootstrap resampling and tidy regression models 

Apply bootstrap resampling to estimate uncertainty in model parameters.Combining fitted models in a tidy way is useful for performing bootstrapping or permutation tests. These approaches have been explored before, for instance by Andrew MacDonald here, and Hadley has explored efficient support for bootstrapping as a potential enhancement to dplyr. The tidymodels package broom fits naturally with dplyr in performing these analyses.

### Introduction 

To use the code in this article, you will need packages `tidymodels` and `tidyr`. 

# Blog 
## Tune XGBoost with tidymodels 

This blog post comes from [Julia Silge's 2020.5.21 post ](https://juliasilge.com/blog/xgboost-tune-volleyball/).

Lately I’ve been publishing screencasts demonstrating how to use the tidymodels framework, starting from just getting started. Today’s screencast explores a more advanced topic in how to tune an XGBoost classification model using with this week’s #TidyTuesday dataset on beach volleyball. 🏐

### Explore the data 

Out modeling goal is to predict whether a beach volleyball team of two won their match based on [game play stats like errors, blocks, attacks, etc from this week’s #TidyTuesday dataset .](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-19/readme.md).

This dataset is quite extensive so its a great opportunity to try a more powerful machine learning algorithm like XGBoost. This model has lots of tuning parameters. 

```{r}
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)

vb_matches
```

This data set has the match stats like serv errors, kills, and so forth divided out by the two players for each team, but we want those combined together because we are going to make a prediction __per team__ (i.e. what makes a team more likely to win). Let’s include predictors like gender, circuit, and year in our model along with the per-match statistics. Let’s omit matches with NA values because we don’t have all kinds of statistics measured for all matches.

```{r}
vb_parsed <- vb_matches %>% 
  transmute(
    circuit,
    gender,
    year,
    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
    w_kills = w_p1_tot_kills + w_p2_tot_kills,
    w_errors = w_p1_tot_errors + w_p2_tot_errors,
    w_aces = w_p1_tot_aces + w_p2_tot_aces,
    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
    w_digs = w_p1_tot_digs + w_p2_tot_digs,
    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
    l_kills = l_p1_tot_kills + l_p2_tot_kills,
    l_errors = l_p1_tot_errors + l_p2_tot_errors,
    l_aces = l_p1_tot_aces + l_p2_tot_aces,
    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
    l_digs = l_p1_tot_digs + l_p2_tot_digs)

vb_parsed %>% 
  Amelia::missmap()

vb_parsed <- vb_parsed %>% 
  na.omit()
```

Still plenty of data! Next, let's create separate dataframes for the winners and losers of each match, and then bind them together. I am using functions like `rename_with` from the upcoming dplyr release here. 

```{r}
library(tidyverse)

winners <- vb_parsed %>%
  select(circuit, gender, year,
         w_attacks:w_digs) %>%
  rename_with(~ str_remove_all(., "w_"), w_attacks:w_digs) %>%
  mutate(win = "win")

losers <- vb_parsed %>% 
  select(circuit, gender, year,
         l_attacks:l_digs) %>%
  rename_with(~ str_remove_all(., "l_"), l_attacks:l_digs) %>%
  mutate(win = "lose")

vb_df <- bind_rows(winners, losers) %>%
  mutate_if(is.character, factor)
```

This is a similar [data prep approach to Joshua Cook](https://twitter.com/JoshDoesa/status/1262738031636672516). 

Exploratory data analysis is always important before modeling. Let's make one plot to explore the relationship in this data. 

```{r}
vb_df %>% 
  pivot_longer(attacks:digs, names_to = "stat", 
               values_to = "value") %>% 
  ggplot(aes(gender, value, fill = win, color = win))+
  geom_boxplot(alpha = .4)+
  facet_wrap(~stat, scales = "free_y", nrow = 2)+
  labs(y = NULL, color = NULL, fill = NULL)
```

We can see differences in errors and blocks especially. There are lots more great examples of #TidyTuesday EDA out there to explore on Twitter!

### Build a model

We can start by loading the tidymodels metapackage, and splitting our data into training and testing sets.

```{r}
library(tidymodels)

set.seed(123)
vb_split <- initial_split(vb_df, strata = win)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)
```

An XGBoost model is based on trees, so we don't need to do much preprocessing for our data; we don’t need to worry about the factors or centering or scaling our data. Let’s just go straight to setting up our model specification. Sounds great, right? On the other hand, we are going to tune a lot of model hyperparameters.

```{r}

xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(), # first three: model complexity
  sample_size = tune(),
  mtry = tune(), # randomness
  learn_rate = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec 
```

Well, let's set up possible values for these hyperparameters to try. Let's use a space-filling design so we can cover the hyperparameter space as well as possible.

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), vb_train),
  learn_rate(),
  size = 30
)

xgb_grid
```

Notice that we had to treat `mtry()` differently because it depends on the actual number of predictors in the data. Let's put the model specification into a workflow for convenience. Since we don't have any complicated data preprocessing, we can use `add_formula()` as our data preprocessor.

```{r}
xgb_wf <- 
  workflow() %>% 
  add_formula(win ~.) %>% 
  add_model(xgb_spec)

xgb_wf
```

Next, let's create cross-validation resamples for tuning our model.

```{r}
set.seed(123)
vb_folds <- vfold_cv(vb_train,
                     strata = win)

vb_folds
```

ITS TIME TO TUNE. We use `tune_grid()` with our tuneable workflow, our resamples and our grid of parameters to try. Let's use `control_grid(save_pred = TRUE)` so we can explore the predictions afterwards.

```{r}
doParallel::registerDoParallel()

set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)

xgb_res
```

This takes a while to finish on my computer!

### Explore results 

We can explore the metrics for all these models.

```{r}
collect_metrics(xgb_res)
```

We can also use visualization to understand our results.

```{r}
xgb_res %>% 
  collect_metrics() %>% 
  filter(.metric == "roc_auc") %>% 
  select(mean, mtry:sample_size) %>% 
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter") %>% 
  ggplot(aes(value, mean, color = parameter))+
  geom_point(alpha =.5, show.legend = FALSE)+
  facet_wrap(~parameter, scales = "free_x")+
  labs(x = NULL, y = "AUC")
```

Remember that we used a space-filling design for the parameters to try. It looks like higher values for tree depth were better, but other than that, the main thing I take away from this plot is that there are several combinations of parameters that perform well.

What are the best performing sets of parameters?

```{r}
show_best(xgb_res, "roc_auc")
```

There may have been lots pf parameters, but we were able to get good performance with several different combinations. Let's choose the best one.

```{r}
best_auc <- select_best(xgb_res, "roc_auc")
best_auc
```

Now let's finalize our tuneable workflow with these values.

```{r}
final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb
```

Instead of `tune()` placeholders, we now have real values for all the model hyperparameters. What are the most important [vaiable importace](https://koalaverse.github.io/vip/).

```{r eval=FALSE}
library(vip)

final_xgb %>%
  fit(data = vb_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

The predictors that are most important in a team winning vs. losing their match are the number of kills, errors, and attacks. There is almost no difference between the two circuits, and very little difference by gender.

It’s time to go back to the testing set! Let’s use `last_fit()` to fit our model one last time on the training data and evaluate our model one last time on the testing set. Notice that this is the first time we have used the testing data during this whole modeling analysis.

```{r}
final_res <- last_fit(final_xgb, vb_split)
collect_metrics(final_res)
```

Our results here indicate that we did not overfit during the tuning process. We can also create a ROC curve for the testing set.

```{r}
final_res %>%
  collect_predictions() %>%
  roc_curve(win, .pred_win) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue", alpha=.7) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```





